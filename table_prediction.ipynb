{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import numpy as np\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch.nn as nn\n",
    "from tqdm import tqdm\n",
    "from torch.nn import functional as F\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from torch.optim.lr_scheduler import LambdaLR"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"./Students_Grading_Dataset.csv\")\n",
    "# df\n",
    "\n",
    "# for att in df:\n",
    "#     print(f\"{att}: {df[att][0]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Manually select Cols (attribute)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Gender</th>\n",
       "      <th>Age</th>\n",
       "      <th>Department</th>\n",
       "      <th>Attendance (%)</th>\n",
       "      <th>Midterm_Score</th>\n",
       "      <th>Final_Score</th>\n",
       "      <th>Assignments_Avg</th>\n",
       "      <th>Quizzes_Avg</th>\n",
       "      <th>Participation_Score</th>\n",
       "      <th>Projects_Score</th>\n",
       "      <th>Total_Score</th>\n",
       "      <th>Grade</th>\n",
       "      <th>Study_Hours_per_Week</th>\n",
       "      <th>Extracurricular_Activities</th>\n",
       "      <th>Internet_Access_at_Home</th>\n",
       "      <th>Parent_Education_Level</th>\n",
       "      <th>Family_Income_Level</th>\n",
       "      <th>Stress_Level (1-10)</th>\n",
       "      <th>Sleep_Hours_per_Night</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Female</td>\n",
       "      <td>22</td>\n",
       "      <td>Engineering</td>\n",
       "      <td>52.29</td>\n",
       "      <td>55.03</td>\n",
       "      <td>57.82</td>\n",
       "      <td>84.22</td>\n",
       "      <td>74.06</td>\n",
       "      <td>3.99</td>\n",
       "      <td>85.90</td>\n",
       "      <td>56.09</td>\n",
       "      <td>F</td>\n",
       "      <td>6.2</td>\n",
       "      <td>No</td>\n",
       "      <td>Yes</td>\n",
       "      <td>High School</td>\n",
       "      <td>Medium</td>\n",
       "      <td>5</td>\n",
       "      <td>4.7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Male</td>\n",
       "      <td>18</td>\n",
       "      <td>Engineering</td>\n",
       "      <td>97.27</td>\n",
       "      <td>97.23</td>\n",
       "      <td>45.80</td>\n",
       "      <td>NaN</td>\n",
       "      <td>94.24</td>\n",
       "      <td>8.32</td>\n",
       "      <td>55.65</td>\n",
       "      <td>50.64</td>\n",
       "      <td>A</td>\n",
       "      <td>19.0</td>\n",
       "      <td>No</td>\n",
       "      <td>Yes</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Medium</td>\n",
       "      <td>4</td>\n",
       "      <td>9.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Male</td>\n",
       "      <td>24</td>\n",
       "      <td>Business</td>\n",
       "      <td>57.19</td>\n",
       "      <td>67.05</td>\n",
       "      <td>93.68</td>\n",
       "      <td>67.70</td>\n",
       "      <td>85.70</td>\n",
       "      <td>5.05</td>\n",
       "      <td>73.79</td>\n",
       "      <td>70.30</td>\n",
       "      <td>D</td>\n",
       "      <td>20.7</td>\n",
       "      <td>No</td>\n",
       "      <td>Yes</td>\n",
       "      <td>Master's</td>\n",
       "      <td>Low</td>\n",
       "      <td>6</td>\n",
       "      <td>6.2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Female</td>\n",
       "      <td>24</td>\n",
       "      <td>Mathematics</td>\n",
       "      <td>95.15</td>\n",
       "      <td>47.79</td>\n",
       "      <td>80.63</td>\n",
       "      <td>66.06</td>\n",
       "      <td>93.51</td>\n",
       "      <td>6.54</td>\n",
       "      <td>92.12</td>\n",
       "      <td>61.63</td>\n",
       "      <td>A</td>\n",
       "      <td>24.8</td>\n",
       "      <td>Yes</td>\n",
       "      <td>Yes</td>\n",
       "      <td>High School</td>\n",
       "      <td>High</td>\n",
       "      <td>3</td>\n",
       "      <td>6.7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Female</td>\n",
       "      <td>23</td>\n",
       "      <td>CS</td>\n",
       "      <td>54.18</td>\n",
       "      <td>46.59</td>\n",
       "      <td>78.89</td>\n",
       "      <td>96.85</td>\n",
       "      <td>83.70</td>\n",
       "      <td>5.97</td>\n",
       "      <td>68.42</td>\n",
       "      <td>66.13</td>\n",
       "      <td>F</td>\n",
       "      <td>15.4</td>\n",
       "      <td>Yes</td>\n",
       "      <td>Yes</td>\n",
       "      <td>High School</td>\n",
       "      <td>High</td>\n",
       "      <td>2</td>\n",
       "      <td>7.1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4995</th>\n",
       "      <td>Male</td>\n",
       "      <td>19</td>\n",
       "      <td>Business</td>\n",
       "      <td>NaN</td>\n",
       "      <td>82.15</td>\n",
       "      <td>60.33</td>\n",
       "      <td>80.09</td>\n",
       "      <td>99.32</td>\n",
       "      <td>5.00</td>\n",
       "      <td>58.42</td>\n",
       "      <td>85.21</td>\n",
       "      <td>D</td>\n",
       "      <td>25.5</td>\n",
       "      <td>No</td>\n",
       "      <td>Yes</td>\n",
       "      <td>High School</td>\n",
       "      <td>Low</td>\n",
       "      <td>10</td>\n",
       "      <td>8.3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4996</th>\n",
       "      <td>Male</td>\n",
       "      <td>19</td>\n",
       "      <td>Business</td>\n",
       "      <td>65.11</td>\n",
       "      <td>86.31</td>\n",
       "      <td>49.80</td>\n",
       "      <td>NaN</td>\n",
       "      <td>88.08</td>\n",
       "      <td>2.79</td>\n",
       "      <td>60.87</td>\n",
       "      <td>95.96</td>\n",
       "      <td>C</td>\n",
       "      <td>5.0</td>\n",
       "      <td>No</td>\n",
       "      <td>Yes</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Medium</td>\n",
       "      <td>4</td>\n",
       "      <td>4.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4997</th>\n",
       "      <td>Female</td>\n",
       "      <td>24</td>\n",
       "      <td>CS</td>\n",
       "      <td>87.54</td>\n",
       "      <td>63.55</td>\n",
       "      <td>64.21</td>\n",
       "      <td>94.28</td>\n",
       "      <td>50.19</td>\n",
       "      <td>3.13</td>\n",
       "      <td>82.65</td>\n",
       "      <td>54.25</td>\n",
       "      <td>A</td>\n",
       "      <td>24.8</td>\n",
       "      <td>Yes</td>\n",
       "      <td>No</td>\n",
       "      <td>High School</td>\n",
       "      <td>Medium</td>\n",
       "      <td>4</td>\n",
       "      <td>6.3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4998</th>\n",
       "      <td>Male</td>\n",
       "      <td>23</td>\n",
       "      <td>CS</td>\n",
       "      <td>92.56</td>\n",
       "      <td>79.79</td>\n",
       "      <td>94.28</td>\n",
       "      <td>81.20</td>\n",
       "      <td>61.18</td>\n",
       "      <td>0.40</td>\n",
       "      <td>94.29</td>\n",
       "      <td>55.84</td>\n",
       "      <td>A</td>\n",
       "      <td>16.1</td>\n",
       "      <td>Yes</td>\n",
       "      <td>Yes</td>\n",
       "      <td>Bachelor's</td>\n",
       "      <td>Low</td>\n",
       "      <td>1</td>\n",
       "      <td>8.4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4999</th>\n",
       "      <td>Female</td>\n",
       "      <td>21</td>\n",
       "      <td>Engineering</td>\n",
       "      <td>83.92</td>\n",
       "      <td>83.24</td>\n",
       "      <td>53.47</td>\n",
       "      <td>51.76</td>\n",
       "      <td>83.51</td>\n",
       "      <td>0.49</td>\n",
       "      <td>69.25</td>\n",
       "      <td>77.86</td>\n",
       "      <td>F</td>\n",
       "      <td>29.2</td>\n",
       "      <td>No</td>\n",
       "      <td>Yes</td>\n",
       "      <td>PhD</td>\n",
       "      <td>Low</td>\n",
       "      <td>2</td>\n",
       "      <td>6.1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5000 rows Ã— 19 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      Gender  Age   Department  Attendance (%)  Midterm_Score  Final_Score  \\\n",
       "0     Female   22  Engineering           52.29          55.03        57.82   \n",
       "1       Male   18  Engineering           97.27          97.23        45.80   \n",
       "2       Male   24     Business           57.19          67.05        93.68   \n",
       "3     Female   24  Mathematics           95.15          47.79        80.63   \n",
       "4     Female   23           CS           54.18          46.59        78.89   \n",
       "...      ...  ...          ...             ...            ...          ...   \n",
       "4995    Male   19     Business             NaN          82.15        60.33   \n",
       "4996    Male   19     Business           65.11          86.31        49.80   \n",
       "4997  Female   24           CS           87.54          63.55        64.21   \n",
       "4998    Male   23           CS           92.56          79.79        94.28   \n",
       "4999  Female   21  Engineering           83.92          83.24        53.47   \n",
       "\n",
       "      Assignments_Avg  Quizzes_Avg  Participation_Score  Projects_Score  \\\n",
       "0               84.22        74.06                 3.99           85.90   \n",
       "1                 NaN        94.24                 8.32           55.65   \n",
       "2               67.70        85.70                 5.05           73.79   \n",
       "3               66.06        93.51                 6.54           92.12   \n",
       "4               96.85        83.70                 5.97           68.42   \n",
       "...               ...          ...                  ...             ...   \n",
       "4995            80.09        99.32                 5.00           58.42   \n",
       "4996              NaN        88.08                 2.79           60.87   \n",
       "4997            94.28        50.19                 3.13           82.65   \n",
       "4998            81.20        61.18                 0.40           94.29   \n",
       "4999            51.76        83.51                 0.49           69.25   \n",
       "\n",
       "      Total_Score Grade  Study_Hours_per_Week Extracurricular_Activities  \\\n",
       "0           56.09     F                   6.2                         No   \n",
       "1           50.64     A                  19.0                         No   \n",
       "2           70.30     D                  20.7                         No   \n",
       "3           61.63     A                  24.8                        Yes   \n",
       "4           66.13     F                  15.4                        Yes   \n",
       "...           ...   ...                   ...                        ...   \n",
       "4995        85.21     D                  25.5                         No   \n",
       "4996        95.96     C                   5.0                         No   \n",
       "4997        54.25     A                  24.8                        Yes   \n",
       "4998        55.84     A                  16.1                        Yes   \n",
       "4999        77.86     F                  29.2                         No   \n",
       "\n",
       "     Internet_Access_at_Home Parent_Education_Level Family_Income_Level  \\\n",
       "0                        Yes            High School              Medium   \n",
       "1                        Yes                    NaN              Medium   \n",
       "2                        Yes               Master's                 Low   \n",
       "3                        Yes            High School                High   \n",
       "4                        Yes            High School                High   \n",
       "...                      ...                    ...                 ...   \n",
       "4995                     Yes            High School                 Low   \n",
       "4996                     Yes                    NaN              Medium   \n",
       "4997                      No            High School              Medium   \n",
       "4998                     Yes             Bachelor's                 Low   \n",
       "4999                     Yes                    PhD                 Low   \n",
       "\n",
       "      Stress_Level (1-10)  Sleep_Hours_per_Night  \n",
       "0                       5                    4.7  \n",
       "1                       4                    9.0  \n",
       "2                       6                    6.2  \n",
       "3                       3                    6.7  \n",
       "4                       2                    7.1  \n",
       "...                   ...                    ...  \n",
       "4995                   10                    8.3  \n",
       "4996                    4                    4.0  \n",
       "4997                    4                    6.3  \n",
       "4998                    1                    8.4  \n",
       "4999                    2                    6.1  \n",
       "\n",
       "[5000 rows x 19 columns]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "unimportant_attribute = ['Student_ID', 'First_Name', 'Last_Name', 'Email']\n",
    "\n",
    "filtered_df = df.drop(unimportant_attribute, axis=1)\n",
    "filtered_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "category_vars = ['Gender', 'Department', 'Grade', 'Extracurricular_Activities', 'Internet_Access_at_Home', 'Parent_Education_Level', 'Family_Income_Level']\n",
    "numerical_vars = list(set(filtered_df.columns) - set(category_vars))\n",
    "\n",
    "to_do_emb = [1] * len(category_vars) + [0] * len(numerical_vars)\n",
    "to_do_emb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Separate rows with-Nan and without-Nan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "row with Nan: (2419, 19)\n",
      "row without Nan: (2581, 19)\n"
     ]
    }
   ],
   "source": [
    "nan_rows = filtered_df.isna().any(axis=1)\n",
    "\n",
    "# Nan rows\n",
    "df_nan = filtered_df[nan_rows]\n",
    "print(f\"row with Nan: {df_nan.shape}\")\n",
    "# Complete rows\n",
    "df_complete = filtered_df[~nan_rows]\n",
    "print(f\"row without Nan: {df_complete.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "df_train: (1935, 19)\n",
      "df_valid: (646, 19)\n"
     ]
    }
   ],
   "source": [
    "# split df_complete into train/valid\n",
    "# data_amount = int(len(df_complete) * 0.8)\n",
    "# df_train = df_complete.iloc[:data_amount, :]\n",
    "# df_valid = df_complete.iloc[data_amount:, :]\n",
    "\n",
    "df_train, df_valid, _, _ = train_test_split(df_complete, df_complete, test_size=0.25, random_state=42)\n",
    "\n",
    "print(f\"df_train: {df_train.shape}\")\n",
    "print(f\"df_valid: {df_valid.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing: \n",
    "1. category to numerical\n",
    "2. max-min norm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def category_to_numerical(data):\n",
    "    le = LabelEncoder()\n",
    "    le.fit(data)\n",
    "    num_data = le.transform(data)\n",
    "    \n",
    "    return num_data, le\n",
    "\n",
    "def max_min_norm(data, train_params = None, process_type = 'train'):\n",
    "    \n",
    "    if process_type == 'train':\n",
    "        data_max = np.max(data)\n",
    "        data_min = np.min(data)\n",
    "    else:\n",
    "        data_max = train_params['Age'][0]\n",
    "        data_min = train_params['Age'][1]\n",
    "        \n",
    "    norm_data = (data - data_min) / (data_max - data_min + 1e-3)    \n",
    "    \n",
    "    if process_type == 'train':\n",
    "        return norm_data, data_max, data_min\n",
    "    else:\n",
    "        return norm_data\n",
    "    \n",
    "def preprocessing(df, train_params = None, process_type = 'train'):\n",
    "    \n",
    "    new_df = pd.DataFrame()\n",
    "    \n",
    "    if process_type == 'train':\n",
    "        train_params = {}\n",
    "        category_var_len = {}\n",
    "    \n",
    "    # # Gender\n",
    "    # gender = df['Gender']\n",
    "    # if process_type == 'train':\n",
    "    #     gender, le = category_to_numerical(gender)\n",
    "    #     train_params['Gender_le'] = le\n",
    "    # else:\n",
    "    #     gender = train_params['Gender_le'].transform(gender)\n",
    "    # new_df['Gender'] = gender\n",
    "    \n",
    "    # # Age\n",
    "    # age = df['Age']\n",
    "    # if process_type == 'train':\n",
    "    #     age, data_max, data_min = max_min_norm(age, process_type = 'train')\n",
    "    #     train_params['Age'] = [data_max, data_min]\n",
    "    # else:\n",
    "    #     age = max_min_norm(age, train_params, process_type = 'valid')\n",
    "        \n",
    "    # new_df['Age'] = age.values\n",
    "\n",
    "\n",
    "    # Category \n",
    "    for cat_name in category_vars:\n",
    "        cat_var = df[cat_name]\n",
    "        if process_type == 'train':\n",
    "            cat_var, le = category_to_numerical(cat_var)\n",
    "            train_params[f'{cat_name}_le'] = le\n",
    "            category_var_len[f'{cat_name}'] = len(np.unique(cat_var))\n",
    "        else:\n",
    "            cat_var = train_params[f'{cat_name}_le'].transform(cat_var)\n",
    "        new_df[f'{cat_name}'] = cat_var\n",
    "    \n",
    "    # Numerical \n",
    "    for num_name in numerical_vars:\n",
    "        num_var = df[num_name]\n",
    "        if process_type == 'train':\n",
    "            num_var, data_max, data_min = max_min_norm(num_var, process_type = 'train')\n",
    "            train_params[num_name] = [data_max, data_min]\n",
    "        else:\n",
    "            num_var = max_min_norm(num_var, train_params, process_type = 'valid')\n",
    "        new_df[num_name] = num_var.values\n",
    "        \n",
    "    if process_type == 'train':\n",
    "        return new_df, train_params, category_var_len\n",
    "    else:\n",
    "        return new_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "category_var_len: {'Gender': 2, 'Department': 4, 'Grade': 5, 'Extracurricular_Activities': 2, 'Internet_Access_at_Home': 2, 'Parent_Education_Level': 4, 'Family_Income_Level': 3}\n",
      "processed_df_train: (1935, 19)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Gender</th>\n",
       "      <th>Department</th>\n",
       "      <th>Grade</th>\n",
       "      <th>Extracurricular_Activities</th>\n",
       "      <th>Internet_Access_at_Home</th>\n",
       "      <th>Parent_Education_Level</th>\n",
       "      <th>Family_Income_Level</th>\n",
       "      <th>Quizzes_Avg</th>\n",
       "      <th>Sleep_Hours_per_Night</th>\n",
       "      <th>Stress_Level (1-10)</th>\n",
       "      <th>Assignments_Avg</th>\n",
       "      <th>Study_Hours_per_Week</th>\n",
       "      <th>Age</th>\n",
       "      <th>Total_Score</th>\n",
       "      <th>Projects_Score</th>\n",
       "      <th>Final_Score</th>\n",
       "      <th>Midterm_Score</th>\n",
       "      <th>Attendance (%)</th>\n",
       "      <th>Participation_Score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.578113</td>\n",
       "      <td>0.339932</td>\n",
       "      <td>0.777691</td>\n",
       "      <td>0.335862</td>\n",
       "      <td>0.413638</td>\n",
       "      <td>0.499917</td>\n",
       "      <td>0.234877</td>\n",
       "      <td>0.195635</td>\n",
       "      <td>0.674993</td>\n",
       "      <td>0.290522</td>\n",
       "      <td>0.613510</td>\n",
       "      <td>0.292971</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>0.225356</td>\n",
       "      <td>0.299940</td>\n",
       "      <td>0.999889</td>\n",
       "      <td>0.386301</td>\n",
       "      <td>0.361431</td>\n",
       "      <td>0.666556</td>\n",
       "      <td>0.999980</td>\n",
       "      <td>0.265248</td>\n",
       "      <td>0.677994</td>\n",
       "      <td>0.440453</td>\n",
       "      <td>0.946770</td>\n",
       "      <td>0.142986</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>0.202720</td>\n",
       "      <td>0.139972</td>\n",
       "      <td>0.333296</td>\n",
       "      <td>0.896900</td>\n",
       "      <td>0.566242</td>\n",
       "      <td>0.499917</td>\n",
       "      <td>0.633147</td>\n",
       "      <td>0.903963</td>\n",
       "      <td>0.725351</td>\n",
       "      <td>0.727973</td>\n",
       "      <td>0.084615</td>\n",
       "      <td>0.115988</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0.920855</td>\n",
       "      <td>0.699860</td>\n",
       "      <td>0.777691</td>\n",
       "      <td>0.686135</td>\n",
       "      <td>0.734910</td>\n",
       "      <td>0.166639</td>\n",
       "      <td>0.985162</td>\n",
       "      <td>0.644716</td>\n",
       "      <td>0.277301</td>\n",
       "      <td>0.471640</td>\n",
       "      <td>0.136225</td>\n",
       "      <td>0.287971</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>0.991767</td>\n",
       "      <td>0.279944</td>\n",
       "      <td>0.666593</td>\n",
       "      <td>0.138508</td>\n",
       "      <td>0.261034</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.118139</td>\n",
       "      <td>0.039807</td>\n",
       "      <td>0.780044</td>\n",
       "      <td>0.371742</td>\n",
       "      <td>0.257646</td>\n",
       "      <td>0.092991</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Gender  Department  Grade  Extracurricular_Activities  \\\n",
       "0       0           0      1                           1   \n",
       "1       1           2      1                           1   \n",
       "2       1           0      4                           0   \n",
       "3       1           1      4                           0   \n",
       "4       1           1      0                           0   \n",
       "\n",
       "   Internet_Access_at_Home  Parent_Education_Level  Family_Income_Level  \\\n",
       "0                        1                       0                    1   \n",
       "1                        1                       3                    1   \n",
       "2                        1                       3                    1   \n",
       "3                        1                       1                    0   \n",
       "4                        1                       2                    2   \n",
       "\n",
       "   Quizzes_Avg  Sleep_Hours_per_Night  Stress_Level (1-10)  Assignments_Avg  \\\n",
       "0     0.578113               0.339932             0.777691         0.335862   \n",
       "1     0.225356               0.299940             0.999889         0.386301   \n",
       "2     0.202720               0.139972             0.333296         0.896900   \n",
       "3     0.920855               0.699860             0.777691         0.686135   \n",
       "4     0.991767               0.279944             0.666593         0.138508   \n",
       "\n",
       "   Study_Hours_per_Week       Age  Total_Score  Projects_Score  Final_Score  \\\n",
       "0              0.413638  0.499917     0.234877        0.195635     0.674993   \n",
       "1              0.361431  0.666556     0.999980        0.265248     0.677994   \n",
       "2              0.566242  0.499917     0.633147        0.903963     0.725351   \n",
       "3              0.734910  0.166639     0.985162        0.644716     0.277301   \n",
       "4              0.261034  0.000000     0.118139        0.039807     0.780044   \n",
       "\n",
       "   Midterm_Score  Attendance (%)  Participation_Score  \n",
       "0       0.290522        0.613510             0.292971  \n",
       "1       0.440453        0.946770             0.142986  \n",
       "2       0.727973        0.084615             0.115988  \n",
       "3       0.471640        0.136225             0.287971  \n",
       "4       0.371742        0.257646             0.092991  "
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "processed_df_train, train_params, category_var_len = preprocessing(df_train, process_type = 'train')\n",
    "# train_params\n",
    "print(f\"category_var_len: {category_var_len}\")\n",
    "print(f\"processed_df_train: {processed_df_train.shape}\")\n",
    "processed_df_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "processed_df_valid: (646, 19)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Gender</th>\n",
       "      <th>Department</th>\n",
       "      <th>Grade</th>\n",
       "      <th>Extracurricular_Activities</th>\n",
       "      <th>Internet_Access_at_Home</th>\n",
       "      <th>Parent_Education_Level</th>\n",
       "      <th>Family_Income_Level</th>\n",
       "      <th>Quizzes_Avg</th>\n",
       "      <th>Sleep_Hours_per_Night</th>\n",
       "      <th>Stress_Level (1-10)</th>\n",
       "      <th>Assignments_Avg</th>\n",
       "      <th>Study_Hours_per_Week</th>\n",
       "      <th>Age</th>\n",
       "      <th>Total_Score</th>\n",
       "      <th>Projects_Score</th>\n",
       "      <th>Final_Score</th>\n",
       "      <th>Midterm_Score</th>\n",
       "      <th>Attendance (%)</th>\n",
       "      <th>Participation_Score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>13.459423</td>\n",
       "      <td>-1.516414</td>\n",
       "      <td>-2.832861</td>\n",
       "      <td>7.818697</td>\n",
       "      <td>1.783036</td>\n",
       "      <td>0.833194</td>\n",
       "      <td>11.488085</td>\n",
       "      <td>13.542743</td>\n",
       "      <td>5.459090</td>\n",
       "      <td>8.838527</td>\n",
       "      <td>7.548742</td>\n",
       "      <td>-2.281286</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>9.023496</td>\n",
       "      <td>-2.066322</td>\n",
       "      <td>-2.332945</td>\n",
       "      <td>13.221130</td>\n",
       "      <td>-0.416597</td>\n",
       "      <td>0.833194</td>\n",
       "      <td>11.439760</td>\n",
       "      <td>5.340777</td>\n",
       "      <td>11.271455</td>\n",
       "      <td>13.339443</td>\n",
       "      <td>6.722213</td>\n",
       "      <td>-1.608065</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>12.379603</td>\n",
       "      <td>-1.866356</td>\n",
       "      <td>-1.833028</td>\n",
       "      <td>13.047825</td>\n",
       "      <td>0.699883</td>\n",
       "      <td>0.499917</td>\n",
       "      <td>7.477087</td>\n",
       "      <td>11.713048</td>\n",
       "      <td>11.853024</td>\n",
       "      <td>13.636061</td>\n",
       "      <td>7.730378</td>\n",
       "      <td>-2.642893</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>10.831528</td>\n",
       "      <td>-1.599733</td>\n",
       "      <td>-1.666389</td>\n",
       "      <td>9.508415</td>\n",
       "      <td>-1.499750</td>\n",
       "      <td>0.833194</td>\n",
       "      <td>7.575404</td>\n",
       "      <td>10.739877</td>\n",
       "      <td>4.319280</td>\n",
       "      <td>6.882186</td>\n",
       "      <td>5.992335</td>\n",
       "      <td>-1.618064</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>5.907349</td>\n",
       "      <td>-2.332945</td>\n",
       "      <td>-1.833028</td>\n",
       "      <td>8.553574</td>\n",
       "      <td>-1.083153</td>\n",
       "      <td>0.833194</td>\n",
       "      <td>12.246292</td>\n",
       "      <td>7.193801</td>\n",
       "      <td>8.655224</td>\n",
       "      <td>11.313114</td>\n",
       "      <td>12.569572</td>\n",
       "      <td>-2.127979</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Gender  Department  Grade  Extracurricular_Activities  \\\n",
       "0       0           2      2                           0   \n",
       "1       0           2      3                           0   \n",
       "2       1           0      0                           0   \n",
       "3       0           1      2                           0   \n",
       "4       0           2      1                           0   \n",
       "\n",
       "   Internet_Access_at_Home  Parent_Education_Level  Family_Income_Level  \\\n",
       "0                        1                       0                    1   \n",
       "1                        1                       3                    1   \n",
       "2                        0                       3                    1   \n",
       "3                        1                       0                    1   \n",
       "4                        1                       1                    2   \n",
       "\n",
       "   Quizzes_Avg  Sleep_Hours_per_Night  Stress_Level (1-10)  Assignments_Avg  \\\n",
       "0    13.459423              -1.516414            -2.832861         7.818697   \n",
       "1     9.023496              -2.066322            -2.332945        13.221130   \n",
       "2    12.379603              -1.866356            -1.833028        13.047825   \n",
       "3    10.831528              -1.599733            -1.666389         9.508415   \n",
       "4     5.907349              -2.332945            -1.833028         8.553574   \n",
       "\n",
       "   Study_Hours_per_Week       Age  Total_Score  Projects_Score  Final_Score  \\\n",
       "0              1.783036  0.833194    11.488085       13.542743     5.459090   \n",
       "1             -0.416597  0.833194    11.439760        5.340777    11.271455   \n",
       "2              0.699883  0.499917     7.477087       11.713048    11.853024   \n",
       "3             -1.499750  0.833194     7.575404       10.739877     4.319280   \n",
       "4             -1.083153  0.833194    12.246292        7.193801     8.655224   \n",
       "\n",
       "   Midterm_Score  Attendance (%)  Participation_Score  \n",
       "0       8.838527        7.548742            -2.281286  \n",
       "1      13.339443        6.722213            -1.608065  \n",
       "2      13.636061        7.730378            -2.642893  \n",
       "3       6.882186        5.992335            -1.618064  \n",
       "4      11.313114       12.569572            -2.127979  "
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "processed_df_valid = preprocessing(df_valid, train_params, process_type = 'valid')\n",
    "print(f\"processed_df_valid: {processed_df_valid.shape}\")\n",
    "processed_df_valid.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TableDataset(Dataset):\n",
    "    def __init__(self, data):\n",
    "        self.data = np.array(data)\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "    \n",
    "    def __getitem__(self, id):\n",
    "        dat = self.data[id, :]\n",
    "        dat = torch.from_numpy(dat)\n",
    "        return dat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 256\n",
    "DROPOUT = 0.3\n",
    "TRANSFORMER_LAYER = 3\n",
    "MASK_PROB = 0.05\n",
    "\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "\n",
    "train_dataset = TableDataset(processed_df_train)\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=BATCH_SIZE, num_workers=0, shuffle=True)\n",
    "\n",
    "valid_dataset = TableDataset(processed_df_valid)\n",
    "valid_dataloader = DataLoader(valid_dataset, batch_size=BATCH_SIZE, num_workers=0, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# category_var_len: {'Gender': 2, 'Department': 4, 'Grade': 5, \n",
    "                #    'Extracurricular_Activities': 2, 'Internet_Access_at_Home': 2, \n",
    "                #    'Parent_Education_Level': 4, 'Family_Income_Level': 3}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Head(nn.Module):\n",
    "    def __init__(self, ori_hidden_size = 1, num_head = 4):\n",
    "        super().__init__()\n",
    "        self.ori_hidden_size = ori_hidden_size\n",
    "        self.num_head = num_head\n",
    "        self.up_emb_size = 32\n",
    "        \n",
    "        # due to ori_hidden_size = 1, we up-project it first so that we can do multiHead att\n",
    "        self.up_emb = nn.Linear(ori_hidden_size, self.up_emb_size, bias=False)\n",
    "        self.down_emb = nn.Linear(self.up_emb_size, ori_hidden_size, bias=False)\n",
    "        \n",
    "        # up_emb_size * 3 for qkv\n",
    "        self.qkv_fn = nn.Linear(self.up_emb_size, self.up_emb_size * 3, bias=False)\n",
    "        self.proj_qkv = nn.Linear(self.up_emb_size, self.up_emb_size, bias = False)\n",
    "                \n",
    "        # other\n",
    "        self.att_dropout = nn.Dropout(DROPOUT)\n",
    "        self.down_dropout = nn.Dropout(DROPOUT)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        '''\n",
    "        num_vars is seq_len in here\n",
    "        \n",
    "        ori_hidden_size = 1\n",
    "        up_emb = 32\n",
    "        \n",
    "        '''        \n",
    "        batch_size, num_vars, _ = x.shape\n",
    "        \n",
    "        # up emb: ori_hidden_size -> up_emb_size\n",
    "        x = self.up_emb(x)\n",
    "\n",
    "        # qkv: up_emb_size * 3\n",
    "        x = self.qkv_fn(x) \n",
    "        q, k, v = x.split(self.up_emb_size, dim = 2) \n",
    "        \n",
    "        # split head: each shape = [batch_size, num_head, num_vars(seq_len), head_size = 8]\n",
    "        q = q.view(batch_size, num_vars, self.num_head, self.up_emb_size // self.num_head).transpose(1, 2)\n",
    "        k = k.view(batch_size, num_vars, self.num_head, self.up_emb_size // self.num_head).transpose(1, 2)\n",
    "        v = v.view(batch_size, num_vars, self.num_head, self.up_emb_size // self.num_head).transpose(1, 2)\n",
    "        \n",
    "        # attention matrix calculation: [batch_size, num_head, num_vars(seq_len), num_vars]\n",
    "        att = (q @ k.transpose(-2,-1)) * (1 / torch.sqrt(torch.ones([1]).to(device) * k.size(-1)))\n",
    "        # att = self.modify_att(att)\n",
    "        att = F.softmax(att, dim = -1)\n",
    "        att = self.att_dropout(att)\n",
    "        \n",
    "        # att matrix * V: [batch_size, num_head, num_vars(seq_len), head_size]\n",
    "        out = att @ v\n",
    "        out = out.transpose(1,2).contiguous().view(batch_size, num_vars, self.up_emb_size)\n",
    "        out = self.proj_qkv(out)\n",
    "        \n",
    "        # down emb: up_emb_size -> ori_hidden_size\n",
    "        out = self.down_emb(out)\n",
    "        out = self.down_dropout(out)\n",
    "        \n",
    "        return out\n",
    "    \n",
    "    def modify_att(self, att):\n",
    "        '''\n",
    "        att: [batch_size, num_head, num_vars(seq_len), num_vars]\n",
    "        '''\n",
    "        \n",
    "        # print(f\"att: {att.shape}\")\n",
    "        # print(f\"category_var_len: {len(category_var_len)}\")\n",
    "        \n",
    "        num_numerical = 12\n",
    "        num_category = 7 # (7 * 2)\n",
    "        \n",
    "        # average rows for category\n",
    "        att_category = []\n",
    "        for c_id in range(num_category):\n",
    "            cate_row = att[:, :, c_id * 2: (c_id + 1)*2, :]\n",
    "            aggregated_cate_row = torch.mean(cate_row, dim = 2)\n",
    "            att_category.append(aggregated_cate_row)\n",
    "            att_category.append(aggregated_cate_row)\n",
    "\n",
    "        att_category = torch.stack(att_category, dim = 2)\n",
    "        # print(f\"aggreatt_categorygated_cate_row: {att_category.shape}\")\n",
    "\n",
    "        # average cols for numerical\n",
    "        att_numerical = []\n",
    "        for n_id in range(num_category): # ([16, 4, 12, 2])\n",
    "            category_col = att[:, :, -num_numerical:, n_id * 2: (n_id + 1)*2]\n",
    "            aggregated_category_col = torch.mean(category_col, dim = 3)\n",
    "            att_numerical.append(aggregated_category_col)\n",
    "            att_numerical.append(aggregated_category_col)\n",
    "            # print(f\"category_col: {category_col.shape}\")\n",
    "            \n",
    "        att_numerical = torch.stack(att_numerical, dim = 3)\n",
    "        att_numerical = torch.cat([att_numerical, att[:, :,  -num_numerical:, -num_numerical:]], dim = 3)\n",
    "\n",
    "        att = torch.cat([att_category, att_numerical], dim = 2)\n",
    "        \n",
    "        return att"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLP(nn.Module):\n",
    "\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.c_fc    = nn.Linear(1, 32)\n",
    "        self.gelu    = nn.GELU(approximate='tanh')\n",
    "        self.c_proj  = nn.Linear(32, 1)\n",
    "        self.dropout = nn.Dropout(DROPOUT)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.c_fc(x)\n",
    "        x = self.gelu(x)\n",
    "        x = self.c_proj(x)\n",
    "        x = self.dropout(x)\n",
    "        \n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Block(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Block, self).__init__()\n",
    "        self.ln_head = nn.LayerNorm(1)\n",
    "        self.head = Head()\n",
    "        self.ln_mlp = nn.LayerNorm(1)\n",
    "        self.mlp = MLP()\n",
    "                \n",
    "    def forward(self, x):\n",
    "        \n",
    "        x = x + self.head(self.ln_head(x))\n",
    "        x = x + self.mlp(self.ln_mlp(x))\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GPT(nn.Module):\n",
    "    def __init__(self, layer):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.blocks = nn.ModuleList(Block() for _ in range(layer))\n",
    "        \n",
    "    def forward(self, x):\n",
    "        \n",
    "        for block in self.blocks:\n",
    "            x = block(x)\n",
    "            \n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class tableModel(nn.Module):\n",
    "    def __init__(self, category_var_len):\n",
    "        super(tableModel,self).__init__()\n",
    "        self.num_category_var = len(category_var_len)\n",
    "        self.num_numerical_var = 12\n",
    "        self.emb_size = 2\n",
    "        self.category_dict = category_var_len\n",
    "        self.batch_size = BATCH_SIZE\n",
    "        \n",
    "        # encode category vars\n",
    "        self.encode_gender = nn.Embedding(category_var_len['Gender'] + 1, self.emb_size)\n",
    "        self.encode_depart = nn.Embedding(category_var_len['Department'] + 1, self.emb_size)\n",
    "        self.encode_grade = nn.Embedding(category_var_len['Grade'] + 1, self.emb_size)\n",
    "        self.encode_activity = nn.Embedding(category_var_len['Extracurricular_Activities'] + 1, self.emb_size)        \n",
    "        self.encode_internet = nn.Embedding(category_var_len['Internet_Access_at_Home'] + 1, self.emb_size)\n",
    "        self.encode_parent = nn.Embedding(category_var_len['Parent_Education_Level'] + 1, self.emb_size)        \n",
    "        self.encode_income = nn.Embedding(category_var_len['Family_Income_Level'] + 1, self.emb_size)  \n",
    "        \n",
    "        self.encoders = [self.encode_gender, self.encode_depart, self.encode_grade, self.encode_activity, self.encode_internet,\n",
    "                        self.encode_parent, self.encode_income]\n",
    "        \n",
    "        self.encode_dropout = nn.Dropout(DROPOUT)\n",
    "        \n",
    "        # decode category vars\n",
    "        self.decode_gender = nn.Linear(self.emb_size, category_var_len['Gender'] + 1, bias=False)\n",
    "        self.encode_gender.weight = self.decode_gender.weight        \n",
    "        self.decode_depart = nn.Linear(self.emb_size, category_var_len['Department'] + 1, bias=False)\n",
    "        self.encode_depart.weight = self.decode_depart.weight       \n",
    "        self.decode_grade = nn.Linear(self.emb_size, category_var_len['Grade'] + 1, bias=False)\n",
    "        self.encode_grade.weight = self.decode_grade.weight          \n",
    "        self.decode_activity = nn.Linear(self.emb_size, category_var_len['Extracurricular_Activities'] + 1, bias=False)\n",
    "        self.encode_activity.weight = self.decode_activity.weight               \n",
    "        self.decode_internet = nn.Linear(self.emb_size, category_var_len['Internet_Access_at_Home'] + 1, bias=False)\n",
    "        self.encode_internet.weight = self.decode_internet.weight    \n",
    "        self.decode_parent = nn.Linear(self.emb_size, category_var_len['Parent_Education_Level'] + 1, bias=False)\n",
    "        self.encode_parent.weight = self.decode_parent.weight    \n",
    "        self.decode_income = nn.Linear(self.emb_size, category_var_len['Family_Income_Level'] + 1, bias=False)\n",
    "        self.encode_income.weight = self.decode_income.weight    \n",
    "        \n",
    "        self.decoders = [self.decode_gender, self.decode_depart, self.decode_grade, self.decode_activity, self.decode_internet,\n",
    "                        self.decode_parent, self.decode_income]\n",
    "        \n",
    "        self.decode_dropout = nn.Dropout(DROPOUT)\n",
    "\n",
    "        \n",
    "        # transformer\n",
    "        self.gpt = GPT(layer = TRANSFORMER_LAYER)\n",
    "        \n",
    "    # def masking_table(self, x, seed = 42):\n",
    "        \n",
    "    #     '''\n",
    "    #     x = (batch_size, num_var = 19)\n",
    "    #     '''\n",
    "        \n",
    "    #     torch.manual_seed(seed)\n",
    "    #     if torch.cuda.is_available():\n",
    "    #         torch.cuda.manual_seed_all(seed)\n",
    "\n",
    "    #     self.masking_prob = 0.2       \n",
    "\n",
    "    #     # category masking\n",
    "    #     category_var = x[:, :self.num_category_var] #(batch_size, num_var = 7)\n",
    "        \n",
    "    #     random_cat = torch.rand((self.batch_size, self.num_category_var)).to(device)\n",
    "    #     masking_cat = random_cat < self.masking_prob\n",
    "    #     mask_token = torch.tensor([2, 4, 5, 2, 2, 4, 3]).to(device)\n",
    "    #     mask_token = mask_token.unsqueeze(0).expand_as(category_var)\n",
    "\n",
    "    #     masked_category_var = torch.where(masking_cat, mask_token, category_var)\n",
    "        \n",
    "    #     # category masking\n",
    "    #     numerical_var = x[:, -self.num_numerical_var:] #(batch_size, num_var = 12)\n",
    "        \n",
    "    #     random_numerical = torch.rand((self.batch_size, self.num_numerical_var)).to(device)\n",
    "    #     masking_numerical = random_numerical < self.masking_prob\n",
    "    #     mask_token = torch.zeros_like(random_numerical).to(device)\n",
    "        \n",
    "    #     masked_numerical_var = torch.where(masking_numerical, mask_token, numerical_var)\n",
    "\n",
    "    #     masked_x = torch.cat([masked_category_var, masked_numerical_var], dim = 1)\n",
    "\n",
    "    #     return masked_x\n",
    "                        \n",
    "                        \n",
    "    # category_var_len: {'Gender': 2, 'Department': 4, 'Grade': 5, \n",
    "                    #    'Extracurricular_Activities': 2, 'Internet_Access_at_Home': 2, \n",
    "                    #    'Parent_Education_Level': 4, 'Family_Income_Level': 3}\n",
    "\n",
    "    def masking_table(self, x, seed=42):\n",
    "        \"\"\"\n",
    "        x: (batch_size, num_var = 19)\n",
    "        \"\"\"\n",
    "        # Set random seed for reproducibility\n",
    "        torch.manual_seed(seed)\n",
    "        if torch.cuda.is_available():\n",
    "            torch.cuda.manual_seed_all(seed)\n",
    "\n",
    "        self.masking_prob = MASK_PROB \n",
    "        device = x.device  # Get the device from input tensor\n",
    "\n",
    "        # Category masking\n",
    "        category_var = x[:, :self.num_category_var].long()  # Ensure category_var is integer type\n",
    "        random_cat = torch.rand_like(category_var, dtype=torch.float, device=device)\n",
    "        masking_cat = random_cat < self.masking_prob\n",
    "        mask_token = torch.tensor([2, 4, 5, 2, 2, 4, 3], device=device, dtype=torch.long).expand_as(category_var)\n",
    "\n",
    "        # Apply mask in-place (avoiding memory allocation overhead)\n",
    "        masked_category_var = category_var.clone()  # Clone to avoid modifying input\n",
    "        masked_category_var[masking_cat] = mask_token[masking_cat]\n",
    "\n",
    "        # Numerical masking\n",
    "        numerical_var = x[:, -self.num_numerical_var:].float()  # Ensure numerical_var is float type\n",
    "        random_numerical = torch.rand_like(numerical_var, dtype=torch.float, device=device)\n",
    "        masking_numerical = random_numerical < self.masking_prob\n",
    "\n",
    "        masked_numerical_var = numerical_var.clone()  # Clone to avoid modifying input\n",
    "        masked_numerical_var[masking_numerical] = 0.5  # Directly set masked values to zero\n",
    "\n",
    "        # Concatenating the masked category and numerical variables\n",
    "        return torch.cat([masked_category_var, masked_numerical_var], dim=1)\n",
    "                        \n",
    "                        \n",
    "    def forward(self, x):\n",
    "        '''\n",
    "        x: [batch_size, 19]\n",
    "        \n",
    "        category vars size = 7 --- embedding ---> category vars size = 14\n",
    "        numerical vars size = 12\n",
    "        \n",
    "        x: [batch_size, 14 + 12]\n",
    "        '''\n",
    "        \n",
    "        ''' masking'''\n",
    "        x = self.masking_table(x)\n",
    "\n",
    "        ''' Encode category ''' \n",
    "        # category vars\n",
    "        cat_vars = []\n",
    "        for c_id, encode_fn in zip(range(self.num_category_var), self.encoders):\n",
    "            emb_c = encode_fn(x[:,c_id].long())\n",
    "            emb_c = self.encode_dropout(emb_c)\n",
    "            cat_vars.append(emb_c)\n",
    "        cat_vars = torch.cat(cat_vars, dim = 1).float() \n",
    "        \n",
    "        # numerical vars\n",
    "        num_vars = x[:, - self.num_numerical_var:].float()\n",
    "\n",
    "        # combine category and numerical vars        \n",
    "        x = torch.cat([cat_vars, num_vars], dim = 1)\n",
    "\n",
    "        '''\n",
    "        Transformer\n",
    "        '''\n",
    "        x = torch.unsqueeze(x, dim = 2)\n",
    "        x = self.gpt(x)\n",
    "        x = torch.squeeze(x, dim = 2)\n",
    "        \n",
    "        \n",
    "        ''' Decode category ''' \n",
    "        # split numerical and category\n",
    "        num_vars = x[:, - self.num_numerical_var:]\n",
    "        cat_vars = x[:, :self.num_category_var * 2]\n",
    "        \n",
    "        # category vars\n",
    "        decoded_cat_vars = []\n",
    "        for c_id, decode_fn in zip(range(self.num_category_var), self.decoders):\n",
    "            emb_c = cat_vars[:, c_id * self.emb_size: (c_id + 1) * self.emb_size]\n",
    "            c_var = decode_fn(emb_c)\n",
    "            c_var = self.decode_dropout(c_var)\n",
    "            decoded_cat_vars.append(c_var)\n",
    "        \n",
    "\n",
    "        return num_vars, decoded_cat_vars\n",
    "    \n",
    "    def inference(self, x):\n",
    "        '''\n",
    "        DO NOT mask during inference\n",
    "        '''\n",
    "        ''' masking'''\n",
    "        x = self.masking_table(x)\n",
    "        \n",
    "        # category vars\n",
    "        cat_vars = []\n",
    "        for c_id, encode_fn in zip(range(self.num_category_var), self.encoders):\n",
    "            emb_c = encode_fn(x[:,c_id].long())\n",
    "            emb_c = self.encode_dropout(emb_c)\n",
    "            cat_vars.append(emb_c)\n",
    "        cat_vars = torch.cat(cat_vars, dim = 1).float()\n",
    "\n",
    "        # numerical vars\n",
    "        num_vars = x[:, - self.num_numerical_var:].float()\n",
    "\n",
    "        # combine category and numerical vars        \n",
    "        x = torch.cat([cat_vars, num_vars], dim = 1)\n",
    "\n",
    "        '''\n",
    "        Transformer\n",
    "        '''\n",
    "        x = torch.unsqueeze(x, dim = 2)\n",
    "        x = self.gpt(x)\n",
    "        x = torch.squeeze(x, dim = 2)\n",
    "        \n",
    "        \n",
    "        ''' Decode category ''' \n",
    "        # split numerical and category\n",
    "        num_vars = x[:, - self.num_numerical_var:]\n",
    "        cat_vars = x[:, :self.num_category_var * 2]\n",
    "        \n",
    "        # category vars\n",
    "        decoded_cat_vars = []\n",
    "        for c_id, decode_fn in zip(range(self.num_category_var), self.decoders):\n",
    "            emb_c = cat_vars[:, c_id * self.emb_size: (c_id + 1) * self.emb_size]\n",
    "            c_var = decode_fn(emb_c)\n",
    "            c_var = self.decode_dropout(c_var)\n",
    "            pred_c = torch.argmax(c_var, dim = -1)\n",
    "            decoded_cat_vars.append(pred_c)\n",
    "        \n",
    "        decoded_cat_vars = torch.stack(decoded_cat_vars, dim = 1)\n",
    "\n",
    "        pred = torch.cat([decoded_cat_vars, num_vars], dim = 1)\n",
    "\n",
    "        return pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "train_dataset = TableDataset(processed_df_train)\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=BATCH_SIZE, num_workers=2, shuffle=True)\n",
    "\n",
    "valid_dataset = TableDataset(processed_df_valid)\n",
    "valid_dataloader = DataLoader(valid_dataset, batch_size=BATCH_SIZE, num_workers=2, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def linear_warmup_decay_lr(lr_init, lr_final, num_warmup_steps, num_training_steps):\n",
    "    \"\"\"\n",
    "    Returns a lambda function for LambdaLR.\n",
    "    - lr_init: åˆå§‹å­¸ç¿’çŽ‡\n",
    "    - lr_final: æœ€çµ‚å­¸ç¿’çŽ‡ï¼ˆä¸æ˜¯ 0ï¼‰\n",
    "    - num_warmup_steps: é ç†±æ­¥æ•¸\n",
    "    - num_training_steps: ç¸½è¨“ç·´æ­¥æ•¸\n",
    "    \"\"\"\n",
    "    def lr_lambda(current_step):\n",
    "        if current_step < num_warmup_steps:\n",
    "            return current_step / num_warmup_steps  # ç·šæ€§é ç†±\n",
    "        else:\n",
    "            progress = (current_step - num_warmup_steps) / (num_training_steps - num_warmup_steps)\n",
    "            return (1 - progress) * (1 - lr_final / lr_init) + (lr_final / lr_init)  # ç·šæ€§è¡°æ¸›åˆ° lr_final\n",
    "    return lr_lambda"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "LEARNING_RATE = 1e-3\n",
    "EPOCHS = 3000\n",
    "\n",
    "\n",
    "model = tableModel(category_var_len).to(device)\n",
    "# optimizer = torch.optim.AdamW(model.parameters(), lr = 1e-3)\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=LEARNING_RATE, betas=(0.9, 0.95), eps=1e-6, weight_decay=0)\n",
    "\n",
    "scheduler = LambdaLR(optimizer, lr_lambda=linear_warmup_decay_lr(lr_init = LEARNING_RATE, lr_final = LEARNING_RATE * 0.0001, num_warmup_steps = 30, num_training_steps = EPOCHS))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "MSE_loss_fn = nn.MSELoss()\n",
    "CE_loss_fn = nn.CrossEntropyLoss()\n",
    "\n",
    "def loss_fn(pred_numerical, pred_category, label):\n",
    "    \n",
    "    num_numerical = 12\n",
    "    num_category = 7\n",
    "    ratio_numerical = num_numerical / (num_numerical + num_category)\n",
    "    ratio_category = 1 / (num_numerical + num_category)\n",
    "    \n",
    "    label_category = label[:, :num_category]\n",
    "    label_numerical = label[:, -num_numerical:]\n",
    "    \n",
    "    total_loss = torch.zeros(1).to(device)\n",
    "    \n",
    "    mse_loss = MSE_loss_fn(pred_numerical, label_numerical)\n",
    "    total_loss += (mse_loss * ratio_numerical)\n",
    "    \n",
    "    for i in range(num_category):\n",
    "        pred = pred_category[i]\n",
    "        loss = CE_loss_fn(pred, label_category[:, i].long())\n",
    "        total_loss += (loss * ratio_category)\n",
    "        \n",
    "    return total_loss\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "iterate epoch:   0%|          | 0/3000 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 0, loss: 0.4723139964044094\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "iterate epoch:   0%|          | 1/3000 [00:00<39:45,  1.26it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 0, val_loss: 1.943771243095398\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "iterate epoch:   1%|          | 30/3000 [00:12<20:13,  2.45it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 30, loss: 0.40724771469831467\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "iterate epoch:   1%|          | 31/3000 [00:12<20:07,  2.46it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 30, val_loss: 1.9087577263514202\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "iterate epoch:   1%|          | 32/3000 [00:13<20:30,  2.41it/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[25], line 18\u001b[0m\n\u001b[1;32m     15\u001b[0m     losses\u001b[38;5;241m.\u001b[39mappend(loss\u001b[38;5;241m.\u001b[39mitem())\n\u001b[1;32m     17\u001b[0m     optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[0;32m---> 18\u001b[0m     \u001b[43mloss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     19\u001b[0m     optimizer\u001b[38;5;241m.\u001b[39mstep()\n\u001b[1;32m     21\u001b[0m scheduler\u001b[38;5;241m.\u001b[39mstep()\n",
      "File \u001b[0;32m~/miniconda3/envs/Vit/lib/python3.12/site-packages/torch/_tensor.py:626\u001b[0m, in \u001b[0;36mTensor.backward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    616\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    617\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m    618\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[1;32m    619\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    624\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[1;32m    625\u001b[0m     )\n\u001b[0;32m--> 626\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    627\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\n\u001b[1;32m    628\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/Vit/lib/python3.12/site-packages/torch/autograd/__init__.py:347\u001b[0m, in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    342\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[1;32m    344\u001b[0m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[1;32m    345\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[1;32m    346\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[0;32m--> 347\u001b[0m \u001b[43m_engine_run_backward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    348\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    349\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    350\u001b[0m \u001b[43m    \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    351\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    352\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    353\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    354\u001b[0m \u001b[43m    \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    355\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/Vit/lib/python3.12/site-packages/torch/autograd/graph.py:823\u001b[0m, in \u001b[0;36m_engine_run_backward\u001b[0;34m(t_outputs, *args, **kwargs)\u001b[0m\n\u001b[1;32m    821\u001b[0m     unregister_hooks \u001b[38;5;241m=\u001b[39m _register_logging_hooks_on_whole_graph(t_outputs)\n\u001b[1;32m    822\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 823\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mVariable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execution_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[1;32m    824\u001b[0m \u001b[43m        \u001b[49m\u001b[43mt_outputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\n\u001b[1;32m    825\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[1;32m    826\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m    827\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m attach_logging_hooks:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "\n",
    "train_LOSS = []\n",
    "valid_LOSS = []\n",
    "\n",
    "for epoch in tqdm(range(EPOCHS), desc=\"iterate epoch\"):\n",
    "    losses = []\n",
    "    val_losses = []\n",
    "    \n",
    "    model.train()\n",
    "    for data in train_dataloader:\n",
    "        \n",
    "        data = data.float().to(device)\n",
    "        # print(f\"data: {data.shape}\")\n",
    "        pred_numerical, pred_category = model(data)\n",
    "        loss = loss_fn(pred_numerical, pred_category, data)\n",
    "        losses.append(loss.item())\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    scheduler.step()\n",
    "    losses = np.mean(losses)\n",
    "    train_LOSS.append(losses)\n",
    "    \n",
    "    if epoch % 30 == 0:    \n",
    "        print(f\"epoch: {epoch}, loss: {losses}\")\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        model.eval()\n",
    "        for data in valid_dataloader:\n",
    "            data = data.float().to(device)\n",
    "                \n",
    "            pred_numerical, pred_category = model(data)\n",
    "            loss = loss_fn(pred_numerical, pred_category, data)\n",
    "            val_losses.append(loss.item())\n",
    "            \n",
    "    val_losses = np.mean(val_losses)\n",
    "    valid_LOSS.append(val_losses)\n",
    "    \n",
    "    if epoch % 30 == 0:    \n",
    "        print(f\"epoch: {epoch}, val_loss: {val_losses}\")\n",
    "        print()\n",
    "        \n",
    "        \n",
    "        \n",
    "        # pred = model.inference(data)\n",
    "        \n",
    "        \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_4023891/2774934369.py:6: UserWarning: No artists with labels found to put in legend.  Note that artists whose label start with an underscore are ignored when legend() is called with no argument.\n",
      "  plt.legend()\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAkAAAAGwCAYAAABB4NqyAAAAOnRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjEwLjAsIGh0dHBzOi8vbWF0cGxvdGxpYi5vcmcvlHJYcgAAAAlwSFlzAAAPYQAAD2EBqD+naQAAN3tJREFUeJzt3X90VPWd//HXBJJJoMkAAvkhISIiCNFs+BUCgiISiUJBbMHWBrBYTixaWPS7btSi0nUjPcVFlopiUcqqQG0I0AWEUIGoCVowQVRk8YASIWMEIZOAJJDc7x/TjAz5QRKSuTO5z8c598z98bl33vfDbfPy/hqbYRiGAAAALCTI7AIAAAB8jQAEAAAshwAEAAAshwAEAAAshwAEAAAshwAEAAAshwAEAAAsp73ZBfij6upqHT9+XOHh4bLZbGaXAwAAGsEwDJWVlSkmJkZBQQ2f4yEA1eH48eOKjY01uwwAANAMRUVF6tGjR4NtCEB1CA8Pl+TuwIiICJOrAQAAjeFyuRQbG+v5O94QAlAdai57RUREEIAAAAgwjbl9hZugAQCA5RCAAACA5RCAAACA5XAPEAAA8BtVVVU6f/58vctDQkIu+4h7YxCAAACA6QzDkNPp1OnTpxtsFxQUpF69eikkJOSKvo8ABAAATFcTfrp3764OHTrU+SRXzYuKi4uL1bNnzyt6WTEBCAAAmKqqqsoTfq666qoG23br1k3Hjx/XhQsXFBwc3Ozv5CZoAABgqpp7fjp06HDZtjWXvqqqqq7oOwlAAADALzTmklZL/UYnAQgAAFgOAQgAAFgOAQgAAFgOAciXDEMqKZEOHjS7EgAA/I5hGC3SpjEIQL709ttSZKQ0ZYrZlQAA4DdqHmc/e/bsZdtWVlZKktq1a3dF38l7gHypd2/35xdfuM8GtdCd7AAABLJ27dqpU6dOKikpkaQGX4T47bffqkOHDmrf/soiDAHIl3r0cH+ePSudOSP96Efm1gMAgJ+IioqSJE8Iqk9QUNAVvwVaIgD5VliY1L69dOGCdPo0AQgAgH+y2WyKjo5W9+7d+THUNsdmkxwO6eRJqbT0hzNCAABAkvty2JXe39MY3ATta506uT8v82u3AACg9RCAfM3hcH8SgAAAMA0ByNfCw92f5eXm1gEAgIURgHytY0f355kz5tYBAICFEYB8jQAEAIDpCEC+RgACAMB0BCBfIwABAGA6ApCvEYAAADAdAcjXCEAAAJiOAORrBCAAAExHAPI1AhAAAKYjAPkaAQgAANMRgHyNAAQAgOkIQL5GAAIAwHQEIF8jAAEAYDoCkK8RgAAAMJ2pASgzM1NDhgxReHi4unfvrkmTJungwYOXXW/Xrl0aNGiQQkNDde211+qll16q1SYrK0v9+/eX3W5X//79lZ2d3Rq70HQEIAAATGdqANq1a5dmz56t3bt3KycnRxcuXFBKSorONBAOjhw5ojvvvFMjR45UQUGBHn/8cf3mN79RVlaWp01+fr6mTp2qtLQ07du3T2lpaZoyZYo++OADX+xWwy4OQIZhbi0AAFiUzTD856/wt99+q+7du2vXrl0aNWpUnW0ee+wxbdy4UQcOHPDMS09P1759+5Sfny9Jmjp1qlwul7Zs2eJpM27cOHXu3FmrV6++bB0ul0sOh0OlpaWKiIi4wr26xOnTUufO7vFz5yS7vWW3DwCARTXl77df3QNUWloqSerSpUu9bfLz85WSkuI174477tCePXt0/vz5Btvk5eXVuc2Kigq5XC6vodXUnAGSuAwGAIBJ/CYAGYahefPm6eabb1Z8fHy97ZxOpyIjI73mRUZG6sKFCzpx4kSDbZxOZ53bzMzMlMPh8AyxsbFXuDcNCA52DxIBCAAAk/hNAHrooYf08ccfN+oSlc1m85quuYp38fy62lw6r0ZGRoZKS0s9Q1FRUVPLb5qaAHTRZTwAAOA77c0uQJIefvhhbdy4Ubm5uerRo0eDbaOiomqdySkpKVH79u111VVXNdjm0rNCNex2u+y+vBfn7Fn35x13cCM0AAAmMPUMkGEYeuihh7Ru3Tq988476tWr12XXSU5OVk5Ojte8bdu2afDgwQr+55mV+toMHz685YpvKXv3ml0BAACWY2oAmj17tl5//XW9+eabCg8Pl9PplNPp1Pfff+9pk5GRoWnTpnmm09PT9dVXX2nevHk6cOCAXn31Va1YsUKPPvqop82cOXO0bds2LVy4UJ9//rkWLlyo7du3a+7cub7cvfrt3v3D+AsvmFcHAAAWZepj8PXdk/Paa69pxowZkqQZM2boyy+/1M6dOz3Ld+3apX/913/Vp59+qpiYGD322GNKT0/32sZf//pXPfnkkzp8+LB69+6tZ599VpMnT25UXa36GHyNvDxpxAj3U2ElJVKHDq3zPQAAWERT/n771XuA/IVPApBhSNdeK335pbR2rTRlSut8DwAAFhGw7wGyFJtNuvde9/i6debWAgCAxRCAzJSa6v7csYOnwQAA8CECkJmSkqTQUPc9QJ99ZnY1AABYBgHITHa7dPPN7vF33jG3FgAALIQAZLbbbnN/7thhbh0AAFgIAchso0e7P3fulKqrTS0FAACrIACZbfBgKTxcOnVK2rfP7GoAALAEApDZ2reXRo50j3MfEAAAPkEA8gfcBwQAgE8RgPxBTQDKzZUuXDC3FgAALIAA5A8SEqTOnaWyMn4dHgAAHyAA+YOgIOnWW93j3AcEAECrIwD5i4sfhwcAAK2KAOQvas4AvfeedP68qaUAANDWEYD8xYAB0lVXSWfPSnv2mF0NAABtGgHIXwQFSbfc4h7nMhgAAK2KAORPai6D7dplahkAALR1BCB/UnMGiPuAAABoVQQgfxIfL3XpIp05w/uAAABoRQQgf8J9QAAA+AQByN9wHxAAAK2OAORvuA8IAIBWRwDyNzfe6L4PqLyc9wEBANBKCED+Jijoh1+Hz8kxtxYAANooApA/GjvW/UkAAgCgVRCA/FFNANq9W3K5zK0FAIA2iADkj3r1kq67TrpwgcfhAQBoBQQgf8VlMAAAWg0ByF8RgAAAaDUEIH81erT7ibCDB6WjR82uBgCANoUA5K86dZKGDXOPb95saikAALQ1BCB/9uMfuz83bDC3DgAA2hgCkD+bONH9+fe/8zg8AAAtyNQAlJubqwkTJigmJkY2m03r169vsP2MGTNks9lqDQMGDPC0WblyZZ1tzp0718p70wr69ZP69nX/Jtjbb5tdDQAAbYapAejMmTNKSEjQ0qVLG9X+hRdeUHFxsWcoKipSly5d9NOf/tSrXUREhFe74uJihYaGtsYutL6as0CXCYcAAKDx2pv55ampqUpNTW10e4fDIYfD4Zlev369Tp06pfvvv9+rnc1mU1RUVIvVaapJk6Tf/17atEmqrJRCQsyuCACAgBfQ9wCtWLFCt99+u+Li4rzml5eXKy4uTj169ND48eNVUFDQ4HYqKirkcrm8Br+RlCRFR7vvAeIyGAAALSJgA1BxcbG2bNmiBx54wGt+v379tHLlSm3cuFGrV69WaGioRowYoUOHDtW7rczMTM/ZJYfDodjY2NYuv/GCgqR773WPv/GGubUAANBG2AzDMMwuQnJftsrOztakSZMa1T4zM1OLFi3S8ePHFdLAZaHq6moNHDhQo0aN0pIlS+psU1FRoYqKCs+0y+VSbGysSktLFRER0aT9aBUffSQNGiSFhkrffCP5Q00AAPgZl8slh8PRqL/fAXkGyDAMvfrqq0pLS2sw/EhSUFCQhgwZ0uAZILvdroiICK/BryQmup8IO3dOWrfO7GoAAAh4ARmAdu3apS+++EIzZ868bFvDMFRYWKjo6GgfVNZKbDbpF79wj7/+urm1AADQBpgagMrLy1VYWKjCwkJJ0pEjR1RYWKij//ztq4yMDE2bNq3WeitWrFBSUpLi4+NrLXvmmWe0detWHT58WIWFhZo5c6YKCwuVnp7eqvvS6u67z/35zjvS4cPm1gIAQIAzNQDt2bNHiYmJSkxMlCTNmzdPiYmJmj9/viT3jc5HL/kh0NLSUmVlZdV79uf06dOaNWuWbrjhBqWkpOjYsWPKzc3V0KFDW3dnWts110h33CEZhvTKK2ZXAwBAQPObm6D9SVNuovKp9eulu++WunWTiooku93sigAA8Btt/iZoyxo/XoqJkb79VsrONrsaAAACFgEokLRvL9W892jJEvflMAAA0GQEoECTnu6+9JWfL+Xmml0NAAABiQAUaKKjpZrfPsvMNLcWAAACFAEoEP2//ye1aydt3Srt3m12NQAABBwCUCC69lpp+nT3+Lx53AsEAEATEYAC1X/8h9Sxo/teoDVrzK4GAICAQgAKVNHRUkaGe3zuXKmkxNRyAAAIJASgQPbII1J8vDv8PPCAVF1tdkUAAAQEAlAgCw11/zhqSIj0t79J//wJEQAA0DACUKBLSJBeesk9/uyz0qJF5tYDAEAAIAC1BfffLz31lHv80UelOXOkigpzawIAwI8RgNqKp5+WnnvOPb5kiTR0qPTOO6aWBACAvyIAtSWPPSZt3Ch17Sp9/LE0ZoyUnCz96U/S0aNmVwcAgN+wGQZv0buUy+WSw+FQaWmpIiIizC6n6b79Vvrd79z3Bp0//8P8yEipd2/pmmuk7t3dQemqq9yfnTtL4eHew49+5H7jNAAAAaApf78JQHUI+ABUw+mUVq2S3npLKiiQqqqavo2wsLqD0eWmO3d2B67ISKlTJ8lma/HdAwDgYgSgK9RmAtDFysulgwelw4elL7+UTpyQTp50f544IZ06JZWVuduVlUkXLrTcd4eE/BCGoqJ++Lx4vOYzPJywBABoFgLQFWqTAagpDMP9FFlZmXcoqmu6vmXffSd9841UWtq07w4Jcf/ER1jYD0NwsDsUNXcICvLdsuZur65BurJpX63j6+9tiNnLqaFxy5vrSv5cNXddvrP11u3SRbrlluZ/bx2a8ve7fYt+M9oGm839ksXQUKlbtyvb1rlz7iD0zTfuS3IXj9dM14yXl0uVle7h1KmW2RcAgH9KTpby8kz7egIQWldoqBQX5x4u58wZ9+W477/3Hs6fd/8XhmG4f+6jZrypw+XWNXv5xYPUtOnmrOMv22jKOg0xezk1NG55TZsrPUtk9vr+UEOg70N8/JV99xUiAMF/dOzoHgAAaGW8BwgAAFgOAQgAAFgOAQgAAFgOAQgAAFgOAQgAAFgOAQgAAFgOAQgAAFgOAQgAAFgOAQgAAFgOAQgAAFgOAQgAAFgOAQgAAFiOqQEoNzdXEyZMUExMjGw2m9avX99g+507d8pms9UaPv/8c692WVlZ6t+/v+x2u/r376/s7OxW3AsAABBoTA1AZ86cUUJCgpYuXdqk9Q4ePKji4mLP0KdPH8+y/Px8TZ06VWlpadq3b5/S0tI0ZcoUffDBBy1dPgAACFA2wzAMs4uQJJvNpuzsbE2aNKneNjt37tTo0aN16tQpderUqc42U6dOlcvl0pYtWzzzxo0bp86dO2v16tV1rlNRUaGKigrPtMvlUmxsrEpLSxUREdGs/QEAAL7lcrnkcDga9fc7IO8BSkxMVHR0tMaMGaMdO3Z4LcvPz1dKSorXvDvuuEN5eXn1bi8zM1MOh8MzxMbGtkrdAADAPwRUAIqOjtby5cuVlZWldevWqW/fvhozZoxyc3M9bZxOpyIjI73Wi4yMlNPprHe7GRkZKi0t9QxFRUWttg8AAMB87c0uoCn69u2rvn37eqaTk5NVVFSkP/zhDxo1apRnvs1m81rPMIxa8y5mt9tlt9tbvmAAAOCXAuoMUF2GDRumQ4cOeaajoqJqne0pKSmpdVYIAABYV8AHoIKCAkVHR3umk5OTlZOT49Vm27ZtGj58uK9LAwAAfsrUS2Dl5eX64osvPNNHjhxRYWGhunTpop49eyojI0PHjh3TqlWrJEmLFy/WNddcowEDBqiyslKvv/66srKylJWV5dnGnDlzNGrUKC1cuFATJ07Uhg0btH37dr333ns+3z8AAOCfTA1Ae/bs0ejRoz3T8+bNkyRNnz5dK1euVHFxsY4ePepZXllZqUcffVTHjh1TWFiYBgwYoE2bNunOO+/0tBk+fLjWrFmjJ598Ur/97W/Vu3dvrV27VklJSb7bMQAA4Nf85j1A/qQp7xEAAAD+oc2/BwgAAOBKEIAAAIDlEIAAAIDlEIAAAIDlEIAAAIDlEIAAAIDlEIAAAIDlEIAAAIDlEIAAAIDlEIAAAIDlEIAAAIDlEIAAAIDlEIAAAIDlEIAAAIDlEIAAAIDlEIAAAIDlEIAAAIDlEIAAAIDlEIAAAIDlEIAAAIDlEIAAAIDlEIAAAIDlEIAAAIDlEIAAAIDlEIAAAIDlEIAAAIDlEIAAAIDlEIAAAIDlEIAAAIDlEIAAAIDlEIAAAIDlEIAAAIDlmBqAcnNzNWHCBMXExMhms2n9+vUNtl+3bp3Gjh2rbt26KSIiQsnJydq6datXm5UrV8pms9Uazp0714p7AgAAAompAejMmTNKSEjQ0qVLG9U+NzdXY8eO1ebNm7V3716NHj1aEyZMUEFBgVe7iIgIFRcXew2hoaGtsQsAACAAtTfzy1NTU5Wamtro9osXL/aa/s///E9t2LBBf/vb35SYmOiZb7PZFBUV1VJlAgCANiag7wGqrq5WWVmZunTp4jW/vLxccXFx6tGjh8aPH1/rDNGlKioq5HK5vAYAANB2BXQAWrRokc6cOaMpU6Z45vXr108rV67Uxo0btXr1aoWGhmrEiBE6dOhQvdvJzMyUw+HwDLGxsb4oHwAAmMRmGIZhdhGS+7JVdna2Jk2a1Kj2q1ev1gMPPKANGzbo9ttvr7dddXW1Bg4cqFGjRmnJkiV1tqmoqFBFRYVn2uVyKTY2VqWlpYqIiGjSfgAAAHO4XC45HI5G/f1u1j1ARUVFstls6tGjhyTpww8/1Jtvvqn+/ftr1qxZzdlkk6xdu1YzZ87UW2+91WD4kaSgoCANGTKkwTNAdrtddru9pcsEAAB+qlmXwH7+859rx44dkiSn06mxY8fqww8/1OOPP64FCxa0aIGXWr16tWbMmKE333xTd91112XbG4ahwsJCRUdHt2pdAAAgcDQrAH3yyScaOnSoJOkvf/mL4uPjlZeXpzfffFMrV65s9HbKy8tVWFiowsJCSdKRI0dUWFioo0ePSpIyMjI0bdo0T/vVq1dr2rRpWrRokYYNGyan0ymn06nS0lJPm2eeeUZbt27V4cOHVVhYqJkzZ6qwsFDp6enN2VUAANAGNSsAnT9/3nPJaPv27frxj38syX0DcnFxcaO3s2fPHiUmJnoeYZ83b54SExM1f/58SVJxcbEnDEnSyy+/rAsXLmj27NmKjo72DHPmzPG0OX36tGbNmqUbbrhBKSkpOnbsmHJzcz2BDQAAoFk3QSclJWn06NG66667lJKSot27dyshIUG7d+/WT37yE3399detUavPNOUmKgAA4B+a8ve7WWeAFi5cqJdfflm33nqrfvaznykhIUGStHHjRs60AAAAv9fsx+CrqqrkcrnUuXNnz7wvv/xSHTp0UPfu3VusQDNwBggAgMDT6meAvv/+e1VUVHjCz1dffaXFixfr4MGDAR9+AABA29esADRx4kStWrVKkvum46SkJC1atEiTJk3SsmXLWrRAAACAltasAPTRRx9p5MiRkqS//vWvioyM1FdffaVVq1bV+7ZlAAAAf9GsAHT27FmFh4dLkrZt26bJkycrKChIw4YN01dffdWiBQIAALS0ZgWg6667TuvXr1dRUZG2bt2qlJQUSVJJSQk3DQMAAL/XrAA0f/58Pfroo7rmmms0dOhQJScnS3KfDap5qSEAAIC/avZj8E6nU8XFxUpISFBQkDtHffjhh4qIiFC/fv1atEhf4zF4AAACT6v/GrwkRUVFKSoqSl9//bVsNpuuvvpqXoIIAAACQrMugVVXV2vBggVyOByKi4tTz5491alTJ/3ud79TdXV1S9cIAADQopp1BuiJJ57QihUr9Nxzz2nEiBEyDEPvv/++nn76aZ07d07PPvtsS9cJAADQYpp1D1BMTIxeeuklz6/A19iwYYN+/etf69ixYy1WoBm4BwgAgMDT6j+F8d1339V5o3O/fv303XffNWeTAAAAPtOsAJSQkKClS5fWmr906VLddNNNV1wUAABAa2rWPUC///3vddddd2n79u1KTk6WzWZTXl6eioqKtHnz5pauEQAAoEU16wzQLbfcov/7v//T3XffrdOnT+u7777T5MmT9emnn+q1115r6RoBAABaVLNfhFiXffv2aeDAgaqqqmqpTZqCm6ABAAg8rX4TNAAAQCAjAAEAAMshAAEAAMtp0lNgkydPbnD56dOnr6QWAAAAn2hSAHI4HJddPm3atCsqCAAAoLU1KQDxiDsAAGgLuAcIAABYDgEIAABYDgEIAABYDgEIAABYDgEIAABYDgEIAABYDgEIAABYDgEIAABYDgEIAABYjqkBKDc3VxMmTFBMTIxsNpvWr19/2XV27dqlQYMGKTQ0VNdee61eeumlWm2ysrLUv39/2e129e/fX9nZ2a1QPQAACFSmBqAzZ84oISFBS5cubVT7I0eO6M4779TIkSNVUFCgxx9/XL/5zW+UlZXlaZOfn6+pU6cqLS1N+/btU1pamqZMmaIPPvigtXYDAAAEGJthGIbZRUiSzWZTdna2Jk2aVG+bxx57TBs3btSBAwc889LT07Vv3z7l5+dLkqZOnSqXy6UtW7Z42owbN06dO3fW6tWrG1WLy+WSw+FQaWmpIiIimrdDAADAp5ry9zug7gHKz89XSkqK17w77rhDe/bs0fnz5xtsk5eXV+92Kyoq5HK5vAYAANB2BVQAcjqdioyM9JoXGRmpCxcu6MSJEw22cTqd9W43MzNTDofDM8TGxrZ88QAAwG8EVACS3JfKLlZzBe/i+XW1uXTexTIyMlRaWuoZioqKWrBiAADgb9qbXUBTREVF1TqTU1JSovbt2+uqq65qsM2lZ4UuZrfbZbfbW75gAADglwLqDFBycrJycnK85m3btk2DBw9WcHBwg22GDx/uszoBAIB/M/UMUHl5ub744gvP9JEjR1RYWKguXbqoZ8+eysjI0LFjx7Rq1SpJ7ie+li5dqnnz5ulXv/qV8vPztWLFCq+nu+bMmaNRo0Zp4cKFmjhxojZs2KDt27frvffe8/n+AQAA/2TqGaA9e/YoMTFRiYmJkqR58+YpMTFR8+fPlyQVFxfr6NGjnva9evXS5s2btXPnTv3Lv/yLfve732nJkiW65557PG2GDx+uNWvW6LXXXtNNN92klStXau3atUpKSvLtzgEAAL/lN+8B8ie8BwgAgMDTZt8DBAAA0BIIQAAAwHIIQAAAwHIIQAAAwHIIQAAAwHIIQAAAwHIIQAAAwHIIQAAAwHIIQAAAwHIIQAAAwHIIQAAAwHIIQAAAwHIIQAAAwHIIQAAAwHIIQAAAwHIIQAAAwHIIQAAAwHIIQAAAwHIIQAAAwHIIQAAAwHIIQAAAwHIIQAAAwHIIQAAAwHIIQAAAwHIIQAAAwHIIQAAAwHIIQAAAwHIIQAAAwHIIQAAAwHIIQAAAwHIIQAAAwHIIQAAAwHIIQAAAwHJMD0AvvviievXqpdDQUA0aNEjvvvtuvW1nzJghm81WaxgwYICnzcqVK+tsc+7cOV/sDgAACACmBqC1a9dq7ty5euKJJ1RQUKCRI0cqNTVVR48erbP9Cy+8oOLiYs9QVFSkLl266Kc//alXu4iICK92xcXFCg0N9cUuAQCAANDezC9//vnnNXPmTD3wwAOSpMWLF2vr1q1atmyZMjMza7V3OBxyOBye6fXr1+vUqVO6//77vdrZbDZFRUU1uo6KigpVVFR4pl0uV1N3BQAABBDTzgBVVlZq7969SklJ8ZqfkpKivLy8Rm1jxYoVuv322xUXF+c1v7y8XHFxcerRo4fGjx+vgoKCBreTmZnpCVcOh0OxsbFN2xkAABBQTAtAJ06cUFVVlSIjI73mR0ZGyul0Xnb94uJibdmyxXP2qEa/fv20cuVKbdy4UatXr1ZoaKhGjBihQ4cO1butjIwMlZaWeoaioqLm7RQAAAgIpl4Ck9yXqy5mGEateXVZuXKlOnXqpEmTJnnNHzZsmIYNG+aZHjFihAYOHKj//u//1pIlS+rclt1ul91ub3rxAAAgIJl2Bqhr165q165drbM9JSUltc4KXcowDL366qtKS0tTSEhIg22DgoI0ZMiQBs8AAQAAazEtAIWEhGjQoEHKycnxmp+Tk6Phw4c3uO6uXbv0xRdfaObMmZf9HsMwVFhYqOjo6CuqFwAAtB2mXgKbN2+e0tLSNHjwYCUnJ2v58uU6evSo0tPTJbnvzTl27JhWrVrltd6KFSuUlJSk+Pj4Wtt85plnNGzYMPXp00cul0tLlixRYWGh/vjHP/pknwAAgP8zNQBNnTpVJ0+e1IIFC1RcXKz4+Hht3rzZ81RXcXFxrXcClZaWKisrSy+88EKd2zx9+rRmzZolp9Mph8OhxMRE5ebmaujQoa2+PwAAIDDYDMMwzC7C37hcLjkcDpWWlioiIsLscgAAQCM05e+36T+FAQAA4GsEIAAAYDkEIAAAYDkEIAAAYDkEIAAAYDkEIAAAYDkEIAAAYDkEIAAAYDkEIAAAYDkEIAAAYDkEIAAAYDkEIAAAYDkEIAAAYDkEIAAAYDkEIAAAYDkEIAAAYDkEIAAAYDkEIAAAYDkEIAAAYDkEIAAAYDkEIAAAYDkEIAAAYDkEIAAAYDkEIAAAYDkEIAAAYDkEIAAAYDkEIAAAYDkEIB+qqpLuuUd6+22zKwEAwNoIQD706qvSunXSvfdKlZVmVwMAgHURgHzoF79wf5aWSsePm1sLAABWRgDyobAwqXdv93hRkbm1AABgZQQgH7v6avcnZ4AAADCP6QHoxRdfVK9evRQaGqpBgwbp3Xffrbftzp07ZbPZag2ff/65V7usrCz1799fdrtd/fv3V3Z2dmvvRqN16+b+PHHC3DoAALAyUwPQ2rVrNXfuXD3xxBMqKCjQyJEjlZqaqqNHjza43sGDB1VcXOwZ+vTp41mWn5+vqVOnKi0tTfv27VNaWpqmTJmiDz74oLV3p1G6dnV/EoAAADCPqQHo+eef18yZM/XAAw/ohhtu0OLFixUbG6tly5Y1uF737t0VFRXlGdq1a+dZtnjxYo0dO1YZGRnq16+fMjIyNGbMGC1evLiV96ZxCEAAAJjPtABUWVmpvXv3KiUlxWt+SkqK8vLyGlw3MTFR0dHRGjNmjHbs2OG1LD8/v9Y277jjjga3WVFRIZfL5TW0FgIQAADmMy0AnThxQlVVVYqMjPSaHxkZKafTWec60dHRWr58ubKysrRu3Tr17dtXY8aMUW5urqeN0+ls0jYlKTMzUw6HwzPExsZewZ41jAAEAID52ptdgM1m85o2DKPWvBp9+/ZV3759PdPJyckqKirSH/7wB40aNapZ25SkjIwMzZs3zzPtcrlaLQQRgAAAMJ9pZ4C6du2qdu3a1TozU1JSUusMTkOGDRumQ4cOeaajoqKavE273a6IiAivobUQgAAAMJ9pASgkJESDBg1STk6O1/ycnBwNHz680dspKChQdHS0Zzo5ObnWNrdt29akbbamiwOQYZhbCwAAVmXqJbB58+YpLS1NgwcPVnJyspYvX66jR48qPT1dkvvS1LFjx7Rq1SpJ7ie8rrnmGg0YMECVlZV6/fXXlZWVpaysLM8258yZo1GjRmnhwoWaOHGiNmzYoO3bt+u9994zZR8vVROAzp1z/yRGp06mlgMAgCWZGoCmTp2qkydPasGCBSouLlZ8fLw2b96suLg4SVJxcbHXO4EqKyv16KOP6tixYwoLC9OAAQO0adMm3XnnnZ42w4cP15o1a/Tkk0/qt7/9rXr37q21a9cqKSnJ5/tXlx/9SOrTRzp0SFq/Xpoxw+yKAACwHpthcCHmUi6XSw6HQ6Wlpa1yP1BmpvT449LQoZKfvJ8RAICA15S/36b/FIYVzZwphYRIH34oXeaVRwAAoBUQgEzQvbs0bZp7/N/+jZuhAQDwNQKQSZ5+WgoLk95/X7rkZdYAAKCVEYBMcvXV0vTp7vGXXza3FgAArIYAZKJ/Pu2v7Gzp5ElzawEAwEoIQCZKSHAP58+7H4kHAAC+QQAy2U9/6v586y1z6wAAwEoIQCarCUDbt3MZDAAAXyEAmez666WbbpKqqrgMBgCArxCA/MCUKe5PLoMBAOAbBCA/UHMZ7O9/5zIYAAC+QADyA9df734a7MIF6aIftgcAAK2EAOQnfv5z9+ebb5pbBwAAVkAA8hP33uv+3LVLKioytxYAANo6ApCf6NlTGjXKPb5mjbm1AADQ1hGA/AiXwQAA8A0CkB/5yU+k4GCpsFD67DOzqwEAoO0iAPmRq66Sxo1zj//5z+bWAgBAW0YA8jO//KX7c+VKqbLS1FIAAGizCEB+5q67pKgoqaRE+tvfzK4GAIC2iQDkZ4KDpfvvd48vX25uLQAAtFUEID80c6b7MydHOnLE3FoAAGiLCEB+qHdv6fbbJcOQXnzR7GoAAGh7CEB+au5c9+fLL0ulpaaWAgBAm0MA8lOpqVL//lJZmfTKK2ZXAwBA20IA8lNBQdIjj7jHFy/mkXgAAFoSAciP3XefFB0tHTvGWSAAAFoSAciP2e3Sk0+6xxcskMrLza0HAIC2ggDk5371K+m669wvRnz+ebOrAQCgbSAA+bngYOk//sM9vnAh7wUCAKAlEIACwJQp0q23SmfPSg8+6H4/EAAAaD4CUACw2dzvA7Lbpa1b3T+UCgAAmo8AFCCuv1566in3+OzZ0v795tYDAEAgMz0Avfjii+rVq5dCQ0M1aNAgvfvuu/W2XbduncaOHatu3bopIiJCycnJ2rp1q1eblStXymaz1RrOnTvX2rvS6v7t36SUFOn776XJk903RgMAgKYzNQCtXbtWc+fO1RNPPKGCggKNHDlSqampOnr0aJ3tc3NzNXbsWG3evFl79+7V6NGjNWHCBBUUFHi1i4iIUHFxsdcQGhrqi11qVe3aSW+8IcXFSV984Q5D331ndlUAAAQem2GYd0ttUlKSBg4cqGXLlnnm3XDDDZo0aZIyMzMbtY0BAwZo6tSpmj9/viT3GaC5c+fq9OnTza7L5XLJ4XCotLRUERERzd5Oazl0SBo5UvrmG/elsb/9zf0JAICVNeXvt2lngCorK7V3716lpKR4zU9JSVFeXl6jtlFdXa2ysjJ16dLFa355ebni4uLUo0cPjR8/vtYZoktVVFTI5XJ5Df6sTx/p73+XevaU/u//pIEDpRdekM6fN7syAAACg2kB6MSJE6qqqlJkZKTX/MjISDmdzkZtY9GiRTpz5oymTJnimdevXz+tXLlSGzdu1OrVqxUaGqoRI0bo0KFD9W4nMzNTDofDM8TGxjZvp3xowADpww+lUaOkM2fcvx5/3XXudwU1sKsAAEAmXgI7fvy4rr76auXl5Sk5Odkz/9lnn9X//M//6PPPP29w/dWrV+uBBx7Qhg0bdPvtt9fbrrq6WgMHDtSoUaO0ZMmSOttUVFSooqLCM+1yuRQbG+u3l8AuVl0tLV8uzZ8vffvtD/NjY6XBg6UbbpB69JCuvlrq1EkKD5ciItxDx47uR+vbtTOtfAAAWkxTLoG191FNtXTt2lXt2rWrdbanpKSk1lmhS61du1YzZ87UW2+91WD4kaSgoCANGTKkwTNAdrtddru98cX7kaAgKT1dmj7dfYP0X/4i7dghFRW5h+zsy28jOFgKDZXCwtyfNUND0yEhPwx2e8PTzZkXZPrziQCAtsy0ABQSEqJBgwYpJydHd999t2d+Tk6OJk6cWO96q1ev1i9/+UutXr1ad91112W/xzAMFRYW6sYbb2yRuv1VWJj0wAPuoaxMKiiQ9uyRDh+Wvv5aOn5ccrncQ1mZ9w+rnj/vHsrKzKv/Uu3aNS082e3uINe+vXvddu1ad7yp69UMQUG1h/rmN7TcZjP7XwgAAptpAUiS5s2bp7S0NA0ePFjJyclavny5jh49qvT0dElSRkaGjh07plWrVklyh59p06bphRde0LBhwzxnj8LCwuRwOCRJzzzzjIYNG6Y+ffrI5XJpyZIlKiws1B//+EdzdtIE4eHue4NGjaq/TVWVdO6c+51C587VHr/cdGWl91BR0fD05dpcuFC7vu+/dw+o25UEqOaGMJvth6Gp081ZpzW20dLblLw/Gzuvqe3ZbsPbuFh9/4HQ2Lasf2XrN7at3S5FRdW9vi+YGoCmTp2qkydPasGCBSouLlZ8fLw2b96suLg4SVJxcbHXO4FefvllXbhwQbNnz9bs2bM986dPn66V//x9iNOnT2vWrFlyOp1yOBxKTExUbm6uhg4d6tN983ft2rnvAerY0exK3KqrawemxgSrS6erqtxhqqrKe7yueb4er66ue6hZ1tS78WrWvzQ8AkAgSE6WGvnQd6sw9T1A/srf3wOEtskw3MPlglJDQ2PaNLddVdUPNV5ca1Omm7OOL7bZ3G3U/Ltd/O/HvJaZ19h1LlXfX7SmzPf1Nqxac1KS+57VlhQQN0ED8Hbx5RUAQOvi/2oBAIDlEIAAAIDlEIAAAIDlEIAAAIDlEIAAAIDlEIAAAIDlEIAAAIDlEIAAAIDlEIAAAIDlEIAAAIDlEIAAAIDlEIAAAIDlEIAAAIDlEIAAAIDltDe7AH9kGIYkyeVymVwJAABorJq/2zV/xxtCAKpDWVmZJCk2NtbkSgAAQFOVlZXJ4XA02MZmNCYmWUx1dbWOHz+u8PBw2Wy2Ft22y+VSbGysioqKFBER0aLbbmvoq8ajrxqPvmoa+qvx6KvGa62+MgxDZWVliomJUVBQw3f5cAaoDkFBQerRo0erfkdERAT/A2kk+qrx6KvGo6+ahv5qPPqq8Vqjry535qcGN0EDAADLIQABAADLIQD5mN1u11NPPSW73W52KX6Pvmo8+qrx6Kumob8aj75qPH/oK26CBgAAlsMZIAAAYDkEIAAAYDkEIAAAYDkEIAAAYDkEIB968cUX1atXL4WGhmrQoEF69913zS7J555++mnZbDavISoqyrPcMAw9/fTTiomJUVhYmG699VZ9+umnXtuoqKjQww8/rK5du6pjx4768Y9/rK+//trXu9LicnNzNWHCBMXExMhms2n9+vVey1uqb06dOqW0tDQ5HA45HA6lpaXp9OnTrbx3LetyfTVjxoxax9mwYcO82lilrzIzMzVkyBCFh4ere/fumjRpkg4ePOjVhmPLrTF9xbHltmzZMt10002eFxkmJydry5YtnuUBcUwZ8Ik1a9YYwcHBxiuvvGJ89tlnxpw5c4yOHTsaX331ldml+dRTTz1lDBgwwCguLvYMJSUlnuXPPfecER4ebmRlZRn79+83pk6dakRHRxsul8vTJj093bj66quNnJwc46OPPjJGjx5tJCQkGBcuXDBjl1rM5s2bjSeeeMLIysoyJBnZ2dley1uqb8aNG2fEx8cbeXl5Rl5enhEfH2+MHz/eV7vZIi7XV9OnTzfGjRvndZydPHnSq41V+uqOO+4wXnvtNeOTTz4xCgsLjbvuusvo2bOnUV5e7mnDseXWmL7i2HLbuHGjsWnTJuPgwYPGwYMHjccff9wIDg42PvnkE8MwAuOYIgD5yNChQ4309HSvef369TP+/d//3aSKzPHUU08ZCQkJdS6rrq42oqKijOeee84z79y5c4bD4TBeeuklwzAM4/Tp00ZwcLCxZs0aT5tjx44ZQUFBxttvv92qtfvSpX/UW6pvPvvsM0OSsXv3bk+b/Px8Q5Lx+eeft/JetY76AtDEiRPrXceqfWUYhlFSUmJIMnbt2mUYBsdWQy7tK8Pg2GpI586djT/96U8Bc0xxCcwHKisrtXfvXqWkpHjNT0lJUV5enklVmefQoUOKiYlRr169dO+99+rw4cOSpCNHjsjpdHr1k91u1y233OLpp7179+r8+fNebWJiYhQfH9+m+7Kl+iY/P18Oh0NJSUmeNsOGDZPD4Whz/bdz5051795d119/vX71q1+ppKTEs8zKfVVaWipJ6tKliySOrYZc2lc1OLa8VVVVac2aNTpz5oySk5MD5pgiAPnAiRMnVFVVpcjISK/5kZGRcjqdJlVljqSkJK1atUpbt27VK6+8IqfTqeHDh+vkyZOevmion5xOp0JCQtS5c+d627RFLdU3TqdT3bt3r7X97t27t6n+S01N1RtvvKF33nlHixYt0j/+8Q/ddtttqqiokGTdvjIMQ/PmzdPNN9+s+Ph4SRxb9amrrySOrYvt379fP/rRj2S325Wenq7s7Gz1798/YI4pfg3eh2w2m9e0YRi15rV1qampnvEbb7xRycnJ6t27t/785z97biRsTj9ZpS9bom/qat/W+m/q1Kme8fj4eA0ePFhxcXHatGmTJk+eXO96bb2vHnroIX388cd67733ai3j2PJWX19xbP2gb9++Kiws1OnTp5WVlaXp06dr165dnuX+fkxxBsgHunbtqnbt2tVKrCUlJbUSstV07NhRN954ow4dOuR5GqyhfoqKilJlZaVOnTpVb5u2qKX6JioqSt98802t7X/77bdtuv+io6MVFxenQ4cOSbJmXz388MPauHGjduzYoR49enjmc2zVVl9f1cXKx1ZISIiuu+46DR48WJmZmUpISNALL7wQMMcUAcgHQkJCNGjQIOXk5HjNz8nJ0fDhw02qyj9UVFTowIEDio6OVq9evRQVFeXVT5WVldq1a5ennwYNGqTg4GCvNsXFxfrkk0/adF+2VN8kJyertLRUH374oafNBx98oNLS0jbdfydPnlRRUZGio6MlWauvDMPQQw89pHXr1umdd95Rr169vJZzbP3gcn1VFysfW5cyDEMVFRWBc0xd8W3UaJSax+BXrFhhfPbZZ8bcuXONjh07Gl9++aXZpfnUI488YuzcudM4fPiwsXv3bmP8+PFGeHi4px+ee+45w+FwGOvWrTP2799v/OxnP6vz0ckePXoY27dvNz766CPjtttuaxOPwZeVlRkFBQVGQUGBIcl4/vnnjYKCAs+rElqqb8aNG2fcdNNNRn5+vpGfn2/ceOONAfX4rWE03FdlZWXGI488YuTl5RlHjhwxduzYYSQnJxtXX321JfvqwQcfNBwOh7Fz506vR7fPnj3racOx5Xa5vuLY+kFGRoaRm5trHDlyxPj444+Nxx9/3AgKCjK2bdtmGEZgHFMEIB/64x//aMTFxRkhISHGwIEDvR6ttIqad0EEBwcbMTExxuTJk41PP/3Us7y6utp46qmnjKioKMNutxujRo0y9u/f77WN77//3njooYeMLl26GGFhYcb48eONo0eP+npXWtyOHTsMSbWG6dOnG4bRcn1z8uRJ47777jPCw8ON8PBw47777jNOnTrlo71sGQ311dmzZ42UlBSjW7duRnBwsNGzZ09j+vTptfrBKn1VVz9JMl577TVPG44tt8v1FcfWD375y196/p5169bNGDNmjCf8GEZgHFM2wzCMKz+PBAAAEDi4BwgAAFgOAQgAAFgOAQgAAFgOAQgAAFgOAQgAAFgOAQgAAFgOAQgAAFgOAQgAAFgOAQgA6mGz2bR+/XqzywDQCghAAPzSjBkzZLPZag3jxo0zuzQAbUB7swsAgPqMGzdOr732mtc8u91uUjUA2hLOAAHwW3a7XVFRUV5D586dJbkvTy1btkypqakKCwtTr1699NZbb3mtv3//ft12220KCwvTVVddpVmzZqm8vNyrzauvvqoBAwbIbrcrOjpaDz30kNfyEydO6O6771aHDh3Up08fbdy40bPs1KlTuu+++9StWzeFhYWpT58+tQIbAP9EAAIQsH7729/qnnvu0b59+/SLX/xCP/vZz3TgwAFJ0tmzZzVu3Dh17txZ//jHP/TWW29p+/btXgFn2bJlmj17tmbNmqX9+/dr48aNuu6667y+45lnntGUKVP08ccf684779R9992n7777zvP9n332mbZs2aIDBw5o2bJl6tq1q+86AEDztchvygNAC5s+fbrRrl07o2PHjl7DggULDMMwDElGenq61zpJSUnGgw8+aBiGYSxfvtzo3LmzUV5e7lm+adMmIygoyHA6nYZhGEZMTIzxxBNP1FuDJOPJJ5/0TJeXlxs2m83YsmWLYRiGMWHCBOP+++9vmR0G4FPcAwTAb40ePVrLli3zmtelSxfPeHJystey5ORkFRYWSpIOHDighIQEdezY0bN8xIgRqq6u1sGDB2Wz2XT8+HGNGTOmwRpuuukmz3jHjh0VHh6ukpISSdKDDz6oe+65Rx999JFSUlI0adIkDR8+vFn7CsC3CEAA/FbHjh1rXZK6HJvNJkkyDMMzXlebsLCwRm0vODi41rrV1dWSpNTUVH311VfatGmTtm/frjFjxmj27Nn6wx/+0KSaAfge9wABCFi7d++uNd2vXz9JUv/+/VVYWKgzZ854lr///vsKCgrS9ddfr/DwcF1zzTX6+9//fkU1dOvWTTNmzNDrr7+uxYsXa/ny5Ve0PQC+wRkgAH6roqJCTqfTa1779u09Nxq/9dZbGjx4sG6++Wa98cYb+vDDD7VixQpJ0n333aennnpK06dP19NPP61vv/1WDz/8sNLS0hQZGSlJevrpp5Wenq7u3bsrNTVVZWVlev/99/Xwww83qr758+dr0KBBGjBggCoqKvS///u/uuGGG1qwBwC0FgIQAL/19ttvKzo62mte37599fnnn0tyP6G1Zs0a/frXv1ZUVJTeeOMN9e/fX5LUoUMHbd26VXPmzNGQIUPUoUMH3XPPPXr++ec925o+fbrOnTun//qv/9Kjjz6qrl276ic/+Umj6wsJCVFGRoa+/PJLhYWFaeTIkVqzZk0L7DmA1mYzDMMwuwgAaCqbzabs7GxNmjTJ7FIABCDuAQIAAJZDAAIAAJbDPUAAAhJX7wFcCc4AAQAAyyEAAQAAyyEAAQAAyyEAAQAAyyEAAQAAyyEAAQAAyyEAAQAAyyEAAQAAy/n/jpkl6dAMIhwAAAAASUVORK5CYII=",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "\n",
    "plt.plot(range(len(train_LOSS)), train_LOSS, color = 'blue')\n",
    "plt.plot(range(len(valid_LOSS)), valid_LOSS, color = 'red')\n",
    "\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "\n",
    "img = Image.open('../../dataset/train/0.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "img_np = np.array(img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(60, 160, 3)"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "img_np.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([128, 128])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = torch.rand((128,128))\n",
    "a.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "16384"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "128 * 128"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ True,  True, False,  ...,  True, False,  True],\n",
       "        [False,  True, False,  ..., False,  True, False],\n",
       "        [False, False, False,  ...,  True, False,  True],\n",
       "        ...,\n",
       "        [ True, False, False,  ..., False, False, False],\n",
       "        [ True, False, False,  ..., False, False,  True],\n",
       "        [ True,  True,  True,  ..., False,  True,  True]])"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mean = torch.mean(a)\n",
    "std = torch.std(a)\n",
    "\n",
    "outlier_upper = mean + 1 * std\n",
    "outlier_down = mean - 1 * std\n",
    "\n",
    "(a < outlier_upper) & (a > outlier_down)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(9468)"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.sum(((a < outlier_upper) & (a > outlier_down)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1., 0., 0., 0., 0., 0., 0., 0.],\n",
       "        [1., 1., 0., 0., 0., 0., 0., 0.],\n",
       "        [1., 1., 1., 0., 0., 0., 0., 0.],\n",
       "        [1., 1., 1., 1., 0., 0., 0., 0.],\n",
       "        [1., 1., 1., 1., 1., 0., 0., 0.],\n",
       "        [1., 1., 1., 1., 1., 1., 0., 0.],\n",
       "        [1., 1., 1., 1., 1., 1., 1., 0.],\n",
       "        [1., 1., 1., 1., 1., 1., 1., 1.]])"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mask = torch.ones((8,8))\n",
    "causal_mask = torch.tril(mask)\n",
    "# causal_mask[:8, :8] = float('-inf')\n",
    "causal_mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "causal_mask = torch.where(causal_mask == 0, float('-inf'), causal_mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1., -inf, -inf, -inf, -inf, -inf, -inf, -inf],\n",
       "        [1., 1., -inf, -inf, -inf, -inf, -inf, -inf],\n",
       "        [1., 1., 1., -inf, -inf, -inf, -inf, -inf],\n",
       "        [1., 1., 1., 1., -inf, -inf, -inf, -inf],\n",
       "        [1., 1., 1., 1., 1., -inf, -inf, -inf],\n",
       "        [1., 1., 1., 1., 1., 1., -inf, -inf],\n",
       "        [1., 1., 1., 1., 1., 1., 1., -inf],\n",
       "        [1., 1., 1., 1., 1., 1., 1., 1.]])"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "causal_mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([64, 8, 8])"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "c = torch.rand((64, 8,8))\n",
    "c.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.3464,   -inf,   -inf,   -inf,   -inf,   -inf,   -inf,   -inf],\n",
       "        [0.7495, 0.4476,   -inf,   -inf,   -inf,   -inf,   -inf,   -inf],\n",
       "        [0.5532, 0.8396, 0.1314,   -inf,   -inf,   -inf,   -inf,   -inf],\n",
       "        [0.9974, 0.1617, 0.9666, 0.8226,   -inf,   -inf,   -inf,   -inf],\n",
       "        [0.2984, 0.0710, 0.8118, 0.5321, 0.2587,   -inf,   -inf,   -inf],\n",
       "        [0.2342, 0.8133, 0.4754, 0.8830, 0.8276, 0.8598,   -inf,   -inf],\n",
       "        [0.9826, 0.2597, 0.8838, 0.6017, 0.3874, 0.0721, 0.6209,   -inf],\n",
       "        [0.3924, 0.4582, 0.3526, 0.3782, 0.0634, 0.5838, 0.8380, 0.0447]])"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "d = c * causal_mask\n",
    "d[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[1.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
       "         [0.5749, 0.4251, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
       "         [0.3347, 0.4457, 0.2195,  ..., 0.0000, 0.0000, 0.0000],\n",
       "         ...,\n",
       "         [0.1036, 0.1849, 0.1319,  ..., 0.1937, 0.0000, 0.0000],\n",
       "         [0.2117, 0.1027, 0.1917,  ..., 0.0851, 0.1474, 0.0000],\n",
       "         [0.1218, 0.1300, 0.1170,  ..., 0.1474, 0.1901, 0.0860]],\n",
       "\n",
       "        [[1.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
       "         [0.2974, 0.7026, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
       "         [0.2228, 0.4092, 0.3680,  ..., 0.0000, 0.0000, 0.0000],\n",
       "         ...,\n",
       "         [0.1692, 0.1492, 0.2031,  ..., 0.2450, 0.0000, 0.0000],\n",
       "         [0.1409, 0.1585, 0.1437,  ..., 0.1256, 0.1026, 0.0000],\n",
       "         [0.1142, 0.1420, 0.0898,  ..., 0.1139, 0.1397, 0.1735]],\n",
       "\n",
       "        [[1.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
       "         [0.2951, 0.7049, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
       "         [0.3815, 0.2521, 0.3664,  ..., 0.0000, 0.0000, 0.0000],\n",
       "         ...,\n",
       "         [0.1765, 0.1265, 0.1265,  ..., 0.1358, 0.0000, 0.0000],\n",
       "         [0.1407, 0.1524, 0.0958,  ..., 0.2129, 0.2050, 0.0000],\n",
       "         [0.1368, 0.1243, 0.0885,  ..., 0.1596, 0.1479, 0.1569]],\n",
       "\n",
       "        ...,\n",
       "\n",
       "        [[1.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
       "         [0.4370, 0.5630, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
       "         [0.3700, 0.2692, 0.3609,  ..., 0.0000, 0.0000, 0.0000],\n",
       "         ...,\n",
       "         [0.2311, 0.1008, 0.1682,  ..., 0.1736, 0.0000, 0.0000],\n",
       "         [0.1258, 0.1241, 0.0958,  ..., 0.1694, 0.1547, 0.0000],\n",
       "         [0.1218, 0.1806, 0.0926,  ..., 0.0790, 0.1656, 0.0736]],\n",
       "\n",
       "        [[1.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
       "         [0.7167, 0.2833, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
       "         [0.3124, 0.4089, 0.2787,  ..., 0.0000, 0.0000, 0.0000],\n",
       "         ...,\n",
       "         [0.2044, 0.1308, 0.1398,  ..., 0.2333, 0.0000, 0.0000],\n",
       "         [0.1035, 0.1106, 0.1177,  ..., 0.1569, 0.1851, 0.0000],\n",
       "         [0.0804, 0.1777, 0.1394,  ..., 0.1025, 0.1220, 0.1192]],\n",
       "\n",
       "        [[1.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
       "         [0.5150, 0.4850, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
       "         [0.4941, 0.2708, 0.2351,  ..., 0.0000, 0.0000, 0.0000],\n",
       "         ...,\n",
       "         [0.1494, 0.1258, 0.1718,  ..., 0.2397, 0.0000, 0.0000],\n",
       "         [0.1049, 0.1863, 0.2322,  ..., 0.1065, 0.1342, 0.0000],\n",
       "         [0.1608, 0.0742, 0.0897,  ..., 0.1379, 0.1752, 0.1230]]])"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "e = torch.softmax(d, dim=-1)\n",
    "e"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0, 1])"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.arange(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nn.Parameter(torch.rand())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Vit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
