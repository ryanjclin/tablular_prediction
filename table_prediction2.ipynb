{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import numpy as np\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch.nn as nn\n",
    "from tqdm import tqdm\n",
    "from torch.nn import functional as F\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from torch.optim.lr_scheduler import LambdaLR"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"./Students_Grading_Dataset.csv\")\n",
    "# df\n",
    "\n",
    "# for att in df:\n",
    "#     print(f\"{att}: {df[att][0]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Manually select Cols (attribute)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Gender</th>\n",
       "      <th>Age</th>\n",
       "      <th>Department</th>\n",
       "      <th>Attendance (%)</th>\n",
       "      <th>Midterm_Score</th>\n",
       "      <th>Final_Score</th>\n",
       "      <th>Assignments_Avg</th>\n",
       "      <th>Quizzes_Avg</th>\n",
       "      <th>Participation_Score</th>\n",
       "      <th>Projects_Score</th>\n",
       "      <th>Total_Score</th>\n",
       "      <th>Grade</th>\n",
       "      <th>Study_Hours_per_Week</th>\n",
       "      <th>Extracurricular_Activities</th>\n",
       "      <th>Internet_Access_at_Home</th>\n",
       "      <th>Parent_Education_Level</th>\n",
       "      <th>Family_Income_Level</th>\n",
       "      <th>Stress_Level (1-10)</th>\n",
       "      <th>Sleep_Hours_per_Night</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Female</td>\n",
       "      <td>22</td>\n",
       "      <td>Engineering</td>\n",
       "      <td>52.29</td>\n",
       "      <td>55.03</td>\n",
       "      <td>57.82</td>\n",
       "      <td>84.22</td>\n",
       "      <td>74.06</td>\n",
       "      <td>3.99</td>\n",
       "      <td>85.90</td>\n",
       "      <td>56.09</td>\n",
       "      <td>F</td>\n",
       "      <td>6.2</td>\n",
       "      <td>No</td>\n",
       "      <td>Yes</td>\n",
       "      <td>High School</td>\n",
       "      <td>Medium</td>\n",
       "      <td>5</td>\n",
       "      <td>4.7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Male</td>\n",
       "      <td>18</td>\n",
       "      <td>Engineering</td>\n",
       "      <td>97.27</td>\n",
       "      <td>97.23</td>\n",
       "      <td>45.80</td>\n",
       "      <td>NaN</td>\n",
       "      <td>94.24</td>\n",
       "      <td>8.32</td>\n",
       "      <td>55.65</td>\n",
       "      <td>50.64</td>\n",
       "      <td>A</td>\n",
       "      <td>19.0</td>\n",
       "      <td>No</td>\n",
       "      <td>Yes</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Medium</td>\n",
       "      <td>4</td>\n",
       "      <td>9.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Male</td>\n",
       "      <td>24</td>\n",
       "      <td>Business</td>\n",
       "      <td>57.19</td>\n",
       "      <td>67.05</td>\n",
       "      <td>93.68</td>\n",
       "      <td>67.70</td>\n",
       "      <td>85.70</td>\n",
       "      <td>5.05</td>\n",
       "      <td>73.79</td>\n",
       "      <td>70.30</td>\n",
       "      <td>D</td>\n",
       "      <td>20.7</td>\n",
       "      <td>No</td>\n",
       "      <td>Yes</td>\n",
       "      <td>Master's</td>\n",
       "      <td>Low</td>\n",
       "      <td>6</td>\n",
       "      <td>6.2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Female</td>\n",
       "      <td>24</td>\n",
       "      <td>Mathematics</td>\n",
       "      <td>95.15</td>\n",
       "      <td>47.79</td>\n",
       "      <td>80.63</td>\n",
       "      <td>66.06</td>\n",
       "      <td>93.51</td>\n",
       "      <td>6.54</td>\n",
       "      <td>92.12</td>\n",
       "      <td>61.63</td>\n",
       "      <td>A</td>\n",
       "      <td>24.8</td>\n",
       "      <td>Yes</td>\n",
       "      <td>Yes</td>\n",
       "      <td>High School</td>\n",
       "      <td>High</td>\n",
       "      <td>3</td>\n",
       "      <td>6.7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Female</td>\n",
       "      <td>23</td>\n",
       "      <td>CS</td>\n",
       "      <td>54.18</td>\n",
       "      <td>46.59</td>\n",
       "      <td>78.89</td>\n",
       "      <td>96.85</td>\n",
       "      <td>83.70</td>\n",
       "      <td>5.97</td>\n",
       "      <td>68.42</td>\n",
       "      <td>66.13</td>\n",
       "      <td>F</td>\n",
       "      <td>15.4</td>\n",
       "      <td>Yes</td>\n",
       "      <td>Yes</td>\n",
       "      <td>High School</td>\n",
       "      <td>High</td>\n",
       "      <td>2</td>\n",
       "      <td>7.1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4995</th>\n",
       "      <td>Male</td>\n",
       "      <td>19</td>\n",
       "      <td>Business</td>\n",
       "      <td>NaN</td>\n",
       "      <td>82.15</td>\n",
       "      <td>60.33</td>\n",
       "      <td>80.09</td>\n",
       "      <td>99.32</td>\n",
       "      <td>5.00</td>\n",
       "      <td>58.42</td>\n",
       "      <td>85.21</td>\n",
       "      <td>D</td>\n",
       "      <td>25.5</td>\n",
       "      <td>No</td>\n",
       "      <td>Yes</td>\n",
       "      <td>High School</td>\n",
       "      <td>Low</td>\n",
       "      <td>10</td>\n",
       "      <td>8.3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4996</th>\n",
       "      <td>Male</td>\n",
       "      <td>19</td>\n",
       "      <td>Business</td>\n",
       "      <td>65.11</td>\n",
       "      <td>86.31</td>\n",
       "      <td>49.80</td>\n",
       "      <td>NaN</td>\n",
       "      <td>88.08</td>\n",
       "      <td>2.79</td>\n",
       "      <td>60.87</td>\n",
       "      <td>95.96</td>\n",
       "      <td>C</td>\n",
       "      <td>5.0</td>\n",
       "      <td>No</td>\n",
       "      <td>Yes</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Medium</td>\n",
       "      <td>4</td>\n",
       "      <td>4.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4997</th>\n",
       "      <td>Female</td>\n",
       "      <td>24</td>\n",
       "      <td>CS</td>\n",
       "      <td>87.54</td>\n",
       "      <td>63.55</td>\n",
       "      <td>64.21</td>\n",
       "      <td>94.28</td>\n",
       "      <td>50.19</td>\n",
       "      <td>3.13</td>\n",
       "      <td>82.65</td>\n",
       "      <td>54.25</td>\n",
       "      <td>A</td>\n",
       "      <td>24.8</td>\n",
       "      <td>Yes</td>\n",
       "      <td>No</td>\n",
       "      <td>High School</td>\n",
       "      <td>Medium</td>\n",
       "      <td>4</td>\n",
       "      <td>6.3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4998</th>\n",
       "      <td>Male</td>\n",
       "      <td>23</td>\n",
       "      <td>CS</td>\n",
       "      <td>92.56</td>\n",
       "      <td>79.79</td>\n",
       "      <td>94.28</td>\n",
       "      <td>81.20</td>\n",
       "      <td>61.18</td>\n",
       "      <td>0.40</td>\n",
       "      <td>94.29</td>\n",
       "      <td>55.84</td>\n",
       "      <td>A</td>\n",
       "      <td>16.1</td>\n",
       "      <td>Yes</td>\n",
       "      <td>Yes</td>\n",
       "      <td>Bachelor's</td>\n",
       "      <td>Low</td>\n",
       "      <td>1</td>\n",
       "      <td>8.4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4999</th>\n",
       "      <td>Female</td>\n",
       "      <td>21</td>\n",
       "      <td>Engineering</td>\n",
       "      <td>83.92</td>\n",
       "      <td>83.24</td>\n",
       "      <td>53.47</td>\n",
       "      <td>51.76</td>\n",
       "      <td>83.51</td>\n",
       "      <td>0.49</td>\n",
       "      <td>69.25</td>\n",
       "      <td>77.86</td>\n",
       "      <td>F</td>\n",
       "      <td>29.2</td>\n",
       "      <td>No</td>\n",
       "      <td>Yes</td>\n",
       "      <td>PhD</td>\n",
       "      <td>Low</td>\n",
       "      <td>2</td>\n",
       "      <td>6.1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5000 rows × 19 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      Gender  Age   Department  Attendance (%)  Midterm_Score  Final_Score  \\\n",
       "0     Female   22  Engineering           52.29          55.03        57.82   \n",
       "1       Male   18  Engineering           97.27          97.23        45.80   \n",
       "2       Male   24     Business           57.19          67.05        93.68   \n",
       "3     Female   24  Mathematics           95.15          47.79        80.63   \n",
       "4     Female   23           CS           54.18          46.59        78.89   \n",
       "...      ...  ...          ...             ...            ...          ...   \n",
       "4995    Male   19     Business             NaN          82.15        60.33   \n",
       "4996    Male   19     Business           65.11          86.31        49.80   \n",
       "4997  Female   24           CS           87.54          63.55        64.21   \n",
       "4998    Male   23           CS           92.56          79.79        94.28   \n",
       "4999  Female   21  Engineering           83.92          83.24        53.47   \n",
       "\n",
       "      Assignments_Avg  Quizzes_Avg  Participation_Score  Projects_Score  \\\n",
       "0               84.22        74.06                 3.99           85.90   \n",
       "1                 NaN        94.24                 8.32           55.65   \n",
       "2               67.70        85.70                 5.05           73.79   \n",
       "3               66.06        93.51                 6.54           92.12   \n",
       "4               96.85        83.70                 5.97           68.42   \n",
       "...               ...          ...                  ...             ...   \n",
       "4995            80.09        99.32                 5.00           58.42   \n",
       "4996              NaN        88.08                 2.79           60.87   \n",
       "4997            94.28        50.19                 3.13           82.65   \n",
       "4998            81.20        61.18                 0.40           94.29   \n",
       "4999            51.76        83.51                 0.49           69.25   \n",
       "\n",
       "      Total_Score Grade  Study_Hours_per_Week Extracurricular_Activities  \\\n",
       "0           56.09     F                   6.2                         No   \n",
       "1           50.64     A                  19.0                         No   \n",
       "2           70.30     D                  20.7                         No   \n",
       "3           61.63     A                  24.8                        Yes   \n",
       "4           66.13     F                  15.4                        Yes   \n",
       "...           ...   ...                   ...                        ...   \n",
       "4995        85.21     D                  25.5                         No   \n",
       "4996        95.96     C                   5.0                         No   \n",
       "4997        54.25     A                  24.8                        Yes   \n",
       "4998        55.84     A                  16.1                        Yes   \n",
       "4999        77.86     F                  29.2                         No   \n",
       "\n",
       "     Internet_Access_at_Home Parent_Education_Level Family_Income_Level  \\\n",
       "0                        Yes            High School              Medium   \n",
       "1                        Yes                    NaN              Medium   \n",
       "2                        Yes               Master's                 Low   \n",
       "3                        Yes            High School                High   \n",
       "4                        Yes            High School                High   \n",
       "...                      ...                    ...                 ...   \n",
       "4995                     Yes            High School                 Low   \n",
       "4996                     Yes                    NaN              Medium   \n",
       "4997                      No            High School              Medium   \n",
       "4998                     Yes             Bachelor's                 Low   \n",
       "4999                     Yes                    PhD                 Low   \n",
       "\n",
       "      Stress_Level (1-10)  Sleep_Hours_per_Night  \n",
       "0                       5                    4.7  \n",
       "1                       4                    9.0  \n",
       "2                       6                    6.2  \n",
       "3                       3                    6.7  \n",
       "4                       2                    7.1  \n",
       "...                   ...                    ...  \n",
       "4995                   10                    8.3  \n",
       "4996                    4                    4.0  \n",
       "4997                    4                    6.3  \n",
       "4998                    1                    8.4  \n",
       "4999                    2                    6.1  \n",
       "\n",
       "[5000 rows x 19 columns]"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "unimportant_attribute = ['Student_ID', 'First_Name', 'Last_Name', 'Email']\n",
    "\n",
    "filtered_df = df.drop(unimportant_attribute, axis=1)\n",
    "filtered_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "category_vars = ['Gender', 'Department', 'Grade', 'Extracurricular_Activities', 'Internet_Access_at_Home', 'Parent_Education_Level', 'Family_Income_Level']\n",
    "numerical_score_vars = ['Attendance (%)', 'Midterm_Score', 'Final_Score', 'Assignments_Avg', 'Quizzes_Avg', 'Participation_Score', 'Projects_Score', 'Total_Score', 'Stress_Level (1-10)']\n",
    "numerical_scalar_vars = list(set(filtered_df.columns) - set(category_vars) - set(numerical_score_vars))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Separate rows with-Nan and without-Nan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "row with Nan: (2419, 19)\n",
      "row without Nan: (2581, 19)\n"
     ]
    }
   ],
   "source": [
    "nan_rows = filtered_df.isna().any(axis=1)\n",
    "\n",
    "# Nan rows\n",
    "df_nan = filtered_df[nan_rows]\n",
    "print(f\"row with Nan: {df_nan.shape}\")\n",
    "# Complete rows\n",
    "df_complete = filtered_df[~nan_rows]\n",
    "print(f\"row without Nan: {df_complete.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "df_train: (1806, 19)\n",
      "df_valid: (775, 19)\n"
     ]
    }
   ],
   "source": [
    "# split df_complete into train/valid\n",
    "# data_amount = int(len(df_complete) * 0.8)\n",
    "# df_train = df_complete.iloc[:data_amount, :]\n",
    "# df_valid = df_complete.iloc[data_amount:, :]\n",
    "\n",
    "df_train, df_valid, _, _ = train_test_split(df_complete, df_complete, test_size=0.3, random_state=0)\n",
    "\n",
    "print(f\"df_train: {df_train.shape}\")\n",
    "print(f\"df_valid: {df_valid.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing: \n",
    "1. category to numerical\n",
    "2. max-min norm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def category_to_numerical(data):\n",
    "    le = LabelEncoder()\n",
    "    le.fit(data)\n",
    "    num_data = le.transform(data)\n",
    "    \n",
    "    return num_data, le\n",
    "\n",
    "# def max_min_norm(data, train_params = None, process_type = 'train'):\n",
    "    \n",
    "#     if process_type == 'train':\n",
    "#         data_max = np.max(data)\n",
    "#         data_min = np.min(data)\n",
    "#     else:\n",
    "#         data_max = train_params['Age'][0]\n",
    "#         data_min = train_params['Age'][1]\n",
    "        \n",
    "#     norm_data = (data - data_min) / (data_max - data_min + 1e-3)    \n",
    "    \n",
    "#     if process_type == 'train':\n",
    "#         return norm_data, data_max, data_min\n",
    "#     else:\n",
    "#         return norm_data\n",
    "    \n",
    "\n",
    "def max_min_norm_score(data, train_params = None, process_type = 'train'):\n",
    "    \n",
    "    if process_type == 'train':\n",
    "        data_max = 100\n",
    "        data_min = 0\n",
    "    else:\n",
    "        data_max = 100\n",
    "        data_min = 0\n",
    "        \n",
    "    norm_data = (data - data_min) / (data_max - data_min)    \n",
    "    \n",
    "    if process_type == 'train':\n",
    "        return norm_data, data_max, data_min\n",
    "    else:\n",
    "        return norm_data\n",
    "    \n",
    "def max_min_norm_scalar(data, train_params = None, process_type = 'train'):\n",
    "    \n",
    "    if process_type == 'train':\n",
    "        data_max = 10\n",
    "        data_min = 0\n",
    "    else:\n",
    "        data_max = 10\n",
    "        data_min = 0\n",
    "        \n",
    "    norm_data = (data - data_min) / (data_max - data_min)    \n",
    "    \n",
    "    if process_type == 'train':\n",
    "        return norm_data, data_max, data_min\n",
    "    else:\n",
    "        return norm_data\n",
    "\n",
    "    \n",
    "def preprocessing(df, train_params = None, process_type = 'train'):\n",
    "    \n",
    "    new_df = pd.DataFrame()\n",
    "    \n",
    "    if process_type == 'train':\n",
    "        train_params = {}\n",
    "        category_var_len = {}\n",
    "\n",
    "    # Category \n",
    "    for cat_name in category_vars:\n",
    "        cat_var = df[cat_name]\n",
    "        if process_type == 'train':\n",
    "            cat_var, le = category_to_numerical(cat_var)\n",
    "            train_params[f'{cat_name}_le'] = le\n",
    "            category_var_len[f'{cat_name}'] = len(np.unique(cat_var))\n",
    "        else:\n",
    "            cat_var = train_params[f'{cat_name}_le'].transform(cat_var)\n",
    "        new_df[f'{cat_name}'] = cat_var\n",
    "    \n",
    "    # Numerical score\n",
    "    for num_name in numerical_score_vars:\n",
    "        num_var = df[num_name]\n",
    "        if process_type == 'train':\n",
    "            num_var, data_max, data_min = max_min_norm_score(num_var, process_type = 'train')\n",
    "            train_params[num_name] = [data_max, data_min]\n",
    "        else:\n",
    "            num_var = max_min_norm_score(num_var, train_params, process_type = 'valid')\n",
    "        new_df[num_name] = num_var.values\n",
    "    \n",
    "    # Numerical scalar\n",
    "    for num_name in numerical_scalar_vars:\n",
    "        num_var = df[num_name]\n",
    "        num_var = np.log(num_var)\n",
    "        if process_type == 'train':\n",
    "            num_var, data_max, data_min = max_min_norm_scalar(num_var, process_type = 'train')\n",
    "            train_params[num_name] = [data_max, data_min]\n",
    "        else:\n",
    "            num_var = max_min_norm_scalar(num_var, train_params, process_type = 'valid')\n",
    "        new_df[num_name] = num_var.values\n",
    "        \n",
    "        \n",
    "    if process_type == 'train':\n",
    "        return new_df, train_params, category_var_len\n",
    "    else:\n",
    "        return new_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "category_var_len: {'Gender': 2, 'Department': 4, 'Grade': 5, 'Extracurricular_Activities': 2, 'Internet_Access_at_Home': 2, 'Parent_Education_Level': 4, 'Family_Income_Level': 3}\n",
      "processed_df_train: (1806, 19)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Gender</th>\n",
       "      <th>Department</th>\n",
       "      <th>Grade</th>\n",
       "      <th>Extracurricular_Activities</th>\n",
       "      <th>Internet_Access_at_Home</th>\n",
       "      <th>Parent_Education_Level</th>\n",
       "      <th>Family_Income_Level</th>\n",
       "      <th>Attendance (%)</th>\n",
       "      <th>Midterm_Score</th>\n",
       "      <th>Final_Score</th>\n",
       "      <th>Assignments_Avg</th>\n",
       "      <th>Quizzes_Avg</th>\n",
       "      <th>Participation_Score</th>\n",
       "      <th>Projects_Score</th>\n",
       "      <th>Total_Score</th>\n",
       "      <th>Stress_Level (1-10)</th>\n",
       "      <th>Sleep_Hours_per_Night</th>\n",
       "      <th>Study_Hours_per_Week</th>\n",
       "      <th>Age</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.8989</td>\n",
       "      <td>0.4255</td>\n",
       "      <td>0.5045</td>\n",
       "      <td>0.5528</td>\n",
       "      <td>0.7634</td>\n",
       "      <td>0.0231</td>\n",
       "      <td>0.5783</td>\n",
       "      <td>0.7426</td>\n",
       "      <td>0.04</td>\n",
       "      <td>0.212823</td>\n",
       "      <td>0.332504</td>\n",
       "      <td>0.317805</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>0.5562</td>\n",
       "      <td>0.8190</td>\n",
       "      <td>0.4543</td>\n",
       "      <td>0.5380</td>\n",
       "      <td>0.9215</td>\n",
       "      <td>0.0950</td>\n",
       "      <td>0.6371</td>\n",
       "      <td>0.8995</td>\n",
       "      <td>0.02</td>\n",
       "      <td>0.193152</td>\n",
       "      <td>0.177495</td>\n",
       "      <td>0.313549</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0.6309</td>\n",
       "      <td>0.6057</td>\n",
       "      <td>0.5680</td>\n",
       "      <td>0.5342</td>\n",
       "      <td>0.8288</td>\n",
       "      <td>0.0529</td>\n",
       "      <td>0.5671</td>\n",
       "      <td>0.5943</td>\n",
       "      <td>0.07</td>\n",
       "      <td>0.158924</td>\n",
       "      <td>0.293916</td>\n",
       "      <td>0.294444</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>0.6077</td>\n",
       "      <td>0.9869</td>\n",
       "      <td>0.4550</td>\n",
       "      <td>0.5771</td>\n",
       "      <td>0.5339</td>\n",
       "      <td>0.0948</td>\n",
       "      <td>0.5516</td>\n",
       "      <td>0.8337</td>\n",
       "      <td>0.10</td>\n",
       "      <td>0.210413</td>\n",
       "      <td>0.282138</td>\n",
       "      <td>0.294444</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>0.9590</td>\n",
       "      <td>0.8459</td>\n",
       "      <td>0.6858</td>\n",
       "      <td>0.5204</td>\n",
       "      <td>0.6389</td>\n",
       "      <td>0.0156</td>\n",
       "      <td>0.6554</td>\n",
       "      <td>0.8854</td>\n",
       "      <td>0.06</td>\n",
       "      <td>0.218605</td>\n",
       "      <td>0.184055</td>\n",
       "      <td>0.304452</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Gender  Department  Grade  Extracurricular_Activities  \\\n",
       "0       1           1      1                           1   \n",
       "1       1           3      2                           0   \n",
       "2       1           1      4                           0   \n",
       "3       1           2      2                           1   \n",
       "4       0           2      1                           0   \n",
       "\n",
       "   Internet_Access_at_Home  Parent_Education_Level  Family_Income_Level  \\\n",
       "0                        1                       0                    1   \n",
       "1                        1                       3                    2   \n",
       "2                        1                       1                    1   \n",
       "3                        1                       1                    2   \n",
       "4                        1                       1                    2   \n",
       "\n",
       "   Attendance (%)  Midterm_Score  Final_Score  Assignments_Avg  Quizzes_Avg  \\\n",
       "0          0.8989         0.4255       0.5045           0.5528       0.7634   \n",
       "1          0.5562         0.8190       0.4543           0.5380       0.9215   \n",
       "2          0.6309         0.6057       0.5680           0.5342       0.8288   \n",
       "3          0.6077         0.9869       0.4550           0.5771       0.5339   \n",
       "4          0.9590         0.8459       0.6858           0.5204       0.6389   \n",
       "\n",
       "   Participation_Score  Projects_Score  Total_Score  Stress_Level (1-10)  \\\n",
       "0               0.0231          0.5783       0.7426                 0.04   \n",
       "1               0.0950          0.6371       0.8995                 0.02   \n",
       "2               0.0529          0.5671       0.5943                 0.07   \n",
       "3               0.0948          0.5516       0.8337                 0.10   \n",
       "4               0.0156          0.6554       0.8854                 0.06   \n",
       "\n",
       "   Sleep_Hours_per_Night  Study_Hours_per_Week       Age  \n",
       "0               0.212823              0.332504  0.317805  \n",
       "1               0.193152              0.177495  0.313549  \n",
       "2               0.158924              0.293916  0.294444  \n",
       "3               0.210413              0.282138  0.294444  \n",
       "4               0.218605              0.184055  0.304452  "
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "processed_df_train, train_params, category_var_len = preprocessing(df_train, process_type = 'train')\n",
    "# train_params\n",
    "print(f\"category_var_len: {category_var_len}\")\n",
    "print(f\"processed_df_train: {processed_df_train.shape}\")\n",
    "processed_df_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "processed_df_valid: (775, 19)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Gender</th>\n",
       "      <th>Department</th>\n",
       "      <th>Grade</th>\n",
       "      <th>Extracurricular_Activities</th>\n",
       "      <th>Internet_Access_at_Home</th>\n",
       "      <th>Parent_Education_Level</th>\n",
       "      <th>Family_Income_Level</th>\n",
       "      <th>Attendance (%)</th>\n",
       "      <th>Midterm_Score</th>\n",
       "      <th>Final_Score</th>\n",
       "      <th>Assignments_Avg</th>\n",
       "      <th>Quizzes_Avg</th>\n",
       "      <th>Participation_Score</th>\n",
       "      <th>Projects_Score</th>\n",
       "      <th>Total_Score</th>\n",
       "      <th>Stress_Level (1-10)</th>\n",
       "      <th>Sleep_Hours_per_Night</th>\n",
       "      <th>Study_Hours_per_Week</th>\n",
       "      <th>Age</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>0.5154</td>\n",
       "      <td>0.6404</td>\n",
       "      <td>0.5002</td>\n",
       "      <td>0.8728</td>\n",
       "      <td>0.9699</td>\n",
       "      <td>0.0399</td>\n",
       "      <td>0.8258</td>\n",
       "      <td>0.8888</td>\n",
       "      <td>0.08</td>\n",
       "      <td>0.214007</td>\n",
       "      <td>0.283908</td>\n",
       "      <td>0.294444</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>0.9970</td>\n",
       "      <td>0.9376</td>\n",
       "      <td>0.6142</td>\n",
       "      <td>0.5457</td>\n",
       "      <td>0.7576</td>\n",
       "      <td>0.0308</td>\n",
       "      <td>0.5397</td>\n",
       "      <td>0.6468</td>\n",
       "      <td>0.08</td>\n",
       "      <td>0.141099</td>\n",
       "      <td>0.194591</td>\n",
       "      <td>0.289037</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0.6151</td>\n",
       "      <td>0.7315</td>\n",
       "      <td>0.4190</td>\n",
       "      <td>0.5537</td>\n",
       "      <td>0.5564</td>\n",
       "      <td>0.0770</td>\n",
       "      <td>0.7862</td>\n",
       "      <td>0.5429</td>\n",
       "      <td>0.09</td>\n",
       "      <td>0.193152</td>\n",
       "      <td>0.294969</td>\n",
       "      <td>0.317805</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>0.5228</td>\n",
       "      <td>0.8212</td>\n",
       "      <td>0.9962</td>\n",
       "      <td>0.5518</td>\n",
       "      <td>0.5023</td>\n",
       "      <td>0.0058</td>\n",
       "      <td>0.8334</td>\n",
       "      <td>0.6564</td>\n",
       "      <td>0.07</td>\n",
       "      <td>0.185630</td>\n",
       "      <td>0.319458</td>\n",
       "      <td>0.304452</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0.9474</td>\n",
       "      <td>0.5340</td>\n",
       "      <td>0.6216</td>\n",
       "      <td>0.8329</td>\n",
       "      <td>0.7903</td>\n",
       "      <td>0.0408</td>\n",
       "      <td>0.6250</td>\n",
       "      <td>0.6557</td>\n",
       "      <td>0.03</td>\n",
       "      <td>0.200148</td>\n",
       "      <td>0.236085</td>\n",
       "      <td>0.313549</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Gender  Department  Grade  Extracurricular_Activities  \\\n",
       "0       1           3      2                           0   \n",
       "1       1           1      1                           1   \n",
       "2       1           3      2                           0   \n",
       "3       1           1      3                           1   \n",
       "4       1           2      0                           0   \n",
       "\n",
       "   Internet_Access_at_Home  Parent_Education_Level  Family_Income_Level  \\\n",
       "0                        1                       2                    1   \n",
       "1                        1                       2                    1   \n",
       "2                        1                       3                    0   \n",
       "3                        1                       2                    1   \n",
       "4                        1                       0                    2   \n",
       "\n",
       "   Attendance (%)  Midterm_Score  Final_Score  Assignments_Avg  Quizzes_Avg  \\\n",
       "0          0.5154         0.6404       0.5002           0.8728       0.9699   \n",
       "1          0.9970         0.9376       0.6142           0.5457       0.7576   \n",
       "2          0.6151         0.7315       0.4190           0.5537       0.5564   \n",
       "3          0.5228         0.8212       0.9962           0.5518       0.5023   \n",
       "4          0.9474         0.5340       0.6216           0.8329       0.7903   \n",
       "\n",
       "   Participation_Score  Projects_Score  Total_Score  Stress_Level (1-10)  \\\n",
       "0               0.0399          0.8258       0.8888                 0.08   \n",
       "1               0.0308          0.5397       0.6468                 0.08   \n",
       "2               0.0770          0.7862       0.5429                 0.09   \n",
       "3               0.0058          0.8334       0.6564                 0.07   \n",
       "4               0.0408          0.6250       0.6557                 0.03   \n",
       "\n",
       "   Sleep_Hours_per_Night  Study_Hours_per_Week       Age  \n",
       "0               0.214007              0.283908  0.294444  \n",
       "1               0.141099              0.194591  0.289037  \n",
       "2               0.193152              0.294969  0.317805  \n",
       "3               0.185630              0.319458  0.304452  \n",
       "4               0.200148              0.236085  0.313549  "
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "processed_df_valid = preprocessing(df_valid, train_params, process_type = 'valid')\n",
    "print(f\"processed_df_valid: {processed_df_valid.shape}\")\n",
    "processed_df_valid.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TableDataset(Dataset):\n",
    "    def __init__(self, data):\n",
    "        self.data = np.array(data)\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "    \n",
    "    def __getitem__(self, id):\n",
    "        dat = self.data[id, :]\n",
    "        dat = torch.from_numpy(dat)\n",
    "        return dat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 256\n",
    "DROPOUT = 0.2\n",
    "\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "\n",
    "train_dataset = TableDataset(processed_df_train)\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=BATCH_SIZE, num_workers=4, shuffle=True)\n",
    "\n",
    "valid_dataset = TableDataset(processed_df_valid)\n",
    "valid_dataloader = DataLoader(valid_dataset, batch_size=BATCH_SIZE, num_workers=4, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# category_var_len: {'Gender': 2, 'Department': 4, 'Grade': 5, \n",
    "                #    'Extracurricular_Activities': 2, 'Internet_Access_at_Home': 2, \n",
    "                #    'Parent_Education_Level': 4, 'Family_Income_Level': 3}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Head(nn.Module):\n",
    "    def __init__(self, num_head, transformer_emb_size, category_emb_size):\n",
    "        super().__init__()\n",
    "        self.num_head = num_head\n",
    "        self.up_emb_size = transformer_emb_size\n",
    "        self.category_emb_size = category_emb_size\n",
    "        \n",
    "        # up_emb_size * 3 for qkv\n",
    "        self.qkv_fn = nn.Linear(self.up_emb_size, self.up_emb_size * 3, bias=False)\n",
    "        self.proj_qkv = nn.Linear(self.up_emb_size, self.up_emb_size, bias = False)\n",
    "                \n",
    "        # other\n",
    "        self.up_dropout = nn.Dropout(DROPOUT)\n",
    "        self.att_dropout = nn.Dropout(DROPOUT)\n",
    "        self.down_dropout = nn.Dropout(DROPOUT)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        '''\n",
    "        num_vars is seq_len in here\n",
    "        \n",
    "        ori_hidden_size = 1\n",
    "        up_emb = 32\n",
    "        \n",
    "        '''        \n",
    "        batch_size, num_vars, _ = x.shape\n",
    "        \n",
    "        # qkv: up_emb_size * 3\n",
    "        x = self.qkv_fn(x) \n",
    "        q, k, v = x.split(self.up_emb_size, dim = 2) \n",
    "        \n",
    "        # split head: each shape = [batch_size, num_head, num_vars(seq_len), head_size = 8]\n",
    "        q = q.view(batch_size, num_vars, self.num_head, self.up_emb_size // self.num_head).transpose(1, 2)\n",
    "        k = k.view(batch_size, num_vars, self.num_head, self.up_emb_size // self.num_head).transpose(1, 2)\n",
    "        v = v.view(batch_size, num_vars, self.num_head, self.up_emb_size // self.num_head).transpose(1, 2)\n",
    "        \n",
    "        # attention matrix calculation: [batch_size, num_head, num_vars(seq_len), num_vars]\n",
    "        att = (q @ k.transpose(-2,-1)) * (1 / torch.sqrt(torch.ones([1]).to(device) * k.size(-1)))\n",
    "        # att = self.modify_att(att)\n",
    "        att = F.softmax(att, dim = -1)\n",
    "        att = self.att_dropout(att)\n",
    "        \n",
    "        # att matrix * V: [batch_size, num_head, num_vars(seq_len), head_size]\n",
    "        out = att @ v\n",
    "        out = out.transpose(1,2).contiguous().view(batch_size, num_vars, self.up_emb_size)\n",
    "        out = self.proj_qkv(out)\n",
    "        \n",
    "        return out\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLP(nn.Module):\n",
    "\n",
    "    def __init__(self, transformer_emb_size):\n",
    "        super().__init__()\n",
    "        self.c_fc    = nn.Linear(transformer_emb_size, transformer_emb_size * 3)\n",
    "        \n",
    "        self.gelu    = nn.GELU(approximate='tanh')\n",
    "        \n",
    "        self.c_proj  = nn.Linear(transformer_emb_size * 3, transformer_emb_size)\n",
    "\n",
    "        self.dropout = nn.Dropout(DROPOUT)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.c_fc(x)\n",
    "\n",
    "        x = self.gelu(x)\n",
    "        \n",
    "        x = self.c_proj(x)\n",
    "\n",
    "        x = self.dropout(x)\n",
    "        \n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Block(nn.Module):\n",
    "    def __init__(self, num_head, transformer_emb_size, category_emb_size):\n",
    "        super(Block, self).__init__()\n",
    "        self.ln_head = nn.LayerNorm(transformer_emb_size)\n",
    "        self.head = Head(num_head, transformer_emb_size, category_emb_size)\n",
    "        self.ln_mlp = nn.LayerNorm(transformer_emb_size)\n",
    "        self.mlp = MLP(transformer_emb_size)\n",
    "                \n",
    "    def forward(self, x):\n",
    "        \n",
    "        x = x + self.head(self.ln_head(x))\n",
    "        x = x + self.mlp(self.ln_mlp(x))\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GPT(nn.Module):\n",
    "    def __init__(self, layer, num_head, seq_len, transformer_emb_size, category_emb_size, dropout):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.blocks = nn.ModuleList(Block(num_head, transformer_emb_size, category_emb_size) for _ in range(layer))\n",
    "        self.positions = nn.Parameter(torch.rand(seq_len, transformer_emb_size))\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "            \n",
    "    def forward(self, x):\n",
    "\n",
    "        # add postion info\n",
    "        x = x + self.positions\n",
    "        x = self.dropout(x)\n",
    "        \n",
    "        for block in self.blocks:\n",
    "            x = block(x)\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "class tableModel(nn.Module):\n",
    "    def __init__(self, category_var_len):\n",
    "        super(tableModel,self).__init__()\n",
    "        self.num_category_var = len(category_var_len)\n",
    "        self.num_numerical_var = 12\n",
    "        self.category_emb_size = 32\n",
    "        self.category_dict = category_var_len\n",
    "        self.batch_size = BATCH_SIZE\n",
    "                \n",
    "        self.mask_prob = 0.05\n",
    "\n",
    "        self.transformer_layer = 12\n",
    "        self.transformer_emb_size = 768\n",
    "        self.num_head = 12\n",
    "        self.seq_len = self.num_category_var + self.num_numerical_var\n",
    "        \n",
    "        '''# encode category vars'''\n",
    "        self.encode_gender = nn.Embedding(category_var_len['Gender'] + 1, self.transformer_emb_size)\n",
    "        self.encode_depart = nn.Embedding(category_var_len['Department'] + 1, self.transformer_emb_size)\n",
    "        self.encode_grade = nn.Embedding(category_var_len['Grade'] + 1, self.transformer_emb_size)\n",
    "        self.encode_activity = nn.Embedding(category_var_len['Extracurricular_Activities'] + 1, self.transformer_emb_size)        \n",
    "        self.encode_internet = nn.Embedding(category_var_len['Internet_Access_at_Home'] + 1, self.transformer_emb_size)\n",
    "        self.encode_parent = nn.Embedding(category_var_len['Parent_Education_Level'] + 1, self.transformer_emb_size)        \n",
    "        self.encode_income = nn.Embedding(category_var_len['Family_Income_Level'] + 1, self.transformer_emb_size)  \n",
    "        \n",
    "        self.encoders = [self.encode_gender, self.encode_depart, self.encode_grade, self.encode_activity, self.encode_internet,\n",
    "                        self.encode_parent, self.encode_income]\n",
    "        \n",
    "        self.encodes_numerical = nn.ModuleList(nn.Linear(1, self.transformer_emb_size) for _ in range( self.num_numerical_var))\n",
    "        \n",
    "        self.encode_dropout = nn.Dropout(DROPOUT)\n",
    "        \n",
    "        '''# decode category vars'''\n",
    "        self.decode_gender = nn.Linear(self.transformer_emb_size, category_var_len['Gender'] + 1, bias=False)\n",
    "        self.encode_gender.weight = self.decode_gender.weight        \n",
    "        self.decode_depart = nn.Linear(self.transformer_emb_size, category_var_len['Department'] + 1, bias=False)\n",
    "        self.encode_depart.weight = self.decode_depart.weight       \n",
    "        self.decode_grade = nn.Linear(self.transformer_emb_size, category_var_len['Grade'] + 1, bias=False)\n",
    "        self.encode_grade.weight = self.decode_grade.weight          \n",
    "        self.decode_activity = nn.Linear(self.transformer_emb_size, category_var_len['Extracurricular_Activities'] + 1, bias=False)\n",
    "        self.encode_activity.weight = self.decode_activity.weight               \n",
    "        self.decode_internet = nn.Linear(self.transformer_emb_size, category_var_len['Internet_Access_at_Home'] + 1, bias=False)\n",
    "        self.encode_internet.weight = self.decode_internet.weight    \n",
    "        self.decode_parent = nn.Linear(self.transformer_emb_size, category_var_len['Parent_Education_Level'] + 1, bias=False)\n",
    "        self.encode_parent.weight = self.decode_parent.weight    \n",
    "        self.decode_income = nn.Linear(self.transformer_emb_size, category_var_len['Family_Income_Level'] + 1, bias=False)\n",
    "        self.encode_income.weight = self.decode_income.weight    \n",
    "        \n",
    "        self.decoders = [self.decode_gender, self.decode_depart, self.decode_grade, self.decode_activity, self.decode_internet,\n",
    "                        self.decode_parent, self.decode_income]\n",
    "        \n",
    "        self.decodes_numerical = nn.ModuleList(nn.Linear(self.transformer_emb_size, 1) for _ in range( self.num_numerical_var))\n",
    "        \n",
    "        self.decode_dropout = nn.Dropout(DROPOUT)\n",
    "\n",
    "        '''# transformer'''\n",
    "        self.gpt = GPT(layer = self.transformer_layer, \n",
    "                       num_head = self.num_head, \n",
    "                       seq_len = self.seq_len,\n",
    "                       transformer_emb_size = self.transformer_emb_size,\n",
    "                       category_emb_size = self.category_emb_size,\n",
    "                       dropout = DROPOUT)\n",
    "        \n",
    "        \n",
    "        ''' linear for numerical '''\n",
    "        self.linear_numerical1 = nn.Linear(self.num_numerical_var, 32, bias = False)\n",
    "        self.linear_numerical2 = nn.Linear(32, 128 , bias = False)\n",
    "        self.linear_numerical3 = nn.Linear(128, 32 , bias = False)\n",
    "        self.linear_numerical4 = nn.Linear(32, self.num_numerical_var , bias = False)\n",
    "\n",
    "        self.relu = nn.ReLU()\n",
    "\n",
    "        \n",
    "    def masking_table(self, x, seed=42, training = True):\n",
    "        \"\"\"\n",
    "        x: (batch_size, num_var = 19)\n",
    "        \"\"\"\n",
    "        # Set random seed for reproducibility\n",
    "        \n",
    "        if training:\n",
    "            seed = torch.randint(0, 5, (1,))\n",
    "            torch.manual_seed(seed)\n",
    "            if torch.cuda.is_available():\n",
    "                torch.cuda.manual_seed_all(seed)\n",
    "        else:\n",
    "            torch.manual_seed(42)\n",
    "            if torch.cuda.is_available():\n",
    "                torch.cuda.manual_seed_all(42)\n",
    "\n",
    "        self.masking_prob = self.mask_prob \n",
    "        device = x.device  # Get the device from input tensor\n",
    "\n",
    "        # Category masking\n",
    "        category_var = x[:, :self.num_category_var].long()  # Ensure category_var is integer type\n",
    "        random_cat = torch.rand_like(category_var, dtype=torch.float, device=device)\n",
    "        masking_cat = random_cat < self.masking_prob\n",
    "        mask_token = torch.tensor([2, 4, 5, 2, 2, 4, 3], device=device, dtype=torch.long).expand_as(category_var)\n",
    "\n",
    "        # Apply mask in-place (avoiding memory allocation overhead)\n",
    "        masked_category_var = category_var.clone()  # Clone to avoid modifying input\n",
    "        masked_category_var[masking_cat] = mask_token[masking_cat]\n",
    "\n",
    "        # Numerical masking\n",
    "        numerical_var = x[:, -self.num_numerical_var:].float()  # Ensure numerical_var is float type\n",
    "        random_numerical = torch.rand_like(numerical_var, dtype=torch.float, device=device)\n",
    "        masking_numerical = random_numerical < self.masking_prob\n",
    "\n",
    "        masked_numerical_var = numerical_var.clone()  # Clone to avoid modifying input\n",
    "        masked_numerical_var[masking_numerical] = 0 # Directly set masked values to zero\n",
    "\n",
    "        masking_position = {\n",
    "            'masking_category': masking_cat,\n",
    "            'masking_numerical': masking_numerical,\n",
    "        }\n",
    "\n",
    "\n",
    "        # Concatenating the masked category and numerical variables\n",
    "        return torch.cat([masked_category_var, masked_numerical_var], dim=1), masking_position\n",
    "        # return torch.cat([category_var, masked_numerical_var], dim=1), masking_position\n",
    "\n",
    "                        \n",
    "    def forward(self, x, training):\n",
    "        ''' masking'''\n",
    "        x, masking_position = self.masking_table(x, training)\n",
    "\n",
    "        ''' Encoding ''' \n",
    "        # category vars\n",
    "        cat_vars = []\n",
    "        for c_id, encode_fn in zip(range(self.num_category_var), self.encoders):\n",
    "            emb_c = encode_fn(x[:,c_id].long())\n",
    "            emb_c = self.encode_dropout(emb_c)\n",
    "            cat_vars.append(emb_c)\n",
    "        cat_vars = torch.stack(cat_vars, dim = 1).float() \n",
    "                \n",
    "        # numerical vars\n",
    "        num_vars = x[:, - self.num_numerical_var:].float()\n",
    "        num_emb = []\n",
    "        for n_id, encode_fn in zip(range(self.num_numerical_var), self.encodes_numerical):\n",
    "            num_var = num_vars[:, n_id].view(-1, 1)\n",
    "            emb_n = encode_fn(num_var)\n",
    "            emb_c = self.encode_dropout(emb_c)\n",
    "            num_emb.append(emb_n)\n",
    "        num_emb = torch.stack(num_emb, dim = 1).float() \n",
    "\n",
    "        # combine category and numerical vars        \n",
    "        x = torch.cat([cat_vars, num_emb], dim = 1)\n",
    "\n",
    "        '''\n",
    "        Transformer\n",
    "        '''\n",
    "        x = self.gpt(x)\n",
    "        \n",
    "        ''' Decode category ''' \n",
    "        # split numerical and category\n",
    "        num_vars = x[:, - self.num_numerical_var:]\n",
    "        cat_vars = x[:, :self.num_category_var]\n",
    "            \n",
    "        # category vars\n",
    "        decoded_cat_vars = []\n",
    "        for c_id, decode_fn in zip(range(self.num_category_var), self.decoders):\n",
    "            emb_c = cat_vars[:, c_id, :]\n",
    "            c_var = decode_fn(emb_c)\n",
    "            c_var = torch.softmax(c_var, dim = -1)\n",
    "            c_var = self.decode_dropout(c_var)\n",
    "            decoded_cat_vars.append(c_var)\n",
    "        \n",
    "        # numerical vars\n",
    "        decoded_num_vars = []\n",
    "        for n_id, decode_fn in zip(range(self.num_numerical_var), self.decodes_numerical):\n",
    "            decode_n = decode_fn(num_vars[:, n_id])\n",
    "            decode_n = self.decode_dropout(decode_n)\n",
    "            decoded_num_vars.append(decode_n)\n",
    "        decoded_num_vars = torch.cat(decoded_num_vars, dim = 1)\n",
    "\n",
    "        decoded_num_vars_ori = decoded_num_vars\n",
    "        decoded_num_vars = self.relu(self.linear_numerical1(decoded_num_vars))\n",
    "        decoded_num_vars = self.relu(self.linear_numerical2(decoded_num_vars))\n",
    "        decoded_num_vars = self.decode_dropout(decoded_num_vars)\n",
    "        decoded_num_vars = self.relu(self.linear_numerical3(decoded_num_vars))\n",
    "        decoded_num_vars = self.decode_dropout(decoded_num_vars)\n",
    "        decoded_num_vars = self.linear_numerical4(decoded_num_vars)\n",
    "        decoded_num_vars = decoded_num_vars + decoded_num_vars_ori\n",
    "\n",
    "        return decoded_num_vars, decoded_cat_vars, masking_position"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "train_dataset = TableDataset(processed_df_train)\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=BATCH_SIZE, num_workers=4, shuffle=True)\n",
    "\n",
    "valid_dataset = TableDataset(processed_df_valid)\n",
    "valid_dataloader = DataLoader(valid_dataset, batch_size=BATCH_SIZE, num_workers=4, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "def linear_warmup_decay_lr(lr_init, lr_final, num_warmup_steps, num_training_steps):\n",
    "    \"\"\"\n",
    "    Returns a lambda function for LambdaLR.\n",
    "    - lr_init: 初始學習率\n",
    "    - lr_final: 最終學習率（不是 0）\n",
    "    - num_warmup_steps: 預熱步數\n",
    "    - num_training_steps: 總訓練步數\n",
    "    \"\"\"\n",
    "    def lr_lambda(current_step):\n",
    "        if current_step < num_warmup_steps:\n",
    "            return current_step / num_warmup_steps  # 線性預熱\n",
    "        else:\n",
    "            progress = (current_step - num_warmup_steps) / (num_training_steps - num_warmup_steps)\n",
    "            return (1 - progress) * (1 - lr_final / lr_init) + (lr_final / lr_init)  # 線性衰減到 lr_final\n",
    "    return lr_lambda"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "LEARNING_RATE = 2e-3\n",
    "EPOCHS = 5000\n",
    "\n",
    "\n",
    "model = tableModel(category_var_len).to(device)\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=LEARNING_RATE, betas=(0.9, 0.95), eps=1e-6, weight_decay=5e-1)\n",
    "\n",
    "scheduler = LambdaLR(optimizer, lr_lambda=linear_warmup_decay_lr(lr_init = LEARNING_RATE, lr_final = LEARNING_RATE * 1e-2, num_warmup_steps = 100, num_training_steps = EPOCHS))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "MSE_loss_fn = nn.MSELoss(reduction='none')  # 逐元素 MSE\n",
    "CE_loss_fn = nn.CrossEntropyLoss(reduction='none')  # 逐元素 CrossEntropy\n",
    "\n",
    "def loss_fn(pred_numerical, pred_category, label, mask_position):\n",
    "    device = label.device\n",
    "    \n",
    "    num_numerical = 12\n",
    "    num_category = 7\n",
    "    ratio_numerical = num_numerical / (num_numerical + num_category)\n",
    "    ratio_category = 1 / (num_numerical + num_category)\n",
    "\n",
    "    masking_category = mask_position['masking_category']  # shape: (batch_size, num_category)\n",
    "    masking_numerical = mask_position['masking_numerical']  # shape: (batch_size, num_numerical)\n",
    "\n",
    "    label_category = label[:, :num_category]\n",
    "    label_numerical = label[:, -num_numerical:]\n",
    "\n",
    "    total_loss = torch.zeros(1, device=device)\n",
    "\n",
    "    # === 1. MSE Loss ===\n",
    "    # 先用 masking_numerical 過濾 pred_numerical 和 label_numerical\n",
    "    pred_numerical_masked = pred_numerical[masking_numerical]\n",
    "    label_numerical_masked = label_numerical[masking_numerical]\n",
    "\n",
    "    if pred_numerical_masked.numel() > 0:  # 確保有 mask 位置\n",
    "        mse_loss = MSE_loss_fn(pred_numerical_masked, label_numerical_masked).mean()\n",
    "        total_loss += (mse_loss * 3) * ratio_numerical\n",
    "\n",
    "    # === 2. CrossEntropy Loss ===\n",
    "    for i in range(num_category):\n",
    "        category_mask = masking_category[:, i]  # shape: (batch_size,)\n",
    "        pred = pred_category[i]  # shape: (batch_size, num_classes)\n",
    "        label_cat = label_category[:, i].long()  # shape: (batch_size,)\n",
    "\n",
    "        # 先用 mask 過濾 pred 和 label\n",
    "        pred_masked = pred[category_mask]\n",
    "        label_masked = label_cat[category_mask]\n",
    "        \n",
    "        if pred_masked.shape[0] > 0:  # 確保有 mask 位置\n",
    "            # print(f\"pred_masked: {pred_masked.shape}\")\n",
    "            # print(f\"label_masked: {label_masked.shape}\")\n",
    "            # raise\n",
    "            ce_loss = CE_loss_fn(pred_masked, label_masked).mean()\n",
    "            total_loss += ce_loss * ratio_category\n",
    "\n",
    "    ce_loss = (total_loss - mse_loss)\n",
    "\n",
    "    return total_loss, mse_loss, ((total_loss - mse_loss))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "iterate epoch:   0%|          | 0/5000 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 0, loss: 1.674682930111885, mse: 0.6244314983487129, ce: 1.0502514243125916\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "iterate epoch:   0%|          | 1/5000 [00:00<1:20:34,  1.03it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 0, val_loss: 1.2914193272590637, val_mse: 0.444048672914505, val_ce_losses: 0.8473706543445587\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "iterate epoch:   2%|▏         | 100/5000 [01:45<1:26:36,  1.06s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 100, loss: 2.3213697522878647, mse: 1.0031017623841763, ce: 1.3182679936289787\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "iterate epoch:   2%|▏         | 101/5000 [01:46<1:28:07,  1.08s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 100, val_loss: 2.8029305934906006, val_mse: 1.2711272835731506, val_ce_losses: 1.53180330991745\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "iterate epoch:   4%|▍         | 200/5000 [03:32<1:25:28,  1.07s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 200, loss: 0.4757934585213661, mse: 0.055387381464242935, ce: 0.4204060770571232\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "iterate epoch:   4%|▍         | 201/5000 [03:34<1:25:33,  1.07s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 200, val_loss: 0.7004018127918243, val_mse: 0.15782703645527363, val_ce_losses: 0.5425747707486153\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "iterate epoch:   6%|▌         | 300/5000 [05:19<1:22:10,  1.05s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 300, loss: 0.34298280254006386, mse: 0.009488878247793764, ce: 0.33349392376840115\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "iterate epoch:   6%|▌         | 301/5000 [05:20<1:19:35,  1.02s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 300, val_loss: 0.4800809174776077, val_mse: 0.04225102625787258, val_ce_losses: 0.43782988935709\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "iterate epoch:   8%|▊         | 400/5000 [07:05<1:24:18,  1.10s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 400, loss: 0.3320746775716543, mse: 0.010361134947743267, ce: 0.32171354070305824\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "iterate epoch:   8%|▊         | 401/5000 [07:06<1:24:45,  1.11s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 400, val_loss: 0.4493557848036289, val_mse: 0.025397315621376038, val_ce_losses: 0.4239584803581238\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "iterate epoch:  10%|█         | 500/5000 [08:52<1:13:50,  1.02it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 500, loss: 0.3148099761456251, mse: 0.007136636122595519, ce: 0.3076733388006687\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "iterate epoch:  10%|█         | 501/5000 [08:53<1:14:23,  1.01it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 500, val_loss: 0.5045193880796432, val_mse: 0.04884209576994181, val_ce_losses: 0.4556773006916046\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "iterate epoch:  12%|█▏        | 600/5000 [10:40<1:18:21,  1.07s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 600, loss: 0.3056306689977646, mse: 0.004770202998770401, ce: 0.3008604682981968\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "iterate epoch:  12%|█▏        | 601/5000 [10:41<1:17:55,  1.06s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 600, val_loss: 0.4848494827747345, val_mse: 0.03638424398377538, val_ce_losses: 0.4484652280807495\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "iterate epoch:  14%|█▍        | 700/5000 [12:21<1:16:56,  1.07s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 700, loss: 0.3116259425878525, mse: 0.008747968066018075, ce: 0.30287797562777996\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "iterate epoch:  14%|█▍        | 701/5000 [12:23<1:17:41,  1.08s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 700, val_loss: 0.481229692697525, val_mse: 0.03567779203876853, val_ce_losses: 0.44555190950632095\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "iterate epoch:  16%|█▌        | 800/5000 [14:07<1:13:55,  1.06s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 800, loss: 0.3039394151419401, mse: 0.005986248317640275, ce: 0.29795316606760025\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "iterate epoch:  16%|█▌        | 801/5000 [14:08<1:15:09,  1.07s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 800, val_loss: 0.4702528491616249, val_mse: 0.036556096747517586, val_ce_losses: 0.4336967468261719\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "iterate epoch:  18%|█▊        | 900/5000 [15:52<1:11:04,  1.04s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 900, loss: 0.3033582028001547, mse: 0.00372694552061148, ce: 0.2996312566101551\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "iterate epoch:  18%|█▊        | 901/5000 [15:53<1:10:33,  1.03s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 900, val_loss: 0.4818481430411339, val_mse: 0.030217438470572233, val_ce_losses: 0.45163069665431976\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "iterate epoch:  20%|██        | 1000/5000 [17:38<1:10:43,  1.06s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 1000, loss: 0.3068159241229296, mse: 0.008642581000458449, ce: 0.29817334190011024\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "iterate epoch:  20%|██        | 1001/5000 [17:39<1:09:45,  1.05s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 1000, val_loss: 0.4574701189994812, val_mse: 0.03220792347565293, val_ce_losses: 0.425262201577425\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "iterate epoch:  22%|██▏       | 1100/5000 [19:23<1:06:39,  1.03s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 1100, loss: 0.3028158098459244, mse: 0.005120024703501258, ce: 0.29769578762352467\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "iterate epoch:  22%|██▏       | 1101/5000 [19:24<1:06:12,  1.02s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 1100, val_loss: 0.5390864461660385, val_mse: 0.05905591230839491, val_ce_losses: 0.48003053665161133\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "iterate epoch:  24%|██▍       | 1200/5000 [21:08<1:02:55,  1.01it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 1200, loss: 0.29885804653167725, mse: 0.0051946941239293665, ce: 0.29366335272789\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "iterate epoch:  24%|██▍       | 1201/5000 [21:09<1:04:17,  1.02s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 1200, val_loss: 0.4484665170311928, val_mse: 0.02353182714432478, val_ce_losses: 0.424934696406126\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "iterate epoch:  26%|██▌       | 1300/5000 [22:57<1:08:37,  1.11s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 1300, loss: 0.299773383885622, mse: 0.006500284740468487, ce: 0.29327309876680374\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "iterate epoch:  26%|██▌       | 1301/5000 [22:58<1:08:18,  1.11s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 1300, val_loss: 0.45818449556827545, val_mse: 0.03422137862071395, val_ce_losses: 0.42396312206983566\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "iterate epoch:  28%|██▊       | 1397/5000 [24:41<1:03:41,  1.06s/it]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[44], line 18\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m data \u001b[38;5;129;01min\u001b[39;00m train_dataloader:\n\u001b[1;32m     16\u001b[0m     data \u001b[38;5;241m=\u001b[39m data\u001b[38;5;241m.\u001b[39mfloat()\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[0;32m---> 18\u001b[0m     pred_numerical, pred_category, masking_position \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtraining\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m     19\u001b[0m     loss, mse_loss, ce_loss \u001b[38;5;241m=\u001b[39m loss_fn(pred_numerical, pred_category, data, masking_position)\n\u001b[1;32m     20\u001b[0m     losses\u001b[38;5;241m.\u001b[39mappend(loss\u001b[38;5;241m.\u001b[39mitem())\n",
      "File \u001b[0;32m~/miniconda3/envs/Vit/lib/python3.12/site-packages/torch/nn/modules/module.py:1739\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1737\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1738\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1739\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/Vit/lib/python3.12/site-packages/torch/nn/modules/module.py:1750\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1745\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1746\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1747\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1748\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1749\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1750\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1752\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1753\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "Cell \u001b[0;32mIn[39], line 151\u001b[0m, in \u001b[0;36mtableModel.forward\u001b[0;34m(self, x, training)\u001b[0m\n\u001b[1;32m    146\u001b[0m x \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mcat([cat_vars, num_emb], dim \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m    148\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m'''\u001b[39;00m\n\u001b[1;32m    149\u001b[0m \u001b[38;5;124;03mTransformer\u001b[39;00m\n\u001b[1;32m    150\u001b[0m \u001b[38;5;124;03m'''\u001b[39;00m\n\u001b[0;32m--> 151\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgpt\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    153\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m''' Decode category '''\u001b[39;00m \n\u001b[1;32m    154\u001b[0m \u001b[38;5;66;03m# split numerical and category\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/Vit/lib/python3.12/site-packages/torch/nn/modules/module.py:1739\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1737\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1738\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1739\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/Vit/lib/python3.12/site-packages/torch/nn/modules/module.py:1750\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1745\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1746\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1747\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1748\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1749\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1750\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1752\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1753\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "Cell \u001b[0;32mIn[38], line 16\u001b[0m, in \u001b[0;36mGPT.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     13\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdropout(x)\n\u001b[1;32m     15\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m block \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mblocks:\n\u001b[0;32m---> 16\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[43mblock\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     18\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m x\n",
      "File \u001b[0;32m~/miniconda3/envs/Vit/lib/python3.12/site-packages/torch/nn/modules/module.py:1739\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1737\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1738\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1739\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/Vit/lib/python3.12/site-packages/torch/nn/modules/module.py:1750\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1745\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1746\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1747\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1748\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1749\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1750\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1752\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1753\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "Cell \u001b[0;32mIn[37], line 11\u001b[0m, in \u001b[0;36mBlock.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x):\n\u001b[0;32m---> 11\u001b[0m     x \u001b[38;5;241m=\u001b[39m x \u001b[38;5;241m+\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mhead\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mln_head\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     12\u001b[0m     x \u001b[38;5;241m=\u001b[39m x \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmlp(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mln_mlp(x))\n\u001b[1;32m     14\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m x\n",
      "File \u001b[0;32m~/miniconda3/envs/Vit/lib/python3.12/site-packages/torch/nn/modules/module.py:1739\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1737\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1738\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1739\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/Vit/lib/python3.12/site-packages/torch/nn/modules/module.py:1750\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1745\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1746\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1747\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1748\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1749\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1750\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1752\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1753\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "Cell \u001b[0;32mIn[35], line 45\u001b[0m, in \u001b[0;36mHead.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     43\u001b[0m out \u001b[38;5;241m=\u001b[39m att \u001b[38;5;241m@\u001b[39m v\n\u001b[1;32m     44\u001b[0m out \u001b[38;5;241m=\u001b[39m out\u001b[38;5;241m.\u001b[39mtranspose(\u001b[38;5;241m1\u001b[39m,\u001b[38;5;241m2\u001b[39m)\u001b[38;5;241m.\u001b[39mcontiguous()\u001b[38;5;241m.\u001b[39mview(batch_size, num_vars, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mup_emb_size)\n\u001b[0;32m---> 45\u001b[0m out \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mproj_qkv\u001b[49m\u001b[43m(\u001b[49m\u001b[43mout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     47\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m out\n",
      "File \u001b[0;32m~/miniconda3/envs/Vit/lib/python3.12/site-packages/torch/nn/modules/module.py:1739\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1737\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1738\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1739\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/Vit/lib/python3.12/site-packages/torch/nn/modules/module.py:1743\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1739\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m   1741\u001b[0m \u001b[38;5;66;03m# torchrec tests the code consistency with the following code\u001b[39;00m\n\u001b[1;32m   1742\u001b[0m \u001b[38;5;66;03m# fmt: off\u001b[39;00m\n\u001b[0;32m-> 1743\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m_call_impl\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m   1744\u001b[0m     forward_call \u001b[38;5;241m=\u001b[39m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_slow_forward \u001b[38;5;28;01mif\u001b[39;00m torch\u001b[38;5;241m.\u001b[39m_C\u001b[38;5;241m.\u001b[39m_get_tracing_state() \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mforward)\n\u001b[1;32m   1745\u001b[0m     \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1746\u001b[0m     \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "\n",
    "train_LOSS = []\n",
    "valid_LOSS = []\n",
    "\n",
    "for epoch in tqdm(range(EPOCHS), desc=\"iterate epoch\"):\n",
    "    losses = []\n",
    "    mse_losses = []\n",
    "    ce_losses = []\n",
    "    \n",
    "    val_losses = []\n",
    "    val_mse_losses = []\n",
    "    val_ce_losses = []\n",
    "    \n",
    "    \n",
    "    model.train()\n",
    "    for data in train_dataloader:\n",
    "        data = data.float().to(device)\n",
    "\n",
    "        pred_numerical, pred_category, masking_position = model(data, training = True)\n",
    "        loss, mse_loss, ce_loss = loss_fn(pred_numerical, pred_category, data, masking_position)\n",
    "        losses.append(loss.item())\n",
    "        mse_losses.append(mse_loss.item())\n",
    "        ce_losses.append(ce_loss.item())\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    scheduler.step()\n",
    "    \n",
    "    losses = np.mean(losses)\n",
    "    mse_losses = np.mean(mse_losses)\n",
    "    ce_losses = np.mean(ce_losses)\n",
    "\n",
    "    train_LOSS.append(losses)\n",
    "    \n",
    "    if epoch % 100 == 0:    \n",
    "        print(f\"epoch: {epoch}, loss: {losses}, mse: {mse_losses}, ce: {ce_losses}\")\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        model.eval()\n",
    "        for data in valid_dataloader:\n",
    "            data = data.float().to(device)\n",
    "                \n",
    "            pred_numerical, pred_category, masking_position = model(data, training = False)\n",
    "            loss, mse_loss, ce_loss = loss_fn(pred_numerical, pred_category, data, masking_position)\n",
    "            val_losses.append(loss.item())\n",
    "            val_mse_losses.append(mse_loss.item())\n",
    "            val_ce_losses.append(ce_loss.item())\n",
    "            \n",
    "    val_losses = np.mean(val_losses)\n",
    "    val_mse_losses = np.mean(val_mse_losses)\n",
    "    val_ce_losses = np.mean(val_ce_losses)\n",
    "    \n",
    "    valid_LOSS.append(val_losses)\n",
    "    \n",
    "    if epoch % 100 == 0:\n",
    "        print(f\"epoch: {epoch}, val_loss: {val_losses}, val_mse: {val_mse_losses}, val_ce_losses: {val_ce_losses}\")\n",
    "        print()\n",
    "        \n",
    "        \n",
    "        \n",
    "        # pred = model.inference(data)\n",
    "        \n",
    "        \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "plt.plot(range(len(train_LOSS)), train_LOSS, color = 'blue')\n",
    "plt.plot(range(len(valid_LOSS)), valid_LOSS, color = 'red')\n",
    "\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "\n",
    "img = Image.open('../../dataset/train/0.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img_np = np.array(img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img_np.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = torch.rand((128,128))\n",
    "a.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "128 * 128"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean = torch.mean(a)\n",
    "std = torch.std(a)\n",
    "\n",
    "outlier_upper = mean + 1 * std\n",
    "outlier_down = mean - 1 * std\n",
    "\n",
    "(a < outlier_upper) & (a > outlier_down)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "k = a[(a < outlier_upper) & (a > outlier_down)]\n",
    "k"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "k = a[(a < outlier_upper) & (a > outlier_down)]\n",
    "k"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.sum(((a < outlier_upper) & (a > outlier_down)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mask = torch.ones((8,8))\n",
    "causal_mask = torch.tril(mask)\n",
    "# causal_mask[:8, :8] = float('-inf')\n",
    "causal_mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "causal_mask = torch.where(causal_mask == 0, float('-inf'), causal_mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "causal_mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "c = torch.rand((64, 8,8))\n",
    "c.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d = c * causal_mask\n",
    "d[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "e = torch.softmax(d, dim=-1)\n",
    "e"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "0.04"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.arange(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nn.Parameter(torch.rand())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Vit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
