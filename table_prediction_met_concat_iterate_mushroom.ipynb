{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import numpy as np\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch.nn as nn\n",
    "from tqdm import tqdm\n",
    "from torch.nn import functional as F\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from torch.optim.lr_scheduler import LambdaLR\n",
    "\n",
    "import os\n",
    "from ucimlrepo import fetch_ucirepo \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.makedirs(\"checkpoints/\", exist_ok=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "columns: Index(['cap-diameter', 'cap-shape', 'cap-surface', 'cap-color',\n",
      "       'does-bruise-or-bleed', 'gill-attachment', 'gill-spacing', 'gill-color',\n",
      "       'stem-height', 'stem-width', 'stem-root', 'stem-surface', 'stem-color',\n",
      "       'veil-type', 'veil-color', 'has-ring', 'ring-type', 'habitat',\n",
      "       'season'],\n",
      "      dtype='object')\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>cap-diameter</th>\n",
       "      <th>cap-shape</th>\n",
       "      <th>cap-surface</th>\n",
       "      <th>cap-color</th>\n",
       "      <th>does-bruise-or-bleed</th>\n",
       "      <th>gill-attachment</th>\n",
       "      <th>gill-spacing</th>\n",
       "      <th>gill-color</th>\n",
       "      <th>stem-height</th>\n",
       "      <th>stem-width</th>\n",
       "      <th>stem-root</th>\n",
       "      <th>stem-surface</th>\n",
       "      <th>stem-color</th>\n",
       "      <th>veil-type</th>\n",
       "      <th>veil-color</th>\n",
       "      <th>has-ring</th>\n",
       "      <th>ring-type</th>\n",
       "      <th>habitat</th>\n",
       "      <th>season</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>15.26</td>\n",
       "      <td>x</td>\n",
       "      <td>g</td>\n",
       "      <td>o</td>\n",
       "      <td>f</td>\n",
       "      <td>e</td>\n",
       "      <td>undefined</td>\n",
       "      <td>w</td>\n",
       "      <td>16.95</td>\n",
       "      <td>1.709</td>\n",
       "      <td>s</td>\n",
       "      <td>y</td>\n",
       "      <td>w</td>\n",
       "      <td>u</td>\n",
       "      <td>w</td>\n",
       "      <td>t</td>\n",
       "      <td>g</td>\n",
       "      <td>d</td>\n",
       "      <td>w</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>16.60</td>\n",
       "      <td>x</td>\n",
       "      <td>g</td>\n",
       "      <td>o</td>\n",
       "      <td>f</td>\n",
       "      <td>e</td>\n",
       "      <td>undefined</td>\n",
       "      <td>w</td>\n",
       "      <td>17.99</td>\n",
       "      <td>1.819</td>\n",
       "      <td>s</td>\n",
       "      <td>y</td>\n",
       "      <td>w</td>\n",
       "      <td>u</td>\n",
       "      <td>w</td>\n",
       "      <td>t</td>\n",
       "      <td>g</td>\n",
       "      <td>d</td>\n",
       "      <td>u</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>14.07</td>\n",
       "      <td>x</td>\n",
       "      <td>g</td>\n",
       "      <td>o</td>\n",
       "      <td>f</td>\n",
       "      <td>e</td>\n",
       "      <td>undefined</td>\n",
       "      <td>w</td>\n",
       "      <td>17.80</td>\n",
       "      <td>1.774</td>\n",
       "      <td>s</td>\n",
       "      <td>y</td>\n",
       "      <td>w</td>\n",
       "      <td>u</td>\n",
       "      <td>w</td>\n",
       "      <td>t</td>\n",
       "      <td>g</td>\n",
       "      <td>d</td>\n",
       "      <td>w</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>14.17</td>\n",
       "      <td>f</td>\n",
       "      <td>h</td>\n",
       "      <td>e</td>\n",
       "      <td>f</td>\n",
       "      <td>e</td>\n",
       "      <td>undefined</td>\n",
       "      <td>w</td>\n",
       "      <td>15.77</td>\n",
       "      <td>1.598</td>\n",
       "      <td>s</td>\n",
       "      <td>y</td>\n",
       "      <td>w</td>\n",
       "      <td>u</td>\n",
       "      <td>w</td>\n",
       "      <td>t</td>\n",
       "      <td>p</td>\n",
       "      <td>d</td>\n",
       "      <td>w</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>14.64</td>\n",
       "      <td>x</td>\n",
       "      <td>h</td>\n",
       "      <td>o</td>\n",
       "      <td>f</td>\n",
       "      <td>e</td>\n",
       "      <td>undefined</td>\n",
       "      <td>w</td>\n",
       "      <td>16.53</td>\n",
       "      <td>1.720</td>\n",
       "      <td>s</td>\n",
       "      <td>y</td>\n",
       "      <td>w</td>\n",
       "      <td>u</td>\n",
       "      <td>w</td>\n",
       "      <td>t</td>\n",
       "      <td>p</td>\n",
       "      <td>d</td>\n",
       "      <td>w</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9995</th>\n",
       "      <td>0.62</td>\n",
       "      <td>x</td>\n",
       "      <td>g</td>\n",
       "      <td>e</td>\n",
       "      <td>f</td>\n",
       "      <td>a</td>\n",
       "      <td>undefined</td>\n",
       "      <td>p</td>\n",
       "      <td>3.89</td>\n",
       "      <td>0.098</td>\n",
       "      <td>undefined</td>\n",
       "      <td>h</td>\n",
       "      <td>k</td>\n",
       "      <td>undefined</td>\n",
       "      <td>undefined</td>\n",
       "      <td>f</td>\n",
       "      <td>f</td>\n",
       "      <td>d</td>\n",
       "      <td>s</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9996</th>\n",
       "      <td>0.78</td>\n",
       "      <td>x</td>\n",
       "      <td>g</td>\n",
       "      <td>e</td>\n",
       "      <td>f</td>\n",
       "      <td>a</td>\n",
       "      <td>undefined</td>\n",
       "      <td>n</td>\n",
       "      <td>4.14</td>\n",
       "      <td>0.103</td>\n",
       "      <td>undefined</td>\n",
       "      <td>h</td>\n",
       "      <td>k</td>\n",
       "      <td>undefined</td>\n",
       "      <td>undefined</td>\n",
       "      <td>f</td>\n",
       "      <td>f</td>\n",
       "      <td>d</td>\n",
       "      <td>s</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9997</th>\n",
       "      <td>0.67</td>\n",
       "      <td>x</td>\n",
       "      <td>g</td>\n",
       "      <td>n</td>\n",
       "      <td>f</td>\n",
       "      <td>a</td>\n",
       "      <td>undefined</td>\n",
       "      <td>p</td>\n",
       "      <td>4.10</td>\n",
       "      <td>0.100</td>\n",
       "      <td>undefined</td>\n",
       "      <td>s</td>\n",
       "      <td>k</td>\n",
       "      <td>undefined</td>\n",
       "      <td>undefined</td>\n",
       "      <td>f</td>\n",
       "      <td>f</td>\n",
       "      <td>d</td>\n",
       "      <td>u</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9998</th>\n",
       "      <td>0.94</td>\n",
       "      <td>x</td>\n",
       "      <td>g</td>\n",
       "      <td>e</td>\n",
       "      <td>f</td>\n",
       "      <td>a</td>\n",
       "      <td>undefined</td>\n",
       "      <td>p</td>\n",
       "      <td>4.53</td>\n",
       "      <td>0.119</td>\n",
       "      <td>undefined</td>\n",
       "      <td>h</td>\n",
       "      <td>k</td>\n",
       "      <td>undefined</td>\n",
       "      <td>undefined</td>\n",
       "      <td>f</td>\n",
       "      <td>f</td>\n",
       "      <td>d</td>\n",
       "      <td>a</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9999</th>\n",
       "      <td>0.80</td>\n",
       "      <td>x</td>\n",
       "      <td>g</td>\n",
       "      <td>n</td>\n",
       "      <td>f</td>\n",
       "      <td>a</td>\n",
       "      <td>undefined</td>\n",
       "      <td>n</td>\n",
       "      <td>4.00</td>\n",
       "      <td>0.100</td>\n",
       "      <td>undefined</td>\n",
       "      <td>h</td>\n",
       "      <td>k</td>\n",
       "      <td>undefined</td>\n",
       "      <td>undefined</td>\n",
       "      <td>f</td>\n",
       "      <td>f</td>\n",
       "      <td>d</td>\n",
       "      <td>u</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>10000 rows × 19 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      cap-diameter cap-shape cap-surface cap-color does-bruise-or-bleed  \\\n",
       "0            15.26         x           g         o                    f   \n",
       "1            16.60         x           g         o                    f   \n",
       "2            14.07         x           g         o                    f   \n",
       "3            14.17         f           h         e                    f   \n",
       "4            14.64         x           h         o                    f   \n",
       "...            ...       ...         ...       ...                  ...   \n",
       "9995          0.62         x           g         e                    f   \n",
       "9996          0.78         x           g         e                    f   \n",
       "9997          0.67         x           g         n                    f   \n",
       "9998          0.94         x           g         e                    f   \n",
       "9999          0.80         x           g         n                    f   \n",
       "\n",
       "     gill-attachment gill-spacing gill-color  stem-height  stem-width  \\\n",
       "0                  e    undefined          w        16.95       1.709   \n",
       "1                  e    undefined          w        17.99       1.819   \n",
       "2                  e    undefined          w        17.80       1.774   \n",
       "3                  e    undefined          w        15.77       1.598   \n",
       "4                  e    undefined          w        16.53       1.720   \n",
       "...              ...          ...        ...          ...         ...   \n",
       "9995               a    undefined          p         3.89       0.098   \n",
       "9996               a    undefined          n         4.14       0.103   \n",
       "9997               a    undefined          p         4.10       0.100   \n",
       "9998               a    undefined          p         4.53       0.119   \n",
       "9999               a    undefined          n         4.00       0.100   \n",
       "\n",
       "      stem-root stem-surface stem-color  veil-type veil-color has-ring  \\\n",
       "0             s            y          w          u          w        t   \n",
       "1             s            y          w          u          w        t   \n",
       "2             s            y          w          u          w        t   \n",
       "3             s            y          w          u          w        t   \n",
       "4             s            y          w          u          w        t   \n",
       "...         ...          ...        ...        ...        ...      ...   \n",
       "9995  undefined            h          k  undefined  undefined        f   \n",
       "9996  undefined            h          k  undefined  undefined        f   \n",
       "9997  undefined            s          k  undefined  undefined        f   \n",
       "9998  undefined            h          k  undefined  undefined        f   \n",
       "9999  undefined            h          k  undefined  undefined        f   \n",
       "\n",
       "     ring-type habitat season  \n",
       "0            g       d      w  \n",
       "1            g       d      u  \n",
       "2            g       d      w  \n",
       "3            p       d      w  \n",
       "4            p       d      w  \n",
       "...        ...     ...    ...  \n",
       "9995         f       d      s  \n",
       "9996         f       d      s  \n",
       "9997         f       d      u  \n",
       "9998         f       d      a  \n",
       "9999         f       d      u  \n",
       "\n",
       "[10000 rows x 19 columns]"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# fetch dataset \n",
    "secondary_mushroom = fetch_ucirepo(id=848) \n",
    "  \n",
    "# data (as pandas dataframes) \n",
    "X = secondary_mushroom.data.features \n",
    "# y = secondary_mushroom.data.targets \n",
    "  \n",
    "df = pd.DataFrame(X)\n",
    "df = df.iloc[:10000, :]\n",
    "\n",
    "# replace NAN attribute with undefined\n",
    "df = df.fillna('undefined')\n",
    "# make cap-diameter, stem-height and stem-width in cm\n",
    "df['stem-width'] = df['stem-width'] / 10\n",
    "# drop \n",
    "unimportant_attribute = ['spore-print-color']\n",
    "df = df.drop(unimportant_attribute, axis=1)\n",
    "\n",
    "\n",
    "print(f\"columns: {df.columns}\")\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "max cap dim: 25.98\n",
      "min cap dim: 0.54\n",
      "max cap dim: 33.92\n",
      "min cap dim: 2.07\n",
      "max cap dim: 2.997\n",
      "min cap dim: 0.074\n"
     ]
    }
   ],
   "source": [
    "print(f\"max cap dim: {max(df['cap-diameter'])}\")\n",
    "print(f\"min cap dim: {min(df['cap-diameter'])}\")\n",
    "print(f\"max cap dim: {max(df['stem-height'])}\")\n",
    "print(f\"min cap dim: {min(df['stem-height'])}\")\n",
    "print(f\"max cap dim: {max(df['stem-width'])}\")\n",
    "print(f\"min cap dim: {min(df['stem-width'])}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "category_vars: 16\n",
      "numerical_vars: 3\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['veil-color',\n",
       " 'stem-surface',\n",
       " 'habitat',\n",
       " 'does-bruise-or-bleed',\n",
       " 'has-ring',\n",
       " 'veil-type',\n",
       " 'ring-type',\n",
       " 'season',\n",
       " 'gill-spacing',\n",
       " 'cap-shape',\n",
       " 'cap-surface',\n",
       " 'gill-attachment',\n",
       " 'stem-root',\n",
       " 'cap-color',\n",
       " 'stem-color',\n",
       " 'gill-color']"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "numerical_scalar_vars = []\n",
    "numerical_score_vars = ['cap-diameter', 'stem-height', 'stem-width']\n",
    "category_vars = list(set(df.columns) - set(numerical_scalar_vars) - set(numerical_score_vars))\n",
    "print(f\"category_vars: {len(category_vars)}\")\n",
    "print(f\"numerical_vars: {len(numerical_scalar_vars + numerical_score_vars)}\")\n",
    "\n",
    "category_vars"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "df_train: (8000, 19)\n",
      "df_valid: (1000, 19)\n"
     ]
    }
   ],
   "source": [
    "df_train, df_valid, _, _ = train_test_split(df, df, test_size=0.2, random_state=0)\n",
    "\n",
    "df_valid = df_valid.iloc[:1000, :]\n",
    "\n",
    "df_train_id = [i for i in range(len(df_train))]\n",
    "df_valid_id = [i for i in range(len(df_valid))]\n",
    "\n",
    "print(f\"df_train: {df_train.shape}\")\n",
    "print(f\"df_valid: {df_valid.shape}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing: \n",
    "1. category to numerical\n",
    "2. max-min norm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "def category_to_numerical(data):\n",
    "    le = LabelEncoder()\n",
    "    le.fit(data)\n",
    "    num_data = le.transform(data)\n",
    "    \n",
    "    return num_data, le\n",
    "\n",
    "def max_min_norm_score(data, train_params = None, process_type = 'train'):\n",
    "    \n",
    "    if process_type == 'train':\n",
    "        data_max = 100\n",
    "        data_min = 0\n",
    "    else:\n",
    "        data_max = 100\n",
    "        data_min = 0\n",
    "        \n",
    "    norm_data = (data - data_min) / (data_max - data_min)    \n",
    "    \n",
    "    if process_type == 'train':\n",
    "        return norm_data, data_max, data_min\n",
    "    else:\n",
    "        return norm_data\n",
    "    \n",
    "def max_min_norm_scalar(data, train_params = None, process_type = 'train'):\n",
    "    \n",
    "    if process_type == 'train':\n",
    "        data_max = 10\n",
    "        data_min = 0\n",
    "    else:\n",
    "        data_max = 10\n",
    "        data_min = 0\n",
    "        \n",
    "    norm_data = (data - data_min) / (data_max - data_min)    \n",
    "    \n",
    "    if process_type == 'train':\n",
    "        return norm_data, data_max, data_min\n",
    "    else:\n",
    "        return norm_data\n",
    "\n",
    "    \n",
    "def preprocessing(df, train_params = None, process_type = 'train'):\n",
    "    \n",
    "    new_df = pd.DataFrame()\n",
    "    \n",
    "    if process_type == 'train':\n",
    "        train_params = {}\n",
    "        category_var_len = {}\n",
    "\n",
    "    # Category \n",
    "    for cat_name in category_vars:\n",
    "        cat_var = df[cat_name]\n",
    "        if process_type == 'train':\n",
    "            cat_var, le = category_to_numerical(cat_var)\n",
    "            train_params[f'{cat_name}_le'] = le\n",
    "            category_var_len[f'{cat_name}'] = len(np.unique(cat_var))\n",
    "        else:\n",
    "            cat_var = train_params[f'{cat_name}_le'].transform(cat_var)\n",
    "        new_df[f'{cat_name}'] = cat_var\n",
    "    \n",
    "    # Numerical score\n",
    "    for num_name in numerical_score_vars:\n",
    "        num_var = df[num_name]\n",
    "        if process_type == 'train':\n",
    "            num_var, data_max, data_min = max_min_norm_score(num_var, process_type = 'train')\n",
    "            train_params[num_name] = [data_max, data_min]\n",
    "        else:\n",
    "            num_var = max_min_norm_score(num_var, train_params, process_type = 'valid')\n",
    "        new_df[num_name] = num_var.values\n",
    "    \n",
    "    # Numerical scalar\n",
    "    for num_name in numerical_scalar_vars:\n",
    "        num_var = df[num_name]\n",
    "        num_var = np.log(num_var)\n",
    "        if process_type == 'train':\n",
    "            num_var, data_max, data_min = max_min_norm_scalar(num_var, process_type = 'train')\n",
    "            train_params[num_name] = [data_max, data_min]\n",
    "        else:\n",
    "            num_var = max_min_norm_scalar(num_var, train_params, process_type = 'valid')\n",
    "        new_df[num_name] = num_var.values\n",
    "        \n",
    "        \n",
    "    if process_type == 'train':\n",
    "        return new_df, train_params, category_var_len\n",
    "    else:\n",
    "        return new_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "category_var_len: {'veil-color': 3, 'stem-surface': 6, 'habitat': 5, 'does-bruise-or-bleed': 2, 'has-ring': 2, 'veil-type': 2, 'ring-type': 8, 'season': 4, 'gill-spacing': 3, 'cap-shape': 6, 'cap-surface': 9, 'gill-attachment': 6, 'stem-root': 4, 'cap-color': 11, 'stem-color': 10, 'gill-color': 9}\n",
      "processed_df_train: (8000, 19)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>veil-color</th>\n",
       "      <th>stem-surface</th>\n",
       "      <th>habitat</th>\n",
       "      <th>does-bruise-or-bleed</th>\n",
       "      <th>has-ring</th>\n",
       "      <th>veil-type</th>\n",
       "      <th>ring-type</th>\n",
       "      <th>season</th>\n",
       "      <th>gill-spacing</th>\n",
       "      <th>cap-shape</th>\n",
       "      <th>cap-surface</th>\n",
       "      <th>gill-attachment</th>\n",
       "      <th>stem-root</th>\n",
       "      <th>cap-color</th>\n",
       "      <th>stem-color</th>\n",
       "      <th>gill-color</th>\n",
       "      <th>cap-diameter</th>\n",
       "      <th>stem-height</th>\n",
       "      <th>stem-width</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>8</td>\n",
       "      <td>7</td>\n",
       "      <td>0.0288</td>\n",
       "      <td>0.0274</td>\n",
       "      <td>0.00658</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>5</td>\n",
       "      <td>3</td>\n",
       "      <td>9</td>\n",
       "      <td>9</td>\n",
       "      <td>7</td>\n",
       "      <td>0.0321</td>\n",
       "      <td>0.0549</td>\n",
       "      <td>0.00336</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>8</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>7</td>\n",
       "      <td>0.1524</td>\n",
       "      <td>0.2348</td>\n",
       "      <td>0.02008</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>7</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>7</td>\n",
       "      <td>2</td>\n",
       "      <td>0.0698</td>\n",
       "      <td>0.0727</td>\n",
       "      <td>0.02107</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>5</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>8</td>\n",
       "      <td>7</td>\n",
       "      <td>0.0651</td>\n",
       "      <td>0.0680</td>\n",
       "      <td>0.01292</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   veil-color  stem-surface  habitat  does-bruise-or-bleed  has-ring  \\\n",
       "0           0             4        1                     0         0   \n",
       "1           0             4        1                     0         0   \n",
       "2           0             4        4                     0         1   \n",
       "3           0             4        1                     0         0   \n",
       "4           1             5        0                     0         1   \n",
       "\n",
       "   veil-type  ring-type  season  gill-spacing  cap-shape  cap-surface  \\\n",
       "0          1          1       0             0          2            5   \n",
       "1          1          1       0             1          0            2   \n",
       "2          1          4       0             2          2            8   \n",
       "3          1          1       3             0          5            7   \n",
       "4          0          5       0             2          5            2   \n",
       "\n",
       "   gill-attachment  stem-root  cap-color  stem-color  gill-color  \\\n",
       "0                1          3          2           8           7   \n",
       "1                5          3          9           9           7   \n",
       "2                4          2          4           4           7   \n",
       "3                3          0          2           7           2   \n",
       "4                2          3          4           8           7   \n",
       "\n",
       "   cap-diameter  stem-height  stem-width  \n",
       "0        0.0288       0.0274     0.00658  \n",
       "1        0.0321       0.0549     0.00336  \n",
       "2        0.1524       0.2348     0.02008  \n",
       "3        0.0698       0.0727     0.02107  \n",
       "4        0.0651       0.0680     0.01292  "
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "processed_df_train, train_params, category_var_len = preprocessing(df_train, process_type = 'train')\n",
    "# train_params\n",
    "print(f\"category_var_len: {category_var_len}\")\n",
    "print(f\"processed_df_train: {processed_df_train.shape}\")\n",
    "\n",
    "cols_name = processed_df_train.columns\n",
    "processed_df_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "processed_df_valid: (1000, 19)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>veil-color</th>\n",
       "      <th>stem-surface</th>\n",
       "      <th>habitat</th>\n",
       "      <th>does-bruise-or-bleed</th>\n",
       "      <th>has-ring</th>\n",
       "      <th>veil-type</th>\n",
       "      <th>ring-type</th>\n",
       "      <th>season</th>\n",
       "      <th>gill-spacing</th>\n",
       "      <th>cap-shape</th>\n",
       "      <th>cap-surface</th>\n",
       "      <th>gill-attachment</th>\n",
       "      <th>stem-root</th>\n",
       "      <th>cap-color</th>\n",
       "      <th>stem-color</th>\n",
       "      <th>gill-color</th>\n",
       "      <th>cap-diameter</th>\n",
       "      <th>stem-height</th>\n",
       "      <th>stem-width</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>5</td>\n",
       "      <td>3</td>\n",
       "      <td>9</td>\n",
       "      <td>9</td>\n",
       "      <td>7</td>\n",
       "      <td>0.0349</td>\n",
       "      <td>0.0472</td>\n",
       "      <td>0.00312</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>7</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>8</td>\n",
       "      <td>7</td>\n",
       "      <td>0.1225</td>\n",
       "      <td>0.1130</td>\n",
       "      <td>0.01517</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>6</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>9</td>\n",
       "      <td>8</td>\n",
       "      <td>7</td>\n",
       "      <td>0.0839</td>\n",
       "      <td>0.1292</td>\n",
       "      <td>0.01337</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>8</td>\n",
       "      <td>7</td>\n",
       "      <td>0.1587</td>\n",
       "      <td>0.0866</td>\n",
       "      <td>0.02580</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>6</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>9</td>\n",
       "      <td>8</td>\n",
       "      <td>7</td>\n",
       "      <td>0.0841</td>\n",
       "      <td>0.1358</td>\n",
       "      <td>0.01486</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   veil-color  stem-surface  habitat  does-bruise-or-bleed  has-ring  \\\n",
       "0           0             4        1                     0         0   \n",
       "1           1             4        0                     0         1   \n",
       "2           1             5        0                     0         1   \n",
       "3           0             4        3                     0         0   \n",
       "4           1             5        0                     0         1   \n",
       "\n",
       "   veil-type  ring-type  season  gill-spacing  cap-shape  cap-surface  \\\n",
       "0          1          1       0             1          0            2   \n",
       "1          0          0       0             2          2            7   \n",
       "2          0          3       0             0          0            6   \n",
       "3          1          1       3             0          2            1   \n",
       "4          0          3       2             0          0            6   \n",
       "\n",
       "   gill-attachment  stem-root  cap-color  stem-color  gill-color  \\\n",
       "0                5          3          9           9           7   \n",
       "1                2          3          4           8           7   \n",
       "2                2          3          9           8           7   \n",
       "3                0          2          2           8           7   \n",
       "4                2          3          9           8           7   \n",
       "\n",
       "   cap-diameter  stem-height  stem-width  \n",
       "0        0.0349       0.0472     0.00312  \n",
       "1        0.1225       0.1130     0.01517  \n",
       "2        0.0839       0.1292     0.01337  \n",
       "3        0.1587       0.0866     0.02580  \n",
       "4        0.0841       0.1358     0.01486  "
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "processed_df_valid = preprocessing(df_valid, train_params, process_type = 'valid')\n",
    "print(f\"processed_df_valid: {processed_df_valid.shape}\")\n",
    "processed_df_valid.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 256\n",
    "# MASK_RATIO = 30"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "def masking_helper(data, MASK_RATIO, seed = 42):\n",
    "    np.random.seed(seed)\n",
    "\n",
    "    rows, cols = data.shape\n",
    "    \n",
    "    # define mask/unmask ratio\n",
    "    unmask_ratio = ((100 - MASK_RATIO) * cols) // 100\n",
    "    mask_ratio = cols - unmask_ratio\n",
    "\n",
    "    # create random index\n",
    "    # shuff_idx = np.array([np.random.permutation(cols) for _ in range(rows)])\n",
    "    shuff_idx = np.random.permutation(cols).reshape(1, cols)\n",
    "    shuff_idx = np.repeat(shuff_idx, rows, axis = 0)\n",
    "\n",
    "    # define mask/unmask idx\n",
    "    mask_idx = shuff_idx[:, :mask_ratio]\n",
    "    unmask_idx = shuff_idx[:, mask_ratio:]\n",
    "    \n",
    "    mask_idx.sort(axis=1)\n",
    "    unmask_idx.sort(axis=1)\n",
    "    \n",
    "    # create new_data (contain unmask cols, but remove mask cols)\n",
    "    new_data = np.zeros((rows, unmask_ratio))\n",
    "    for i in range(rows):\n",
    "        new_data[i] = data[i][unmask_idx[i]]\n",
    "        \n",
    "    return new_data, unmask_idx, mask_idx, unmask_ratio\n",
    "\n",
    "def masking_fn(data, mask_ratio, seed):\n",
    "    sample_size = len(data)\n",
    "\n",
    "    new_data, unmask_idx, mask_idx, unmask_ratio = masking_helper(data, mask_ratio, seed)\n",
    "        \n",
    "    X = [[],[],[],[]]\n",
    "    Y = []\n",
    "\n",
    "    for i in range(sample_size):\n",
    "        X[0].append(new_data[i]) # unmask data\n",
    "        X[1].append(unmask_idx[i]) # unmask id\n",
    "        X[2].append(mask_idx[i]) # mask id\n",
    "        X[3].append(np.ones(unmask_ratio)) # len = unmask, serve as random noisee in VAE\n",
    "        Y.append(data[i][list(unmask_idx[i])+list(mask_idx[i])]) # label (unmask + mask)\n",
    "\n",
    "    X[0] = torch.tensor(np.array(X[0])) # unmask data\n",
    "    X[1] = torch.tensor(np.array(X[1])) # unmask id\n",
    "    X[2] = torch.tensor(np.array(X[2])) # mask id\n",
    "    X[3] = torch.tensor(np.array(X[3])) # latent space\n",
    "    \n",
    "    Y = torch.tensor(np.array(Y))\n",
    "    \n",
    "\n",
    "    return X, Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "def collate_fn_train(data_wtih_type):\n",
    "\n",
    "    # data\n",
    "    data = [dat[0] for dat in data_wtih_type]\n",
    "    data = np.array(data)\n",
    "    \n",
    "    # data type: training/valid\n",
    "    data_type = data_wtih_type[0][1]\n",
    "\n",
    "    if data_type == 'training':\n",
    "        seed = np.random.randint(3, size=1)\n",
    "        mask_ratio = np.random.randint(low = 3, high = 3 + 1, size=1) * 10\n",
    "        mask_ratio = mask_ratio[0]\n",
    "\n",
    "    else:\n",
    "        seed = 42\n",
    "        mask_ratio = 30\n",
    "        \n",
    "    data, label = masking_fn(data, mask_ratio, seed)\n",
    "\n",
    "    return data, label, mask_ratio\n",
    "\n",
    "class TableDataset(Dataset):\n",
    "    def __init__(self, data, data_type = 'training'):\n",
    "        self.data = data\n",
    "        self.data_type = data_type\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "    \n",
    "    def __getitem__(self, id):\n",
    "        dat = self.data.iloc[id, :]\n",
    "        dat = np.array(dat)\n",
    "        return dat, self.data_type\n",
    "        \n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "\n",
    "augmented_processed_df_train = [processed_df_train] * 1\n",
    "augmented_processed_df_train = pd.concat(augmented_processed_df_train, axis = 0)\n",
    "\n",
    "train_dataset = TableDataset(augmented_processed_df_train)\n",
    "# train_dataloader = DataLoader(train_dataset, batch_size=BATCH_SIZE, num_workers = 4, shuffle=False, collate_fn = collate_fn_train)\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=False, collate_fn = collate_fn_train)\n",
    "\n",
    "\n",
    "valid_dataset = TableDataset(processed_df_valid, data_type = 'valid')\n",
    "# valid_dataloader = DataLoader(valid_dataset, batch_size=BATCH_SIZE, num_workers = 4, shuffle=False, collate_fn = collate_fn_train)\n",
    "valid_dataloader = DataLoader(valid_dataset, batch_size=BATCH_SIZE, shuffle=False, collate_fn = collate_fn_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Head(nn.Module):\n",
    "    def __init__(self, num_head, encoder_emb_dim, dropout):\n",
    "        super().__init__()\n",
    "        self.num_head = num_head\n",
    "        self.encoder_emb_dim = encoder_emb_dim\n",
    "\n",
    "        # up_emb_size * 3 for qkv\n",
    "        self.qkv_fn = nn.Linear(self.encoder_emb_dim, self.encoder_emb_dim * 3, bias=False)\n",
    "        self.proj_qkv = nn.Linear(self.encoder_emb_dim, self.encoder_emb_dim, bias = False)\n",
    "                \n",
    "        # other\n",
    "        self.att_dropout = nn.Dropout(dropout)\n",
    "    \n",
    "            \n",
    "    def softclamp(self, t, value = 50.):\n",
    "        return (t / value).tanh() * value\n",
    "        \n",
    "        \n",
    "    def forward(self, x, iterative):\n",
    "\n",
    "        batch_size, num_vars, _ = x.shape\n",
    "\n",
    "        # qkv: up_emb_size * 3\n",
    "        x = self.qkv_fn(x) \n",
    "        q, k, v = x.split(self.encoder_emb_dim, dim = 2) \n",
    "        \n",
    "        # split head: each shape = [batch_size, num_head, num_vars(seq_len), head_size = 8]\n",
    "        q = q.view(batch_size, num_vars, self.num_head, self.encoder_emb_dim // self.num_head).transpose(1, 2)\n",
    "        k = k.view(batch_size, num_vars, self.num_head, self.encoder_emb_dim // self.num_head).transpose(1, 2)\n",
    "        v = v.view(batch_size, num_vars, self.num_head, self.encoder_emb_dim // self.num_head).transpose(1, 2)\n",
    "        \n",
    "        # attention matrix calculation: [batch_size, num_head, num_vars(seq_len), num_vars]\n",
    "        att = (q @ k.transpose(-2,-1)) * (1 / torch.sqrt(torch.ones([1]).to(device) * k.size(-1)))\n",
    "        \n",
    "        # masking the att from unmask to mask (Q_unmask to K_mask), \n",
    "        # because mask is basically noise, it's meaningless to ask informative vector (Q_unmask) to refer to noise vector (K_mask)\n",
    "        '''\n",
    "        it's not causal mask\n",
    "        it looks like:\n",
    "                unmask, unmaskm, mask  \n",
    "        unmask  [1,      1,      0] \n",
    "        unmask  [1,      1,      0] \n",
    "        mask    [1,      1,      1] \n",
    "        '''\n",
    "        if iterative:\n",
    "            mask_id = num_vars - 1\n",
    "            att_mask = torch.ones_like(att)\n",
    "            att_mask[:, :, :mask_id, -1] = 0\n",
    "            att = att.masked_fill(att_mask == 0, float('-inf')) \n",
    "\n",
    "        # att = self.softclamp(att, 15)\n",
    "\n",
    "        att = F.softmax(att, dim = -1)\n",
    "        att = self.att_dropout(att)\n",
    "        \n",
    "        # att matrix * V: [batch_size, num_head, num_vars(seq_len), head_size]\n",
    "        out = att @ v\n",
    "        out = out.transpose(1,2).contiguous().view(batch_size, num_vars, self.encoder_emb_dim)\n",
    "        out = self.proj_qkv(out)\n",
    "        \n",
    "        return out\n",
    "\n",
    "class MLP(nn.Module):\n",
    "\n",
    "    def __init__(self, transformer_emb_size, dropout):\n",
    "        super().__init__()\n",
    "        self.up    = nn.Linear(transformer_emb_size, transformer_emb_size * 3)\n",
    "        self.relu  = nn.ReLU()\n",
    "        self.down  = nn.Linear(transformer_emb_size * 3, transformer_emb_size)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.up(x)\n",
    "        x = self.dropout(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.down(x)\n",
    "        \n",
    "        return x\n",
    "    \n",
    "class Block(nn.Module):\n",
    "    def __init__(self, encoder_emb_dim, num_head, dropout):\n",
    "        super(Block, self).__init__()\n",
    "        self.ln_head = nn.LayerNorm(encoder_emb_dim)\n",
    "        self.head = Head(num_head, encoder_emb_dim, dropout)\n",
    "        self.dropout_head = nn.Dropout(dropout)\n",
    "\n",
    "        self.ln_mlp = nn.LayerNorm(encoder_emb_dim)\n",
    "        self.mlp = MLP(encoder_emb_dim, dropout)\n",
    "        self.dropout_mlp = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x, iterative):\n",
    "        \n",
    "        # att \n",
    "        ori_x = x \n",
    "        x = self.head(x, iterative)\n",
    "        x = self.dropout_head(x)\n",
    "        x = self.ln_head(ori_x + x)\n",
    "        \n",
    "        # linear \n",
    "        ori_x = x \n",
    "        x = self.mlp(x)\n",
    "        x = self.dropout_mlp(x)\n",
    "        x = self.ln_mlp(ori_x + x)\n",
    "\n",
    "        return x\n",
    "\n",
    "class Transformer(nn.Module):\n",
    "    def __init__(self, encoder_emb_dim, layers, num_head, dropout):\n",
    "        super().__init__()\n",
    "        self.blocks = nn.ModuleList(Block(encoder_emb_dim, num_head, dropout) for _ in range(layers))\n",
    "        \n",
    "    def forward(self, x, iterative = False):\n",
    "\n",
    "        for block in self.blocks:\n",
    "            x = block(x, iterative)\n",
    "            \n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "category_vars: 16\n",
      "numerical_vars: 3\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['veil-color',\n",
       " 'stem-surface',\n",
       " 'habitat',\n",
       " 'does-bruise-or-bleed',\n",
       " 'has-ring',\n",
       " 'veil-type',\n",
       " 'ring-type',\n",
       " 'season',\n",
       " 'gill-spacing',\n",
       " 'cap-shape',\n",
       " 'cap-surface',\n",
       " 'gill-attachment',\n",
       " 'stem-root',\n",
       " 'cap-color',\n",
       " 'stem-color',\n",
       " 'gill-color']"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "numerical_scalar_vars = []\n",
    "numerical_score_vars = ['cap-diameter', 'stem-height', 'stem-width']\n",
    "category_vars = list(set(df.columns) - set(numerical_scalar_vars) - set(numerical_score_vars))\n",
    "print(f\"category_vars: {len(category_vars)}\")\n",
    "print(f\"numerical_vars: {len(numerical_scalar_vars + numerical_score_vars)}\")\n",
    "\n",
    "category_vars"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Feature_Embed(nn.Module):\n",
    "    def __init__(self, cols_name, category_vars, feature_emb_size, num_variables, position_emb_dim, category_var_len):\n",
    "        super().__init__()\n",
    "        self.cols_name = cols_name\n",
    "        self.category_vars = category_vars\n",
    "        self.feature_emb_size = feature_emb_size\n",
    "        self.category_var_len = category_var_len\n",
    "        self.hidden_size = feature_emb_size + position_emb_dim\n",
    "\n",
    "        # Category feature embedding\n",
    "        self.cat_encoders = {\n",
    "            'ring-type': nn.Embedding(category_var_len['ring-type'] + 1, feature_emb_size),\n",
    "            'gill-color': nn.Embedding(category_var_len['gill-color'] + 1, feature_emb_size),\n",
    "            'cap-shape': nn.Embedding(category_var_len['cap-shape'] + 1, feature_emb_size),\n",
    "            'stem-surface': nn.Embedding(category_var_len['stem-surface'] + 1, feature_emb_size),\n",
    "            'veil-color': nn.Embedding(category_var_len['veil-color'] + 1, feature_emb_size),\n",
    "            'habitat': nn.Embedding(category_var_len['habitat'] + 1, feature_emb_size),\n",
    "            'season': nn.Embedding(category_var_len['season'] + 1, feature_emb_size),\n",
    "            'does-bruise-or-bleed': nn.Embedding(category_var_len['does-bruise-or-bleed'] + 1, feature_emb_size),\n",
    "            'veil-type': nn.Embedding(category_var_len['veil-type'] + 1, feature_emb_size),\n",
    "            'cap-surface': nn.Embedding(category_var_len['cap-surface'] + 1, feature_emb_size),\n",
    "            'has-ring': nn.Embedding(category_var_len['has-ring'] + 1, feature_emb_size),\n",
    "            'stem-root': nn.Embedding(category_var_len['stem-root'] + 1, feature_emb_size),\n",
    "            'gill-attachment': nn.Embedding(category_var_len['gill-attachment'] + 1, feature_emb_size),\n",
    "            'cap-color': nn.Embedding(category_var_len['cap-color'] + 1, feature_emb_size),\n",
    "            'stem-color': nn.Embedding(category_var_len['stem-color'] + 1, feature_emb_size),\n",
    "            'gill-spacing': nn.Embedding(category_var_len['gill-spacing'] + 1, feature_emb_size),\n",
    "        }\n",
    "        \n",
    "        self.cat_decoder1 = nn.Linear(self.hidden_size, self.hidden_size * 2, bias = False)\n",
    "        self.cat_decoder2 = nn.Linear(self.hidden_size * 2, self.hidden_size * 4, bias = False)\n",
    "        self.cat_decoder3 = nn.Linear(self.hidden_size * 4, self.hidden_size * 2, bias = False)\n",
    "        self.cat_decoder4 = nn.Linear(self.hidden_size * 2, self.hidden_size, bias = False)\n",
    "        self.cat_decoder5 = nn.Linear(self.hidden_size, feature_emb_size, bias = False)\n",
    "        \n",
    "        self.cat_decoders = {\n",
    "        # self.cat_decoders_proj_pred = {\n",
    "            'ring-type': nn.Linear(self.hidden_size, category_var_len['ring-type'] + 1, bias=False),\n",
    "            'gill-color': nn.Linear(self.hidden_size, category_var_len['gill-color'] + 1, bias=False),\n",
    "            'cap-shape': nn.Linear(self.hidden_size, category_var_len['cap-shape'] + 1, bias=False),\n",
    "            'stem-surface': nn.Linear(self.hidden_size, category_var_len['stem-surface'] + 1, bias=False),\n",
    "            'veil-color': nn.Linear(self.hidden_size, category_var_len['veil-color'] + 1, bias=False),\n",
    "            'habitat': nn.Linear(self.hidden_size, category_var_len['habitat'] + 1, bias=False),\n",
    "            'season': nn.Linear(self.hidden_size, category_var_len['season'] + 1, bias=False),\n",
    "            'does-bruise-or-bleed': nn.Linear(self.hidden_size, category_var_len['does-bruise-or-bleed'] + 1, bias=False),\n",
    "            'veil-type': nn.Linear(self.hidden_size, category_var_len['veil-type'] + 1, bias=False),\n",
    "            'cap-surface': nn.Linear(self.hidden_size, category_var_len['cap-surface'] + 1, bias=False),\n",
    "            'has-ring': nn.Linear(self.hidden_size, category_var_len['has-ring'] + 1, bias=False),\n",
    "            'stem-root': nn.Linear(self.hidden_size, category_var_len['stem-root'] + 1, bias=False),\n",
    "            'gill-attachment': nn.Linear(self.hidden_size, category_var_len['gill-attachment'] + 1, bias=False),\n",
    "            'cap-color': nn.Linear(self.hidden_size, category_var_len['cap-color'] + 1, bias=False),\n",
    "            'stem-color': nn.Linear(self.hidden_size, category_var_len['stem-color'] + 1, bias=False),\n",
    "            'gill-spacing': nn.Linear(self.hidden_size, category_var_len['gill-spacing'] + 1, bias=False),\n",
    "        }\n",
    "        \n",
    "        # self.cat_encoders['Gender'].weight = self.cat_decoders_proj_pred['Gender'].weight        \n",
    "        # self.cat_encoders['Department'].weight = self.cat_decoders_proj_pred['Department'].weight        \n",
    "        # self.cat_encoders['Grade'].weight = self.cat_decoders_proj_pred['Grade'].weight        \n",
    "        # self.cat_encoders['Extracurricular_Activities'].weight = self.cat_decoders_proj_pred['Extracurricular_Activities'].weight        \n",
    "        # self.cat_encoders['Internet_Access_at_Home'].weight = self.cat_decoders_proj_pred['Internet_Access_at_Home'].weight        \n",
    "        # self.cat_encoders['Parent_Education_Level'].weight = self.cat_decoders_proj_pred['Parent_Education_Level'].weight        \n",
    "        # self.cat_encoders['Family_Income_Level'].weight = self.cat_decoders_proj_pred['Family_Income_Level'].weight        \n",
    "\n",
    "        # numerical decoder\n",
    "        self.numerical_decoder1 = nn.Linear(self.hidden_size, self.hidden_size * 2, bias = False)\n",
    "        self.numerical_decoder2 = nn.Linear(self.hidden_size * 2, self.hidden_size * 4, bias = False)\n",
    "        self.numerical_decoder3 = nn.Linear(self.hidden_size * 4, self.hidden_size * 2, bias = False)\n",
    "        self.numerical_decoder4 = nn.Linear(self.hidden_size * 2, self.hidden_size, bias = False)\n",
    "        self.numerical_decoder5 = nn.Linear(self.hidden_size, 1, bias = False)\n",
    "\n",
    "        self.relu = nn.ReLU()\n",
    "\n",
    "        self.cols_id_name = {}\n",
    "        count = 0\n",
    "        for col in self.cols_name:\n",
    "            self.cols_id_name[count] = col\n",
    "            count += 1\n",
    "            \n",
    "        # positional embedding \n",
    "        self.position_emb_dim = position_emb_dim\n",
    "        self.pos_emb = nn.Embedding(num_variables, position_emb_dim)            \n",
    "        self.num_variables = num_variables\n",
    "\n",
    "    def encode(self, unmasked_data, unmasked_idx, masked_idx):\n",
    "        \n",
    "        batch_size, num_unmask_cols = unmasked_data.shape\n",
    "        _, num_mask_cols = masked_idx.shape\n",
    "        device = unmasked_data.device\n",
    "        \n",
    "        # position info\n",
    "        unmasked_pos_info = self.pos_emb(unmasked_idx.long()).float()\n",
    "        masked_pos_info = self.pos_emb(masked_idx.long()).float()\n",
    "\n",
    "        ''' Feature Encoding for Unmask '''\n",
    "        \n",
    "        # iterate every col in unmask\n",
    "        # for category, do nn.Embedding\n",
    "        # for numerical, repeat feature_emb_size times\n",
    "        unmasked_emb = []\n",
    "        for c in range(num_unmask_cols):\n",
    "            unmask_attribute_value = unmasked_data[:, c]\n",
    "            \n",
    "            # make sure unmask_attribute_id is unique along one col dimension\n",
    "            # use the unique id to see the choosen attribute is categorial or numerical\n",
    "            unmask_attribute_id = unmasked_idx[:, c]\n",
    "            \n",
    "            assert len(torch.unique(unmask_attribute_id)) == 1, \"unmask_attribute_id in one small batch has to be the same\"\n",
    "            unmask_attribute_id = torch.unique(unmask_attribute_id).item()\n",
    "\n",
    "            # this col is categorial \n",
    "            if self.cols_id_name[unmask_attribute_id] in self.category_vars:\n",
    "                # print(f\"col_name: {self.cols_id_name[unmask_attribute_id]}\")\n",
    "                encoder = self.cat_encoders[self.cols_id_name[unmask_attribute_id]].to(device)\n",
    "                unmask_attribute_emb = encoder(unmask_attribute_value.long()).float()\n",
    "                unmasked_emb.append(unmask_attribute_emb)\n",
    "            # this col is numerical\n",
    "            else:\n",
    "                unmask_attribute_value = torch.unsqueeze(unmask_attribute_value, dim = 1)\n",
    "                unmask_attribute_emb = unmask_attribute_value.repeat(1, self.feature_emb_size)\n",
    "                unmasked_emb.append(unmask_attribute_emb)\n",
    "\n",
    "        unmasked_emb = torch.stack(unmasked_emb, dim = 1)\n",
    "        \n",
    "        ''' Positional Encoding for Unmask '''\n",
    "        unmasked_emb = torch.cat([unmasked_emb, unmasked_pos_info], dim = 2) # (batch_size, num_unmask_vars, feature_emb_size + pos_emb_size)\n",
    "\n",
    "        \n",
    "        ''' Feature Encoding for Mask '''\n",
    "        # create noise for categorial and numerical seperately\n",
    "        masked_emb = []\n",
    "        for c in range(num_mask_cols):\n",
    "            mask_attribute_id = masked_idx[:, c]\n",
    "            assert len(torch.unique(mask_attribute_id)) == 1, \"mask_attribute_id in one small batch has to be the same\"\n",
    "            mask_attribute_id = torch.unique(mask_attribute_id).item()\n",
    "\n",
    "            # this col is categorial \n",
    "            if self.cols_id_name[mask_attribute_id] in self.category_vars:\n",
    "                # get col name\n",
    "                col_name = self.cols_id_name[mask_attribute_id]\n",
    "                encoder = self.cat_encoders[col_name].to(device)\n",
    "                \n",
    "                # create categorial mask id\n",
    "                category_mask_id = self.category_var_len[col_name]\n",
    "                categorial_latent = torch.ones(batch_size).to(device) * category_mask_id\n",
    "\n",
    "                categorial_emb = encoder(categorial_latent.long()).float()\n",
    "                masked_emb.append(categorial_emb)\n",
    "                \n",
    "            # this col is numerical\n",
    "            else:\n",
    "                # create rand(0-1) as numerical latent\n",
    "                numerical_latent = torch.rand(batch_size).to(device)\n",
    "                numerical_latent = torch.unsqueeze(numerical_latent, dim = 1)\n",
    "                numerical_emb = numerical_latent.repeat(1, self.feature_emb_size)\n",
    "                masked_emb.append(numerical_emb)\n",
    "\n",
    "        masked_emb = torch.stack(masked_emb, dim = 1)\n",
    "        \n",
    "        ''' Positional Encoding for Mask '''\n",
    "        masked_emb = torch.cat([masked_emb, masked_pos_info], dim = 2) # (batch_size, num_mask_vars, feature_emb_size + pos_emb_size)\n",
    "\n",
    "        return unmasked_emb, masked_emb\n",
    "    \n",
    "    def decode(self, all_emb, unmask_mask_id):\n",
    "\n",
    "        batch_size, num_cols, hidden_size = all_emb.shape\n",
    "        device = all_emb.device\n",
    "        \n",
    "        ''' decode for unmask and mask '''\n",
    "        categorial_preds = []\n",
    "        numerical_preds = []\n",
    "        \n",
    "        categorial_col_name = []\n",
    "        numerical_col_name = []\n",
    "        \n",
    "        for c in range(num_cols):\n",
    "            # attribute value\n",
    "            attribute_value = all_emb[:, c, :]\n",
    "\n",
    "            # attribute id\n",
    "            attribute_id = unmask_mask_id[:, c]\n",
    "            assert len(torch.unique(attribute_id)) == 1, \"attribute_id in one small batch has to be the same\"\n",
    "            attribute_id = torch.unique(attribute_id).item()\n",
    "\n",
    "            # this col is categorial \n",
    "            if self.cols_id_name[attribute_id] in self.category_vars:\n",
    "                col_name = self.cols_id_name[attribute_id]\n",
    "                decoder = self.cat_decoders[col_name].to(device)\n",
    "                cat_pred = decoder(attribute_value)\n",
    "\n",
    "                categorial_preds.append(cat_pred)\n",
    "                categorial_col_name.append(col_name)\n",
    "                \n",
    "                # col_name = self.cols_id_name[attribute_id]\n",
    "                # categorial_preds.append(attribute_value)\n",
    "                # categorial_col_name.append(col_name)\n",
    "                \n",
    "            # this col is numerical\n",
    "            else:                \n",
    "                col_name = self.cols_id_name[attribute_id]\n",
    "                numerical_col_name.append(col_name)\n",
    "                \n",
    "                numerical_preds.append(attribute_value)\n",
    "\n",
    "        # # decode for categorial\n",
    "        # categorial_preds = torch.stack(categorial_preds, dim = 1)\n",
    "        \n",
    "        # ori_categorial_preds = categorial_preds\n",
    "        \n",
    "        # categorial_preds = self.cat_decoder1(categorial_preds).to(device)\n",
    "        # categorial_preds = self.relu(categorial_preds)\n",
    "        \n",
    "        # categorial_preds = self.cat_decoder2(categorial_preds).to(device)\n",
    "        # categorial_preds = self.relu(categorial_preds)\n",
    "        \n",
    "        # categorial_preds = self.cat_decoder3(categorial_preds).to(device)\n",
    "        # categorial_preds = self.relu(categorial_preds)\n",
    "        \n",
    "        # categorial_preds = self.cat_decoder4(categorial_preds).to(device)\n",
    "        # categorial_preds = self.relu(categorial_preds)\n",
    "        \n",
    "        # # residual connection\n",
    "        # categorial_preds = categorial_preds + ori_categorial_preds\n",
    "        # categorial_preds = self.cat_decoder5(categorial_preds).to(device)\n",
    "\n",
    "        # categorial_projs = []\n",
    "        # for i in range(len(self.category_vars)):\n",
    "        #     decoder = self.cat_decoders_proj_pred[categorial_col_name[i]].to(device)\n",
    "        #     pred = decoder(categorial_preds[:, i, :])\n",
    "        #     categorial_projs.append(pred)\n",
    "        # categorial_preds = categorial_projs\n",
    "\n",
    "        # decode for numerical\n",
    "        numerical_preds = torch.stack(numerical_preds, dim = 1)\n",
    "        \n",
    "        ori_numerical_preds = numerical_preds\n",
    "        \n",
    "        numerical_preds = self.numerical_decoder1(numerical_preds).to(device)\n",
    "        numerical_preds = self.relu(numerical_preds)\n",
    "        \n",
    "        numerical_preds = self.numerical_decoder2(numerical_preds).to(device)\n",
    "        numerical_preds = self.relu(numerical_preds)\n",
    "        \n",
    "        numerical_preds = self.numerical_decoder3(numerical_preds).to(device)\n",
    "        numerical_preds = self.relu(numerical_preds)\n",
    "        \n",
    "        numerical_preds = self.numerical_decoder4(numerical_preds).to(device)\n",
    "        numerical_preds = self.relu(numerical_preds)\n",
    "        \n",
    "        # residual connection\n",
    "        numerical_preds = numerical_preds + ori_numerical_preds\n",
    "        numerical_preds = self.numerical_decoder5(numerical_preds).to(device)\n",
    "\n",
    "        numerical_preds = torch.squeeze(numerical_preds)\n",
    "        \n",
    "        return categorial_preds, numerical_preds, categorial_col_name, numerical_col_name\n",
    "    \n",
    "    \n",
    "    def reorder_label_fn(self, label, unmask_mask_id):\n",
    "        \n",
    "        batch_size, num_cols = label.shape\n",
    "        device = label.device\n",
    "        \n",
    "        ''' reorder for unmask and mask '''\n",
    "        categorial_labels = []\n",
    "        numerical_labels = []\n",
    "        \n",
    "        for c in range(num_cols):\n",
    "            # attribute value\n",
    "            attribute_value = label[:, c]\n",
    "\n",
    "            # attribute id\n",
    "            attribute_id = unmask_mask_id[:, c]\n",
    "            assert len(torch.unique(attribute_id)) == 1, \"attribute_id in one small batch has to be the same\"\n",
    "            attribute_id = torch.unique(attribute_id).item()\n",
    "\n",
    "            # this col is categorial \n",
    "            if self.cols_id_name[attribute_id] in self.category_vars:\n",
    "                categorial_labels.append(attribute_value)\n",
    "                \n",
    "            # this col is numerical\n",
    "            else:\n",
    "                numerical_labels.append(attribute_value)\n",
    "              \n",
    "        numerical_labels = torch.stack(numerical_labels, dim = 1)\n",
    "\n",
    "        return categorial_labels, numerical_labels\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "class tableMET(nn.Module):\n",
    "    def __init__(self, position_emb_dim, layers_encode, layers_decode, layers_iterative_transformer, \\\n",
    "                 num_head, num_variables, cols_name, category_vars, feature_emb_size, category_var_len, dropout):\n",
    "        super().__init__()\n",
    "        self.num_variables = num_variables\n",
    "\n",
    "        # feature embedding\n",
    "        self.feature_emb_fn = Feature_Embed(cols_name, category_vars, feature_emb_size, num_variables, position_emb_dim, category_var_len)\n",
    "\n",
    "        # encoder and decoder\n",
    "        self.transformer_encode = Transformer(position_emb_dim + feature_emb_size, layers_encode, num_head, dropout)\n",
    "        self.transformer_decode = Transformer(position_emb_dim + feature_emb_size, layers_decode, num_head, dropout)\n",
    "        \n",
    "        # iterative transformer decoding\n",
    "        # self.iterative_transformer_decode = nn.ModuleList(Transformer(position_emb_dim + feature_emb_size, layers_iterative_transformer, num_head, dropout) for _ in range(self.mask_ratio))\n",
    "        self.iterative_transformer_decode = Transformer(position_emb_dim + feature_emb_size, layers_iterative_transformer, num_head, dropout)\n",
    "        \n",
    "    def forward(self, unmasked_data, unmasked_idx, masked_idx, label, mask_ratio):\n",
    "        self.unmask_ratio = ((100 - mask_ratio) * self.num_variables) // 100\n",
    "        self.mask_ratio = self.num_variables - ((100 - mask_ratio) * self.num_variables) // 100\n",
    "    \n",
    "        # print(f\"self.unmask_ratio: {self.unmask_ratio}\")\n",
    "        # print(f\"self.mask_ratio: {self.mask_ratio}\")\n",
    "    \n",
    "        ''' feature & position encoding for Unmask and Mask '''\n",
    "        # unmask_emb: [batch_size, num_unmask = 12, hidden_size(position_emb_dim + feature_emb_size)]: ([4, 12, 64])\n",
    "        # mask_emb: [batch_size, num_mask = 6, hidden_size(position_emb_dim + feature_emb_size)]: ([4, 6, 64])\n",
    "        unmask_emb, mask_emb = self.feature_emb_fn.encode(unmasked_data, unmasked_idx, masked_idx)\n",
    "                \n",
    "        # transformer (encoder) only for unmasked part\n",
    "        # unmask_emb: [batch_size, num_unmask = 12, hidden_size(position_emb_dim + feature_emb_size)]: ([4, 12, 64])\n",
    "        ori_unmask_emb = unmask_emb\n",
    "        unmask_emb = self.transformer_encode(unmask_emb)\n",
    "        unmask_emb = unmask_emb + ori_unmask_emb\n",
    "        \n",
    "        # iterative transformer decoding\n",
    "        mask_emb_iter = []\n",
    "        for mask_id in range(self.mask_ratio):\n",
    "            maske_e = torch.unsqueeze(mask_emb[:, mask_id, :], dim = 1)\n",
    "            unmask_mask_emb = torch.cat([unmask_emb, maske_e], dim = 1)\n",
    "            unmask_mask_emb = self.iterative_transformer_decode(unmask_mask_emb, iterative = True)\n",
    "            mask_emb_ = torch.unsqueeze(unmask_mask_emb[:, -1, :], dim = 1)\n",
    "            mask_emb_iter.append(mask_emb_)\n",
    "\n",
    "        mask_emb_iter = torch.cat(mask_emb_iter, dim = 1)\n",
    "    \n",
    "        # concat unmask_emb and mask_emb\n",
    "        all_emb = torch.cat([unmask_emb, mask_emb_iter], dim = 1)\n",
    "        \n",
    "        # transformer (decoder) for both unmasked and masked\n",
    "        # all_emb: [batch_size, num_vars = 18, hidden_size(position_emb_dim + feature_emb_size)]: ([4, 18, 64])\n",
    "        ori_all_emb = all_emb\n",
    "        all_emb = self.transformer_decode(all_emb)\n",
    "        all_emb = all_emb + ori_all_emb\n",
    "        \n",
    "        ''' feature & position decoding for Unmask and Mask '''\n",
    "        unmask_mask_id = torch.cat([unmasked_idx, masked_idx], dim = 1)\n",
    "        categorial_preds, numerical_preds, categorial_col_name, numerical_col_name = self.feature_emb_fn.decode(all_emb, unmask_mask_id)\n",
    "\n",
    "        ''' reorder label: make label' variable order align as pred '''\n",
    "        categorial_labels, numerical_labels = self.feature_emb_fn.reorder_label_fn(label, unmask_mask_id)\n",
    "            \n",
    "\n",
    "        return categorial_preds, numerical_preds, categorial_labels, numerical_labels, categorial_col_name, numerical_col_name\n",
    "    \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "def linear_warmup_decay_lr(lr_init, lr_final, num_warmup_steps, num_training_steps):\n",
    "    def lr_lambda(current_step):\n",
    "        if current_step < num_warmup_steps:\n",
    "            return current_step / num_warmup_steps \n",
    "        else:\n",
    "            progress = (current_step - num_warmup_steps) / (num_training_steps - num_warmup_steps)\n",
    "            return (1 - progress) * (1 - lr_final / lr_init) + (lr_final / lr_init)  \n",
    "    return lr_lambda"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "LEARNING_RATE = 1e-4\n",
    "EPOCHS = 2000 \n",
    "layers_encode = 12\n",
    "layers_decode = 1\n",
    "layers_iterative_transformer = 4\n",
    "num_head = 1\n",
    "dropout = 0.4\n",
    "num_numerical = 3\n",
    "num_category = 16\n",
    "num_variables = num_numerical + num_category\n",
    "cols_name = processed_df_train.columns\n",
    "position_emb_dim = 16\n",
    "feature_emb_size = 16\n",
    "category_var_len = category_var_len\n",
    "\n",
    "is_power_of_two = lambda n: n > 0 and (n & (n - 1)) == 0\n",
    "assert is_power_of_two(position_emb_dim + feature_emb_size), \"position_emb_dim + feature_emb_size should be power term of 2\"\n",
    "\n",
    "model = tableMET(position_emb_dim, layers_encode, layers_decode, layers_iterative_transformer,\\\n",
    "                num_head, num_variables, cols_name, category_vars, feature_emb_size, category_var_len, dropout).to(device)\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=LEARNING_RATE, betas=(0.9, 0.95), eps=1e-6, weight_decay=1e-3)\n",
    "\n",
    "scheduler = LambdaLR(optimizer, lr_lambda=linear_warmup_decay_lr(lr_init = LEARNING_RATE, lr_final = LEARNING_RATE * 1e-2, num_warmup_steps = 10, num_training_steps = EPOCHS))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "MSE_loss_fn = nn.MSELoss()\n",
    "CE_loss_fn = nn.CrossEntropyLoss()\n",
    "\n",
    "def loss_fn(categorial_pred, numerical_pred, categorial_label, numerical_label):\n",
    "    \n",
    "    num_numerical = 3\n",
    "    num_category = 16\n",
    "    ratio_numerical = num_numerical / (num_numerical + num_category)\n",
    "    ratio_category = 1 / (num_numerical + num_category)\n",
    "    \n",
    "    total_loss = torch.zeros(1).to(device)\n",
    "    \n",
    "    mse_loss = MSE_loss_fn(numerical_pred, numerical_label)\n",
    "    mse_loss = mse_loss * 1 \n",
    "    total_loss += (mse_loss * ratio_numerical)\n",
    "    \n",
    "    for i in range(num_category):\n",
    "        pred = categorial_pred[i]\n",
    "        label = categorial_label[i]        \n",
    "        loss = CE_loss_fn(pred, label.long())\n",
    "        total_loss += (loss * ratio_category)\n",
    "        \n",
    "    return total_loss, mse_loss, (total_loss - mse_loss).item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "iterate epoch:   0%|          | 0/2000 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TRAINING, total variables are 19: first 13 unmasked, latter 6\n",
      "epoch: 0, loss: 2.2582236900925636, mse: 1.0852527897804976, ce: 1.172970898449421\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "iterate epoch:   0%|          | 1/2000 [00:11<6:12:26, 11.18s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "VALID, total variables are 19: first 13 unmasked, latter 6\n",
      "epoch: 0, val_loss: 2.419527232646942, val_mse: 1.254297286272049, val_ce_losses: 1.1652299463748932\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "iterate epoch:   2%|▎         | 50/2000 [05:10<2:36:03,  4.80s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TRAINING, total variables are 19: first 13 unmasked, latter 6\n",
      "epoch: 50, loss: 0.27843034407123923, mse: 0.0010236183352390071, ce: 0.2774067260324955\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "iterate epoch:   3%|▎         | 51/2000 [05:14<2:35:25,  4.78s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "VALID, total variables are 19: first 13 unmasked, latter 6\n",
      "epoch: 50, val_loss: 0.726648136973381, val_mse: 0.0075845487881451845, val_ce_losses: 0.7190635949373245\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "iterate epoch:   5%|▌         | 100/2000 [09:44<5:12:14,  9.86s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TRAINING, total variables are 19: first 13 unmasked, latter 6\n",
      "epoch: 100, loss: 0.16292003635317087, mse: 0.0004125081050005974, ce: 0.16250752843916416\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "iterate epoch:   5%|▌         | 101/2000 [09:57<5:40:43, 10.77s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "VALID, total variables are 19: first 13 unmasked, latter 6\n",
      "epoch: 100, val_loss: 0.7416488081216812, val_mse: 0.00435432035010308, val_ce_losses: 0.7372944802045822\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "iterate epoch:   8%|▊         | 150/2000 [20:21<6:07:35, 11.92s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TRAINING, total variables are 19: first 13 unmasked, latter 6\n",
      "epoch: 150, loss: 0.11716814362443984, mse: 0.00029580923273897497, ce: 0.11687233415432274\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "iterate epoch:   8%|▊         | 151/2000 [20:33<6:04:56, 11.84s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "VALID, total variables are 19: first 13 unmasked, latter 6\n",
      "epoch: 150, val_loss: 0.7513004392385483, val_mse: 0.0031795668182894588, val_ce_losses: 0.7481208741664886\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "iterate epoch:  10%|█         | 201/2000 [29:39<2:04:32,  4.15s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TRAINING, total variables are 19: first 13 unmasked, latter 6\n",
      "epoch: 200, loss: 0.0937263264786452, mse: 0.00021635634766425937, ce: 0.09350997000001371\n",
      "VALID, total variables are 19: first 13 unmasked, latter 6\n",
      "epoch: 200, val_loss: 0.777335450053215, val_mse: 0.002684125443920493, val_ce_losses: 0.7746513336896896\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "iterate epoch:  12%|█▎        | 250/2000 [37:36<4:51:12,  9.98s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TRAINING, total variables are 19: first 13 unmasked, latter 6\n",
      "epoch: 250, loss: 0.08232744340784848, mse: 0.0001799331644178892, ce: 0.08214751037303358\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "iterate epoch:  13%|█▎        | 251/2000 [37:46<4:54:05, 10.09s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "VALID, total variables are 19: first 13 unmasked, latter 6\n",
      "epoch: 250, val_loss: 0.8073562681674957, val_mse: 0.0028003312181681395, val_ce_losses: 0.8045559376478195\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "iterate epoch:  15%|█▌        | 300/2000 [46:30<5:03:38, 10.72s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TRAINING, total variables are 19: first 13 unmasked, latter 6\n",
      "epoch: 300, loss: 0.07521716190967709, mse: 0.00017975629839384055, ce: 0.07503740605898201\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "iterate epoch:  15%|█▌        | 301/2000 [46:40<5:01:01, 10.63s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "VALID, total variables are 19: first 13 unmasked, latter 6\n",
      "epoch: 300, val_loss: 0.8400614410638809, val_mse: 0.002696294104680419, val_ce_losses: 0.8373651504516602\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "iterate epoch:  16%|█▋        | 326/2000 [50:09<4:17:36,  9.23s/it]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[85], line 25\u001b[0m\n\u001b[1;32m     21\u001b[0m latent \u001b[38;5;241m=\u001b[39m data[\u001b[38;5;241m3\u001b[39m]\u001b[38;5;241m.\u001b[39mfloat()\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[1;32m     22\u001b[0m label \u001b[38;5;241m=\u001b[39m label\u001b[38;5;241m.\u001b[39mfloat()\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[1;32m     24\u001b[0m categorial_pred, numerical_pred, categorial_label, numerical_label, categorial_col_name,\\\n\u001b[0;32m---> 25\u001b[0m     numerical_col_name \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43munmasked_data\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43munmasked_idx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmasked_idx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlabel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmask_ratio\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     29\u001b[0m loss, mse_loss, ce_loss \u001b[38;5;241m=\u001b[39m loss_fn(categorial_pred, numerical_pred, categorial_label, numerical_label)\n\u001b[1;32m     31\u001b[0m losses\u001b[38;5;241m.\u001b[39mappend(loss\u001b[38;5;241m.\u001b[39mitem())\n",
      "File \u001b[0;32m~/miniconda3/envs/Vit/lib/python3.12/site-packages/torch/nn/modules/module.py:1739\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1737\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1738\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1739\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/Vit/lib/python3.12/site-packages/torch/nn/modules/module.py:1750\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1745\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1746\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1747\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1748\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1749\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1750\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1752\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1753\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "Cell \u001b[0;32mIn[81], line 61\u001b[0m, in \u001b[0;36mtableMET.forward\u001b[0;34m(self, unmasked_data, unmasked_idx, masked_idx, label, mask_ratio)\u001b[0m\n\u001b[1;32m     58\u001b[0m categorial_preds, numerical_preds, categorial_col_name, numerical_col_name \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfeature_emb_fn\u001b[38;5;241m.\u001b[39mdecode(all_emb, unmask_mask_id)\n\u001b[1;32m     60\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m''' reorder label: make label' variable order align as pred '''\u001b[39;00m\n\u001b[0;32m---> 61\u001b[0m categorial_labels, numerical_labels \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfeature_emb_fn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreorder_label_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlabel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43munmask_mask_id\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     64\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m categorial_preds, numerical_preds, categorial_labels, numerical_labels, categorial_col_name, numerical_col_name\n",
      "Cell \u001b[0;32mIn[80], line 274\u001b[0m, in \u001b[0;36mFeature_Embed.reorder_label_fn\u001b[0;34m(self, label, unmask_mask_id)\u001b[0m\n\u001b[1;32m    272\u001b[0m \u001b[38;5;66;03m# attribute id\u001b[39;00m\n\u001b[1;32m    273\u001b[0m attribute_id \u001b[38;5;241m=\u001b[39m unmask_mask_id[:, c]\n\u001b[0;32m--> 274\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43munique\u001b[49m\u001b[43m(\u001b[49m\u001b[43mattribute_id\u001b[49m\u001b[43m)\u001b[49m) \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m1\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mattribute_id in one small batch has to be the same\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    275\u001b[0m attribute_id \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39munique(attribute_id)\u001b[38;5;241m.\u001b[39mitem()\n\u001b[1;32m    277\u001b[0m \u001b[38;5;66;03m# this col is categorial \u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/Vit/lib/python3.12/site-packages/torch/_jit_internal.py:624\u001b[0m, in \u001b[0;36mboolean_dispatch.<locals>.fn\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    622\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m if_true(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    623\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 624\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mif_false\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/Vit/lib/python3.12/site-packages/torch/_jit_internal.py:624\u001b[0m, in \u001b[0;36mboolean_dispatch.<locals>.fn\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    622\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m if_true(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    623\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 624\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mif_false\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/Vit/lib/python3.12/site-packages/torch/functional.py:1080\u001b[0m, in \u001b[0;36m_return_output\u001b[0;34m(input, sorted, return_inverse, return_counts, dim)\u001b[0m\n\u001b[1;32m   1077\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28minput\u001b[39m):\n\u001b[1;32m   1078\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m _unique_impl(\u001b[38;5;28minput\u001b[39m, \u001b[38;5;28msorted\u001b[39m, return_inverse, return_counts, dim)\n\u001b[0;32m-> 1080\u001b[0m output, _, _ \u001b[38;5;241m=\u001b[39m \u001b[43m_unique_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43msorted\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreturn_inverse\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreturn_counts\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdim\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1081\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m output\n",
      "File \u001b[0;32m~/miniconda3/envs/Vit/lib/python3.12/site-packages/torch/functional.py:973\u001b[0m, in \u001b[0;36m_unique_impl\u001b[0;34m(input, sorted, return_inverse, return_counts, dim)\u001b[0m\n\u001b[1;32m    965\u001b[0m     output, inverse_indices, counts \u001b[38;5;241m=\u001b[39m _VF\u001b[38;5;241m.\u001b[39munique_dim(\n\u001b[1;32m    966\u001b[0m         \u001b[38;5;28minput\u001b[39m,\n\u001b[1;32m    967\u001b[0m         dim,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    970\u001b[0m         return_counts\u001b[38;5;241m=\u001b[39mreturn_counts,\n\u001b[1;32m    971\u001b[0m     )\n\u001b[1;32m    972\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 973\u001b[0m     output, inverse_indices, counts \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_unique2\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    974\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    975\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43msorted\u001b[39;49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43msorted\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    976\u001b[0m \u001b[43m        \u001b[49m\u001b[43mreturn_inverse\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_inverse\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    977\u001b[0m \u001b[43m        \u001b[49m\u001b[43mreturn_counts\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_counts\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    978\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    979\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m output, inverse_indices, counts\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "train_LOSS = []\n",
    "valid_LOSS = []\n",
    "\n",
    "for epoch in tqdm(range(EPOCHS), desc=\"iterate epoch\"):\n",
    "    \n",
    "    losses = []\n",
    "    mse_losses = []\n",
    "    ce_losses = []\n",
    "    \n",
    "    val_losses = []\n",
    "    val_mse_losses = []\n",
    "    val_ce_losses = []\n",
    "\n",
    "    model.train()\n",
    "    for data, label, mask_ratio in train_dataloader:\n",
    "        unmask_ratio = ((100 - mask_ratio) * num_variables) // 100\n",
    "        \n",
    "        unmasked_data = data[0].float().to(device)\n",
    "        unmasked_idx = data[1].long().to(device)\n",
    "        masked_idx = data[2].long().to(device)\n",
    "        latent = data[3].float().to(device)\n",
    "        label = label.float().to(device)\n",
    "        \n",
    "        categorial_pred, numerical_pred, categorial_label, numerical_label, categorial_col_name,\\\n",
    "            numerical_col_name = model(unmasked_data, unmasked_idx, masked_idx, label, mask_ratio)\n",
    "        \n",
    "        \n",
    "        \n",
    "        loss, mse_loss, ce_loss = loss_fn(categorial_pred, numerical_pred, categorial_label, numerical_label)\n",
    "        \n",
    "        losses.append(loss.item())\n",
    "        mse_losses.append(mse_loss.item())\n",
    "        ce_losses.append(ce_loss)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    scheduler.step()\n",
    "    \n",
    "    losses = np.mean(losses)\n",
    "    mse_losses = np.mean(mse_losses)\n",
    "    ce_losses = np.mean(ce_losses)\n",
    "    train_LOSS.append(losses)\n",
    "    \n",
    "    if epoch % 50 == 0:\n",
    "        print(f'TRAINING, total variables are {num_variables}: first {unmask_ratio} unmasked, latter {num_variables - unmask_ratio}')\n",
    "        print(f\"epoch: {epoch}, loss: {losses}, mse: {mse_losses}, ce: {ce_losses}\")\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        model.eval()\n",
    "        for data, label, mask_ratio in valid_dataloader:\n",
    "            unmask_ratio = ((100 - mask_ratio) * num_variables) // 100            \n",
    "            \n",
    "            unmasked_data = data[0].float().to(device)\n",
    "            unmasked_idx = data[1].long().to(device)\n",
    "            masked_idx = data[2].long().to(device)\n",
    "            latent = data[3].float().to(device)\n",
    "            label = label.float().to(device)\n",
    "\n",
    "            categorial_pred, numerical_pred, categorial_label, numerical_label, categorial_col_name,\\\n",
    "                numerical_col_name = model(unmasked_data, unmasked_idx, masked_idx, label, mask_ratio)\n",
    "            \n",
    "            loss, mse_loss, ce_loss = loss_fn(categorial_pred, numerical_pred, categorial_label, numerical_label)\n",
    "\n",
    "            val_losses.append(loss.item())\n",
    "            val_mse_losses.append(mse_loss.item())\n",
    "            val_ce_losses.append(ce_loss)            \n",
    "            \n",
    "    val_losses = np.mean(val_losses)\n",
    "    val_mse_losses = np.mean(val_mse_losses)\n",
    "    val_ce_losses = np.mean(val_ce_losses)\n",
    "    valid_LOSS.append(val_losses)\n",
    "    \n",
    "    if epoch % 50 == 0:\n",
    "        print(f'VALID, total variables are {num_variables}: first {unmask_ratio} unmasked, latter {num_variables - unmask_ratio}')\n",
    "        print(f\"epoch: {epoch}, val_loss: {val_losses}, val_mse: {val_mse_losses}, val_ce_losses: {val_ce_losses}\")\n",
    "        print()      \n",
    "\n",
    "model.save_pretrained(\"checkpoints/FT_MET_mushroom\")\n",
    "  \n",
    "        \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(range(len(train_LOSS)), train_LOSS, color = 'blue')\n",
    "plt.plot(range(len(valid_LOSS)), valid_LOSS, color = 'red')\n",
    "\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reverse_norm(df):\n",
    "    \n",
    "    new_df = pd.DataFrame()\n",
    "    \n",
    "\n",
    "    # Category \n",
    "    for cat_name in category_vars:\n",
    "        cat_var = df[cat_name]\n",
    "        new_df[f'{cat_name}'] = cat_var\n",
    "    \n",
    "    # Numerical score\n",
    "    for num_name in numerical_score_vars:\n",
    "        num_var = df[num_name]\n",
    "        \n",
    "        # reverse (0 - 100) min max norm\n",
    "        num_var = (num_var * 100) + 0\n",
    "        \n",
    "        new_df[num_name] = num_var.values\n",
    "    \n",
    "    # Numerical scalar\n",
    "    for num_name in numerical_scalar_vars:\n",
    "        num_var = df[num_name]\n",
    "        \n",
    "        # reverse (0 - 10) min max norm\n",
    "        num_var = (num_var * 10) + 0\n",
    "        # reverse\n",
    "        num_var = np.exp(num_var)\n",
    "        new_df[num_name] = num_var.values\n",
    "        \n",
    "    return new_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "select_sample_id = 5\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "unmasked_idx_one = np.array(unmasked_idx[select_sample_id].detach().cpu(), dtype=int)\n",
    "masked_idx_one = np.array(masked_idx[select_sample_id].detach().cpu(), dtype=int)\n",
    "\n",
    "\n",
    "categorial_pred_one = []\n",
    "for pred in categorial_pred:\n",
    "    argmax_pred = torch.argmax(pred, dim = 1)\n",
    "    categorial_pred_one.append(argmax_pred)\n",
    "categorial_pred_one = torch.stack(categorial_pred_one, dim = 1)\n",
    "categorial_pred_one = np.array(categorial_pred_one[select_sample_id].detach().cpu())\n",
    "\n",
    "categorial_label_one = torch.stack(categorial_label, dim = 1)\n",
    "categorial_label_one = np.array(categorial_label_one[select_sample_id].detach().cpu())\n",
    "\n",
    "numerical_pred_one = np.array(numerical_pred[select_sample_id].detach().cpu())\n",
    "numerical_label_one = np.array(numerical_label[select_sample_id].detach().cpu())\n",
    "\n",
    "\n",
    "print(f\"categorial_pred_one: {categorial_pred_one.shape}\")\n",
    "print(f\"categorial_label_one: {categorial_label_one.shape}\")\n",
    "print(f\"numerical_pred_one: {numerical_pred_one.shape}\")\n",
    "print(f\"numerical_label_one: {numerical_label_one.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred = np.concat([categorial_pred_one, numerical_pred_one], axis = 0)\n",
    "label = np.concat([categorial_label_one, numerical_label_one], axis = 0)\n",
    "\n",
    "res = pd.DataFrame([pred, label], columns = categorial_col_name + numerical_col_name)\n",
    "reverse_res = reverse_norm(res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reverse_res[cols_name[masked_idx_one]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reverse_res[cols_name[unmasked_idx_one]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "category_vars = ['Gender', 'Department', 'Grade', 'Extracurricular_Activities', 'Internet_Access_at_Home', 'Parent_Education_Level', 'Family_Income_Level']\n",
    "numerical_score_vars = ['Attendance (%)', 'Midterm_Score', 'Final_Score', 'Assignments_Avg', 'Quizzes_Avg', 'Projects_Score', 'Total_Score']\n",
    "numerical_scalar_vars = list(set(df.columns) - set(category_vars) - set(numerical_score_vars))\n",
    "numerical_scalar_vars\n",
    "# ['Study_Hours_per_Week', 'Age', 'Stress_Level (1-10)', 'Sleep_Hours_per_Night']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Vit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
