{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import numpy as np\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch.nn as nn\n",
    "from tqdm import tqdm\n",
    "from torch.nn import functional as F\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from torch.optim.lr_scheduler import LambdaLR"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"./Students_Grading_Dataset.csv\")\n",
    "# df\n",
    "\n",
    "# for att in df:\n",
    "#     print(f\"{att}: {df[att][0]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Manually select Cols (attribute)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "unimportant_attribute = ['Student_ID', 'First_Name', 'Last_Name', 'Email', 'Participation_Score']\n",
    "\n",
    "filtered_df = df.drop(unimportant_attribute, axis=1)\n",
    "filtered_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "category_vars = ['Gender', 'Department', 'Grade', 'Extracurricular_Activities', 'Internet_Access_at_Home', 'Parent_Education_Level', 'Family_Income_Level']\n",
    "# numerical_score_vars = ['Attendance (%)', 'Midterm_Score', 'Final_Score', 'Assignments_Avg', 'Quizzes_Avg', 'Participation_Score', 'Projects_Score', 'Total_Score', 'Stress_Level (1-10)']\n",
    "numerical_score_vars = ['Attendance (%)', 'Midterm_Score', 'Final_Score', 'Assignments_Avg', 'Quizzes_Avg', 'Projects_Score', 'Total_Score']\n",
    "\n",
    "numerical_scalar_vars = list(set(filtered_df.columns) - set(category_vars) - set(numerical_score_vars))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Separate rows with-Nan and without-Nan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nan_rows = filtered_df.isna().any(axis=1)\n",
    "\n",
    "# Nan rows\n",
    "df_nan = filtered_df[nan_rows]\n",
    "print(f\"row with Nan: {df_nan.shape}\")\n",
    "# Complete rows\n",
    "df_complete = filtered_df[~nan_rows]\n",
    "print(f\"row without Nan: {df_complete.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split df_complete into train/valid\n",
    "# data_amount = int(len(df_complete) * 0.8)\n",
    "# df_train = df_complete.iloc[:data_amount, :]\n",
    "# df_valid = df_complete.iloc[data_amount:, :]\n",
    "\n",
    "df_train, df_valid, _, _ = train_test_split(df_complete, df_complete, test_size=0.3, random_state=0)\n",
    "\n",
    "print(f\"df_train: {df_train.shape}\")\n",
    "print(f\"df_valid: {df_valid.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing: \n",
    "1. category to numerical\n",
    "2. max-min norm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def category_to_numerical(data):\n",
    "    le = LabelEncoder()\n",
    "    le.fit(data)\n",
    "    num_data = le.transform(data)\n",
    "    \n",
    "    return num_data, le\n",
    "\n",
    "# def max_min_norm(data, train_params = None, process_type = 'train'):\n",
    "    \n",
    "#     if process_type == 'train':\n",
    "#         data_max = np.max(data)\n",
    "#         data_min = np.min(data)\n",
    "#     else:\n",
    "#         data_max = train_params['Age'][0]\n",
    "#         data_min = train_params['Age'][1]\n",
    "        \n",
    "#     norm_data = (data - data_min) / (data_max - data_min + 1e-3)    \n",
    "    \n",
    "#     if process_type == 'train':\n",
    "#         return norm_data, data_max, data_min\n",
    "#     else:\n",
    "#         return norm_data\n",
    "    \n",
    "\n",
    "def max_min_norm_score(data, train_params = None, process_type = 'train'):\n",
    "    \n",
    "    if process_type == 'train':\n",
    "        data_max = 100\n",
    "        data_min = 0\n",
    "    else:\n",
    "        data_max = 100\n",
    "        data_min = 0\n",
    "        \n",
    "    norm_data = (data - data_min) / (data_max - data_min)    \n",
    "    \n",
    "    if process_type == 'train':\n",
    "        return norm_data, data_max, data_min\n",
    "    else:\n",
    "        return norm_data\n",
    "    \n",
    "def max_min_norm_scalar(data, train_params = None, process_type = 'train'):\n",
    "    \n",
    "    if process_type == 'train':\n",
    "        data_max = 10\n",
    "        data_min = 0\n",
    "    else:\n",
    "        data_max = 10\n",
    "        data_min = 0\n",
    "        \n",
    "    norm_data = (data - data_min) / (data_max - data_min)    \n",
    "    \n",
    "    if process_type == 'train':\n",
    "        return norm_data, data_max, data_min\n",
    "    else:\n",
    "        return norm_data\n",
    "\n",
    "    \n",
    "def preprocessing(df, train_params = None, process_type = 'train'):\n",
    "    \n",
    "    new_df = pd.DataFrame()\n",
    "    \n",
    "    if process_type == 'train':\n",
    "        train_params = {}\n",
    "        category_var_len = {}\n",
    "\n",
    "    # Category \n",
    "    for cat_name in category_vars:\n",
    "        cat_var = df[cat_name]\n",
    "        if process_type == 'train':\n",
    "            cat_var, le = category_to_numerical(cat_var)\n",
    "            train_params[f'{cat_name}_le'] = le\n",
    "            category_var_len[f'{cat_name}'] = len(np.unique(cat_var))\n",
    "        else:\n",
    "            cat_var = train_params[f'{cat_name}_le'].transform(cat_var)\n",
    "        new_df[f'{cat_name}'] = cat_var\n",
    "    \n",
    "    # Numerical score\n",
    "    for num_name in numerical_score_vars:\n",
    "        num_var = df[num_name]\n",
    "        if process_type == 'train':\n",
    "            num_var, data_max, data_min = max_min_norm_score(num_var, process_type = 'train')\n",
    "            train_params[num_name] = [data_max, data_min]\n",
    "        else:\n",
    "            num_var = max_min_norm_score(num_var, train_params, process_type = 'valid')\n",
    "        new_df[num_name] = num_var.values\n",
    "    \n",
    "    # Numerical scalar\n",
    "    for num_name in numerical_scalar_vars:\n",
    "        num_var = df[num_name]\n",
    "        num_var = np.log(num_var)\n",
    "        if process_type == 'train':\n",
    "            num_var, data_max, data_min = max_min_norm_scalar(num_var, process_type = 'train')\n",
    "            train_params[num_name] = [data_max, data_min]\n",
    "        else:\n",
    "            num_var = max_min_norm_scalar(num_var, train_params, process_type = 'valid')\n",
    "        new_df[num_name] = num_var.values\n",
    "        \n",
    "        \n",
    "    if process_type == 'train':\n",
    "        return new_df, train_params, category_var_len\n",
    "    else:\n",
    "        return new_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "processed_df_train, train_params, category_var_len = preprocessing(df_train, process_type = 'train')\n",
    "# train_params\n",
    "print(f\"category_var_len: {category_var_len}\")\n",
    "print(f\"processed_df_train: {processed_df_train.shape}\")\n",
    "processed_df_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "processed_df_valid = preprocessing(df_valid, train_params, process_type = 'valid')\n",
    "print(f\"processed_df_valid: {processed_df_valid.shape}\")\n",
    "processed_df_valid.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TableDataset(Dataset):\n",
    "    def __init__(self, data):\n",
    "        self.data = np.array(data)\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "    \n",
    "    def __getitem__(self, id):\n",
    "        dat = self.data[id, :]\n",
    "        dat = torch.from_numpy(dat)\n",
    "        return dat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 256\n",
    "DROPOUT = 0.3\n",
    "\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "\n",
    "train_dataset = TableDataset(processed_df_train)\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=BATCH_SIZE, num_workers=4, shuffle=True)\n",
    "\n",
    "valid_dataset = TableDataset(processed_df_valid)\n",
    "valid_dataloader = DataLoader(valid_dataset, batch_size=BATCH_SIZE, num_workers=4, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# category_var_len: {'Gender': 2, 'Department': 4, 'Grade': 5, \n",
    "                #    'Extracurricular_Activities': 2, 'Internet_Access_at_Home': 2, \n",
    "                #    'Parent_Education_Level': 4, 'Family_Income_Level': 3}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class tableModel(nn.Module):\n",
    "    def __init__(self, category_var_len):\n",
    "        super(tableModel,self).__init__()\n",
    "        self.num_category_var = len(category_var_len)\n",
    "        self.num_numerical_var = 12\n",
    "        self.category_emb_size = 1\n",
    "        self.category_dict = category_var_len\n",
    "        self.batch_size = BATCH_SIZE\n",
    "                \n",
    "        self.mask_prob = 0.05\n",
    "\n",
    "        self.transformer_layer = 4\n",
    "        self.transformer_emb_size = 32\n",
    "        self.num_head = 4\n",
    "        self.seq_len = (self.num_category_var * self.category_emb_size) + self.num_numerical_var\n",
    "        \n",
    "\n",
    "        ''' linear for numerical '''\n",
    "        self.linear_numerical1 = nn.Linear(self.num_numerical_var, 32, bias = False)\n",
    "        self.linear_numerical2 = nn.Linear(32, 64 , bias = False)\n",
    "        self.linear_numerical3 = nn.Linear(64, 32 , bias = False)\n",
    "        self.linear_numerical4 = nn.Linear(32, self.num_numerical_var , bias = False)\n",
    "\n",
    "        self.relu = nn.ReLU()\n",
    "        \n",
    "        self.dropout = nn.Dropout(DROPOUT)\n",
    "        \n",
    "    def masking_table(self, x, seed=42, training = True):\n",
    "        \"\"\"\n",
    "        x: (batch_size, num_var = 19)\n",
    "        \"\"\"\n",
    "        # Set random seed for reproducibility\n",
    "        \n",
    "        if training:\n",
    "            seed = torch.randint(0, 5, (1,))\n",
    "            torch.manual_seed(seed)\n",
    "            if torch.cuda.is_available():\n",
    "                torch.cuda.manual_seed_all(seed)\n",
    "        else:\n",
    "            torch.manual_seed(42)\n",
    "            if torch.cuda.is_available():\n",
    "                torch.cuda.manual_seed_all(42)\n",
    "\n",
    "        self.masking_prob = self.mask_prob \n",
    "        device = x.device  # Get the device from input tensor\n",
    "\n",
    "        # Category masking\n",
    "        category_var = x[:, :self.num_category_var].long()  # Ensure category_var is integer type\n",
    "        random_cat = torch.rand_like(category_var, dtype=torch.float, device=device)\n",
    "        masking_cat = random_cat < self.masking_prob\n",
    "        mask_token = torch.tensor([2, 4, 5, 2, 2, 4, 3], device=device, dtype=torch.long).expand_as(category_var)\n",
    "\n",
    "        # Apply mask in-place (avoiding memory allocation overhead)\n",
    "        masked_category_var = category_var.clone()  # Clone to avoid modifying input\n",
    "        masked_category_var[masking_cat] = mask_token[masking_cat]\n",
    "\n",
    "        # Numerical masking\n",
    "        numerical_var = x[:, -self.num_numerical_var:].float()  # Ensure numerical_var is float type\n",
    "        random_numerical = torch.rand_like(numerical_var, dtype=torch.float, device=device)\n",
    "        masking_numerical = random_numerical < self.masking_prob\n",
    "\n",
    "        masked_numerical_var = numerical_var.clone()  # Clone to avoid modifying input\n",
    "        masked_numerical_var[masking_numerical] = 1.5   # Directly set masked values to zero\n",
    "        # masked_numerical_var[masking_numerical] = torch.rand((1,), device = device)  # Directly set masked values to zero\n",
    "\n",
    "        masking_position = {\n",
    "            'masking_category': masking_cat,\n",
    "            'masking_numerical': masking_numerical,\n",
    "        }\n",
    "\n",
    "        # Concatenating the masked category and numerical variables\n",
    "        # return torch.cat([masked_category_var, masked_numerical_var], dim=1), masking_position\n",
    "        return torch.cat([category_var, masked_numerical_var], dim=1), masking_position\n",
    "\n",
    "                        \n",
    "    def forward(self, x, training):\n",
    "        '''\n",
    "        x: [batch_size, 19]\n",
    "        \n",
    "        category vars size = 7 --- embedding ---> category vars size = 14\n",
    "        numerical vars size = 12\n",
    "        \n",
    "        x: [batch_size, 14 + 12]\n",
    "        '''\n",
    "        \n",
    "        ''' masking'''\n",
    "        x, masking_position = self.masking_table(x, training)\n",
    "        \n",
    "        cat_vars = x[:, :self.num_category_var * self.category_emb_size]\n",
    "        num_vars = x[:, - self.num_numerical_var:]\n",
    "        \n",
    "        num_vars_ori = num_vars\n",
    "        num_vars = self.relu(self.linear_numerical1(num_vars))\n",
    "        num_vars = self.dropout(num_vars)\n",
    "        num_vars = self.relu(self.linear_numerical2(num_vars))\n",
    "        num_vars = self.dropout(num_vars)\n",
    "        num_vars = self.relu(self.linear_numerical3(num_vars))\n",
    "        num_vars = self.dropout(num_vars)\n",
    "        num_vars = self.linear_numerical4(num_vars)\n",
    "        num_vars = num_vars + num_vars_ori\n",
    "        \n",
    "\n",
    "        return num_vars, cat_vars, masking_position\n",
    "    \n",
    "    # def inference(self, x):\n",
    "    #     '''\n",
    "    #     DO NOT mask during inference\n",
    "    #     '''\n",
    "    #     ''' masking'''\n",
    "    #     x = self.masking_table(x)\n",
    "        \n",
    "    #     # category vars\n",
    "    #     cat_vars = []\n",
    "    #     for c_id, encode_fn in zip(range(self.num_category_var), self.encoders):\n",
    "    #         emb_c = encode_fn(x[:,c_id].long())\n",
    "    #         emb_c = self.encode_dropout(emb_c)\n",
    "    #         cat_vars.append(emb_c)\n",
    "    #     cat_vars = torch.cat(cat_vars, dim = 1).float()\n",
    "\n",
    "    #     # numerical vars\n",
    "    #     num_vars = x[:, - self.num_numerical_var:].float()\n",
    "\n",
    "    #     # combine category and numerical vars        \n",
    "    #     x = torch.cat([cat_vars, num_vars], dim = 1)\n",
    "\n",
    "    #     '''\n",
    "    #     Transformer\n",
    "    #     '''\n",
    "    #     x = torch.unsqueeze(x, dim = 2)\n",
    "    #     x = self.gpt(x)\n",
    "    #     x = torch.squeeze(x, dim = 2)\n",
    "        \n",
    "        \n",
    "    #     ''' Decode category ''' \n",
    "    #     # split numerical and category\n",
    "    #     num_vars = x[:, - self.num_numerical_var:]\n",
    "    #     cat_vars = x[:, :self.num_category_var * 2]\n",
    "        \n",
    "    #     # category vars\n",
    "    #     decoded_cat_vars = []\n",
    "    #     for c_id, decode_fn in zip(range(self.num_category_var), self.decoders):\n",
    "    #         emb_c = cat_vars[:, c_id * self.emb_size: (c_id + 1) * self.emb_size]\n",
    "    #         c_var = decode_fn(emb_c)\n",
    "    #         c_var = self.decode_dropout(c_var)\n",
    "    #         pred_c = torch.argmax(c_var, dim = -1)\n",
    "    #         decoded_cat_vars.append(pred_c)\n",
    "        \n",
    "    #     decoded_cat_vars = torch.stack(decoded_cat_vars, dim = 1)\n",
    "\n",
    "    #     pred = torch.cat([decoded_cat_vars, num_vars], dim = 1)\n",
    "\n",
    "    #     return pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "train_dataset = TableDataset(processed_df_train)\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=BATCH_SIZE, num_workers=4, shuffle=True)\n",
    "\n",
    "valid_dataset = TableDataset(processed_df_valid)\n",
    "valid_dataloader = DataLoader(valid_dataset, batch_size=BATCH_SIZE, num_workers=4, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def linear_warmup_decay_lr(lr_init, lr_final, num_warmup_steps, num_training_steps):\n",
    "    \"\"\"\n",
    "    Returns a lambda function for LambdaLR.\n",
    "    - lr_init: 初始學習率\n",
    "    - lr_final: 最終學習率（不是 0）\n",
    "    - num_warmup_steps: 預熱步數\n",
    "    - num_training_steps: 總訓練步數\n",
    "    \"\"\"\n",
    "    def lr_lambda(current_step):\n",
    "        if current_step < num_warmup_steps:\n",
    "            return current_step / num_warmup_steps  # 線性預熱\n",
    "        else:\n",
    "            progress = (current_step - num_warmup_steps) / (num_training_steps - num_warmup_steps)\n",
    "            return (1 - progress) * (1 - lr_final / lr_init) + (lr_final / lr_init)  # 線性衰減到 lr_final\n",
    "    return lr_lambda"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "LEARNING_RATE = 2e-3\n",
    "EPOCHS = 5000\n",
    "\n",
    "\n",
    "model = tableModel(category_var_len).to(device)\n",
    "# optimizer = torch.optim.AdamW(model.parameters(), lr = 1e-3)\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=LEARNING_RATE, betas=(0.9, 0.95), eps=1e-6, weight_decay=1e-3)\n",
    "\n",
    "scheduler = LambdaLR(optimizer, lr_lambda=linear_warmup_decay_lr(lr_init = LEARNING_RATE, lr_final = LEARNING_RATE * 1e-2, num_warmup_steps = 100, num_training_steps = EPOCHS))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# MSE_loss_fn = nn.MSELoss()\n",
    "# CE_loss_fn = nn.CrossEntropyLoss()\n",
    "\n",
    "# def loss_fn(pred_numerical, pred_category, label, masking_position):\n",
    "    \n",
    "#     num_numerical = 12\n",
    "#     num_category = 7\n",
    "#     ratio_numerical = num_numerical / (num_numerical + num_｀category)\n",
    "#     ratio_category = 1 / (num_numerical + num_category)\n",
    "    \n",
    "#     label_category = label[:, :num_category]\n",
    "#     label_numerical = label[:, -num_numerical:]\n",
    "    \n",
    "#     total_loss = torch.zeros(1).to(device)\n",
    "    \n",
    "#     mse_loss = MSE_loss_fn(pred_numerical, label_numerical)\n",
    "#     total_loss += (mse_loss * ratio_numerical)\n",
    "    \n",
    "#     for i in range(num_category):\n",
    "#         pred = pred_category[i]\n",
    "#         loss = CE_loss_fn(pred, label_category[:, i].long())\n",
    "#         total_loss += (loss * ratio_category)\n",
    "        \n",
    "#     return total_loss, mse_loss, (total_loss - mse_loss)\n",
    "\n",
    "MSE_loss_fn = nn.MSELoss(reduction='none')  # 逐元素 MSE\n",
    "CE_loss_fn = nn.CrossEntropyLoss(reduction='none')  # 逐元素 CrossEntropy\n",
    "\n",
    "def loss_fn(pred_numerical, pred_category, label, mask_position):\n",
    "    device = label.device\n",
    "    \n",
    "    num_numerical = 12\n",
    "    num_category = 7\n",
    "    ratio_numerical = num_numerical / (num_numerical + num_category)\n",
    "    ratio_category = 1 / (num_numerical + num_category)\n",
    "\n",
    "    masking_category = mask_position['masking_category']  # shape: (batch_size, num_category)\n",
    "    masking_numerical = mask_position['masking_numerical']  # shape: (batch_size, num_numerical)\n",
    "\n",
    "    label_category = label[:, :num_category]\n",
    "    label_numerical = label[:, -num_numerical:]\n",
    "\n",
    "    total_loss = torch.zeros(1, device=device)\n",
    "\n",
    "    # === 1. MSE Loss ===\n",
    "    # 先用 masking_numerical 過濾 pred_numerical 和 label_numerical\n",
    "    pred_numerical_masked = pred_numerical[masking_numerical]\n",
    "    label_numerical_masked = label_numerical[masking_numerical]\n",
    "\n",
    "    if pred_numerical_masked.numel() > 0:  # 確保有 mask 位置\n",
    "        mse_loss = MSE_loss_fn(pred_numerical_masked, label_numerical_masked).mean()\n",
    "        total_loss += (mse_loss * 1)\n",
    "\n",
    "    # # === 2. CrossEntropy Loss ===\n",
    "    # for i in range(num_category):\n",
    "    #     category_mask = masking_category[:, i]  # shape: (batch_size,)\n",
    "    #     pred = pred_category[i]  # shape: (batch_size, num_classes)\n",
    "    #     label_cat = label_category[:, i].long()  # shape: (batch_size,)\n",
    "\n",
    "    #     # 先用 mask 過濾 pred 和 label\n",
    "    #     pred_masked = pred[category_mask]\n",
    "    #     label_masked = label_cat[category_mask]\n",
    "        \n",
    "    #     if pred_masked.shape[0] > 0:  # 確保有 mask 位置\n",
    "    #         ce_loss = CE_loss_fn(pred_masked, label_masked).mean()\n",
    "    #         total_loss += ce_loss * ratio_category\n",
    "\n",
    "    # ce_loss = (total_loss - mse_loss)\n",
    "\n",
    "    # if (total_loss - mse_loss) < 0.20:\n",
    "    #     total_loss = mse_loss * 50 + ce_loss\n",
    "    #     return total_loss, mse_loss, ce_loss\n",
    "    \n",
    "    return total_loss, mse_loss, torch.zeros([1])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "train_LOSS = []\n",
    "valid_LOSS = []\n",
    "\n",
    "for epoch in tqdm(range(EPOCHS), desc=\"iterate epoch\"):\n",
    "    losses = []\n",
    "    mse_losses = []\n",
    "    ce_losses = []\n",
    "    \n",
    "    val_losses = []\n",
    "    val_mse_losses = []\n",
    "    val_ce_losses = []\n",
    "    \n",
    "    \n",
    "    model.train()\n",
    "    for data in train_dataloader:\n",
    "        data = data.float().to(device)\n",
    "\n",
    "        pred_numerical, pred_category, masking_position = model(data, training = True)\n",
    "        loss, mse_loss, ce_loss = loss_fn(pred_numerical, pred_category, data, masking_position)\n",
    "        losses.append(loss.item())\n",
    "        mse_losses.append(mse_loss.item())\n",
    "        ce_losses.append(ce_loss.item())\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    scheduler.step()\n",
    "    \n",
    "    losses = np.mean(losses)\n",
    "    mse_losses = np.mean(mse_losses)\n",
    "    ce_losses = np.mean(ce_losses)\n",
    "\n",
    "    train_LOSS.append(losses)\n",
    "    \n",
    "    if epoch % 100 == 0:    \n",
    "        print(f\"epoch: {epoch}, loss: {losses}, mse: {mse_losses}, ce: {ce_losses}\")\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        model.eval()\n",
    "        for data in valid_dataloader:\n",
    "            data = data.float().to(device)\n",
    "                \n",
    "            pred_numerical, pred_category, masking_position = model(data, training = False)\n",
    "            loss, mse_loss, ce_loss = loss_fn(pred_numerical, pred_category, data, masking_position)\n",
    "            val_losses.append(loss.item())\n",
    "            val_mse_losses.append(mse_loss.item())\n",
    "            val_ce_losses.append(ce_loss.item())\n",
    "            \n",
    "    val_losses = np.mean(val_losses)\n",
    "    val_mse_losses = np.mean(val_mse_losses)\n",
    "    val_ce_losses = np.mean(val_ce_losses)\n",
    "    \n",
    "    valid_LOSS.append(val_losses)\n",
    "    \n",
    "    if epoch % 100 == 0:\n",
    "        print(f\"epoch: {epoch}, val_loss: {val_losses}, val_mse: {val_mse_losses}, val_ce_losses: {val_ce_losses}\")\n",
    "        print()\n",
    "        \n",
    "        \n",
    "        \n",
    "        # pred = model.inference(data)\n",
    "        \n",
    "        \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "plt .plot(range(len(train_LOSS)), train_LOSS, color = 'blue')\n",
    "plt.plot(range(len(valid_LOSS)), valid_LOSS, color = 'red')\n",
    "\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "\n",
    "img = Image.open('../../dataset/train/0.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img_np = np.array(img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img_np.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = torch.rand((128,128))\n",
    "a.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "128 * 128"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean = torch.mean(a)\n",
    "std = torch.std(a)\n",
    "\n",
    "outlier_upper = mean + 1 * std\n",
    "outlier_down = mean - 1 * std\n",
    "\n",
    "(a < outlier_upper) & (a > outlier_down)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "k = a[(a < outlier_upper) & (a > outlier_down)]\n",
    "k"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "k = a[(a < outlier_upper) & (a > outlier_down)]\n",
    "k"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.sum(((a < outlier_upper) & (a > outlier_down)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mask = torch.ones((8,8))\n",
    "causal_mask = torch.tril(mask)\n",
    "# causal_mask[:8, :8] = float('-inf')\n",
    "causal_mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "causal_mask = torch.where(causal_mask == 0, float('-inf'), causal_mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "causal_mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "c = torch.rand((64, 8,8))\n",
    "c.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d = c * causal_mask\n",
    "d[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "e = torch.softmax(d, dim=-1)\n",
    "e"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "0.04"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.arange(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nn.Parameter(torch.rand())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Vit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
